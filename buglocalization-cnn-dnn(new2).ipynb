{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":11265211,"sourceType":"datasetVersion","datasetId":7041476},{"sourceId":11280721,"sourceType":"datasetVersion","datasetId":7052679}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Load dữ liệu đã xử lí","metadata":{"id":"-eUizc5QmpX4"}},{"cell_type":"code","source":"!pip install nltk\n","metadata":{"id":"9qfVWdges0io","outputId":"e6051f73-0867-4eb4-b5c0-984722d836d3","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:14.414045Z","iopub.execute_input":"2025-04-13T16:54:14.414318Z","iopub.status.idle":"2025-04-13T16:54:19.299304Z","shell.execute_reply.started":"2025-04-13T16:54:14.414290Z","shell.execute_reply":"2025-04-13T16:54:19.298201Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import nltk\nnltk.download('averaged_perceptron_tagger_eng')","metadata":{"id":"8SUNdusXgQ-3","outputId":"937573b4-53fb-4bb5-9c39-84eb53bc1890","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:19.300665Z","iopub.execute_input":"2025-04-13T16:54:19.300994Z","iopub.status.idle":"2025-04-13T16:54:21.026401Z","shell.execute_reply.started":"2025-04-13T16:54:19.300964Z","shell.execute_reply":"2025-04-13T16:54:21.025616Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# English stop words\nstop_words = set(\n    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n     'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',\n     'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',\n     'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n     'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n     'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n     'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n     'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n     'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n     'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n     'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n     'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n     's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o',\n     're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven',\n     'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won',\n     'wouldn', 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'n', 'p', 'q', 'u', 'v',\n     'w', 'x', 'z', 'us'])\n\n# Java language keywords\njava_keywords = set(\n    ['abstract', 'assert', 'boolean', 'break', 'byte', 'case',\n     'catch', 'char', 'class', 'const', 'continue', 'default', 'do', 'double',\n     'else', 'enum', 'extends', 'false', 'final', 'finally', 'float', 'for', 'goto',\n     'if', 'implements', 'import', 'instanceof', 'int', 'interface', 'long',\n     'native', 'new', 'null', 'package', 'private', 'protected', 'public', 'return',\n     'short', 'static', 'strictfp', 'super', 'switch', 'synchronized', 'this',\n     'throw', 'throws', 'transient', 'true', 'try', 'void', 'volatile', 'while'])\n\nfrom collections import namedtuple\nfrom pathlib import Path\n\n# Dataset root directory (điều chỉnh đường dẫn nếu cần)\n_DATASET_ROOT = Path('/content/drive/MyDrive/Colab Notebooks/NLP/Task bug localization/')\n\nDataset = namedtuple('Dataset', ['name', 'src', 'bug_repo', 'repo_url', 'features'])\n\n# Các dataset được định nghĩa\naspectj = Dataset(\n    'aspectj',\n    _DATASET_ROOT / 'source files/org.aspectj',\n    _DATASET_ROOT / 'bug reports/AspectJ.txt',\n    \"https://github.com/eclipse/org.aspectj/tree/bug433351.git\",\n    _DATASET_ROOT / 'bug reports/AspectJ.xlsx'\n)\n\neclipse = Dataset(\n    'eclipse',\n    _DATASET_ROOT / 'source files/eclipse.platform.ui-johna-402445',\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.txt',\n    \"https://github.com/eclipse/eclipse.platform.ui.git\",\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.xlsx'\n)\n\nswt = Dataset(\n    'swt',\n    _DATASET_ROOT / 'source files/eclipse.platform.swt-xulrunner-31',\n    _DATASET_ROOT / 'bug reports/SWT.txt',\n    \"https://github.com/eclipse/eclipse.platform.swt.git\",\n    _DATASET_ROOT / 'bug reports/SWT.xlsx'\n)\n\ntomcat = Dataset(\n    'tomcat',\n    _DATASET_ROOT / 'source files/tomcat-7.0.51',\n    _DATASET_ROOT / 'bug reports/Tomcat.txt',\n    \"https://github.com/apache/tomcat.git\",\n    _DATASET_ROOT / 'bug reports/Tomcat.xlsx'\n)\n\nbirt = Dataset(\n    'birt',\n    _DATASET_ROOT / 'source files/birt-20140211-1400',\n    _DATASET_ROOT / 'bug reports/Birt.txt',\n    \"https://github.com/apache/birt.git\",\n    _DATASET_ROOT / 'bug reports/Birt.xlsx'\n)\n\n\n### Current dataset in use. (change this name to change the dataset)\nDATASET = tomcat\n\nclass BugReport:\n    \"\"\"Class representing each bug report\"\"\"\n    __slots__ = ['summary', 'description', 'fixed_files', 'report_time', 'pos_tagged_summary', 'pos_tagged_description','stack_traces','stack_traces_remove']\n\n    def __init__(self, summary, description, fixed_files, report_time):\n        self.summary = summary\n        self.description = description\n        self.fixed_files = fixed_files\n        self.report_time = report_time\n        self.pos_tagged_summary = None\n        self.pos_tagged_description = None\n        self.stack_traces = None\n        self.stack_traces_remove = None\n\nclass SourceFile:\n    \"\"\"Class representing each source file\"\"\"\n    __slots__ = ['all_content', 'comments', 'class_names', 'attributes', 'method_names', 'variables', 'file_name',\n                 'pos_tagged_comments', 'exact_file_name', 'package_name']\n\n    def __init__(self, all_content, comments, class_names, attributes, method_names, variables, file_name,\n                 package_name):\n        self.all_content = all_content\n        self.comments = comments\n        self.class_names = class_names\n        self.attributes = attributes\n        self.method_names = method_names\n        self.variables = variables\n        self.file_name = file_name\n        self.exact_file_name = file_name[0]\n        self.package_name = package_name\n        self.pos_tagged_comments = None\n\n\nclass Parser:\n    \"\"\"Class containing different parsers\"\"\"\n    __slots__ = ['name', 'src', 'bug_repo']\n\n    def __init__(self, pro):\n        self.name = pro.name\n        self.src = pro.src\n        self.bug_repo = pro.bug_repo\n\n    def report_parser(self):\n        reader = csv.DictReader(open(self.bug_repo, \"r\"), delimiter=\"\\t\")\n        bug_reports = OrderedDict()\n        # raw_texts = []\n        # fixed_files = []\n        for line in reader:\n            # line[\"raw_text\"] = line[\"summary\"] + ' ' + line[\"description\"]\n            line[\"report_time\"] = datetime.strptime(line[\"report_time\"], \"%Y-%m-%d %H:%M:%S\")\n            temp = line[\"files\"].strip().split(\".java\")\n            length = len(temp)\n            x = []\n            for i, f in enumerate(temp):\n                if i == (length - 1):\n                    x.append(os.path.normpath(f))\n                    continue\n                x.append(os.path.normpath(f + \".java\"))\n            line[\"files\"] = x\n            bug_reports[line[\"bug_id\"]] = BugReport(line[\"summary\"], line[\"description\"], line[\"files\"],\n                                                    line[\"report_time\"])\n        # bug_reports = tsv2dict(self.bug_repo)\n\n        return bug_reports\n\n    def src_parser(self):\n        \"\"\"Parse source code directory of a program and colect its java files\"\"\"\n\n        # Gettting the list of source files recursively from the source directory\n        src_addresses = glob.glob(str(self.src) + '/**/*.java', recursive=True)\n        print(src_addresses)\n        # Creating a java lexer instance for pygments.lex() method\n        java_lexer = JavaLexer()\n        src_files = OrderedDict()\n        # src_files = dict()\n        # Looping to parse each source file\n        for src_file in src_addresses:\n            with open(src_file, encoding='latin-1') as file:\n                src = file.read()\n\n            # Placeholder for different parts of a source file\n            comments = ''\n            class_names = []\n            attributes = []\n            method_names = []\n            variables = []\n\n            # Source parsing\n            parse_tree = None\n            try:\n                parse_tree = javalang.parse.parse(src)\n                for path, node in parse_tree.filter(javalang.tree.VariableDeclarator):\n                    if isinstance(path[-2], javalang.tree.FieldDeclaration):\n                        attributes.append(node.name)\n                    elif isinstance(path[-2], javalang.tree.VariableDeclaration):\n                        variables.append(node.name)\n            except:\n                pass\n\n            # Triming the source file\n            ind = False\n            if parse_tree:\n                if parse_tree.imports:\n                    last_imp_path = parse_tree.imports[-1].path\n                    src = src[src.index(last_imp_path) + len(last_imp_path) + 1:]\n                elif parse_tree.package:\n                    package_name = parse_tree.package.name\n                    src = src[src.index(package_name) + len(package_name) + 1:]\n                else:  # no import and no package declaration\n                    ind = True\n            # javalang can't parse the source file\n            else:\n                ind = True\n\n            # Lexically tokenize the source file\n            lexed_src = pygments.lex(src, java_lexer)\n\n            for i, token in enumerate(lexed_src):\n                if token[0] in Token.Comment:\n                    if ind and i == 0 and token[0] is Token.Comment.Multiline:\n                        src = src[src.index(token[1]) + len(token[1]):]\n                        continue\n                    comments = comments + token[1]\n                elif token[0] is Token.Name.Class:\n                    class_names.append(token[1])\n                elif token[0] is Token.Name.Function:\n                    method_names.append(token[1])\n\n            # get the package declaration if exists\n            if parse_tree and parse_tree.package:\n                package_name = parse_tree.package.name\n            else:\n                package_name = None\n\n            if self.name == 'aspectj' or 'tomcat' or 'eclipse' or 'swt':\n                src_files[os.path.relpath(src_file, start=self.src)] = SourceFile(src, comments, class_names,\n                                                                                  attributes, method_names, variables, [\n                                                                                      os.path.basename(src_file).split(\n                                                                                          '.')[0]], package_name)\n            else:\n                # If source files has package declaration\n                if package_name:\n                    src_id = (package_name + '.' + os.path.basename(src_file))\n                else:\n                    src_id = os.path.basename(src_file)\n                src_files[src_id] = SourceFile(src, comments, class_names, attributes, method_names, variables,\n                                               [os.path.basename(src_file).split('.')[0]], package_name)\n            # print(src_files)\n            # print(\"===========\")\n        return src_files\n\n\nclass ReportPreprocessing:\n    \"\"\"Class preprocess bug reports\"\"\"\n    __slots__ = ['bug_reports']\n\n    def __init__(self, bug_reports):\n        self.bug_reports = bug_reports\n\n    def extract_stack_traces(self):\n        \"\"\"Extract stack traces from bug reports\"\"\"\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            report.stack_traces = st\n\n    def extract_stack_traces_remove(self):\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            at = []\n            for x in st:\n                if (x[1] == 'Unknown Source'):\n                    temp = 'Unknown'\n                    y = x[0]+ '(' + temp\n                else:\n                    y = x[0] + '(' + x[1] + ')'\n                at.append(y)\n            report.stack_traces_remove = at\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from bug reports raw_text\"\"\"\n        for report in self.bug_reports.values():\n            # Tokenizing using word_tokeize for more accurate pos-tagging\n            sum_tok = nltk.word_tokenize(report.summary)\n            desc_tok = nltk.word_tokenize(report.description)\n            sum_pos = nltk.pos_tag(sum_tok)\n            desc_pos = nltk.pos_tag(desc_tok)\n            report.pos_tagged_summary = [token for token, pos in sum_pos if 'NN' in pos or 'VB' in pos]\n            report.pos_tagged_description = [token for token, pos in desc_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"Tokenize bug report intro tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = nltk.wordpunct_tokenize(report.summary)\n            report.description = nltk.wordpunct_tokenize(report.description)\n\n    def _split_camelcase(self, tokens):\n        # copy tokens\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camel case detection for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        \"\"\"Split camelcase indentifiers\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = self._split_camelcase(report.summary)\n            report.description = self._split_camelcase(report.description)\n            report.pos_tagged_summary = self._split_camelcase(report.pos_tagged_summary)\n            report.pos_tagged_description = self._split_camelcase(report.pos_tagged_description)\n\n    def normalize(self):\n        \"\"\"remove punctuation, numbers and lowecase conversion\"\"\"\n        # build a translate table for punctuation and number removal\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n\n        for report in self.bug_reports.values():\n            summary_punctnum_rem = [token.translate(punctnum_table) for token in report.summary]\n            desc_punctnum_rem = [token.translate(punctnum_table) for token in report.description]\n            pos_sum_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_summary]\n            pos_desc_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_description]\n            report.summary = [token.lower() for token in summary_punctnum_rem if token]\n            report.description = [token.lower() for token in desc_punctnum_rem if token]\n            report.pos_tagged_summary = [token.lower() for token in pos_sum_punctnum_rem if token]\n            report.pos_tagged_description = [token.lower() for token in pos_desc_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        \"\"\"removing stop word from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in stop_words]\n            report.description = [token for token in report.description if token not in stop_words]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in stop_words]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in stop_words]\n\n    def remove_java_keywords(self):\n        \"\"\"removing java language keywords from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in java_keywords]\n            report.description = [token for token in report.description if token not in java_keywords]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in java_keywords]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for report in self.bug_reports.values():\n            report.summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.summary], report.summary]))\n            report.description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.description], report.description]))\n            report.pos_tagged_summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_summary], report.pos_tagged_summary]))\n            report.pos_tagged_description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_description], report.pos_tagged_description]))\n\n    def preprocess(self):\n        self.extract_stack_traces()\n        self.extract_stack_traces_remove()\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_java_keywords()\n        self.stem()\n\nclass SrcPreprocessing:\n    \"\"\"class to preprocess source code\"\"\"\n    __slots__ = ['src_files']\n\n    def __init__(self, src_files):\n        self.src_files = src_files\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from comments\"\"\"\n        for src in self.src_files.values():\n            # tokenize using word_tokenize\n            comments_tok = nltk.word_tokenize(src.comments)\n            comments_pos = nltk.pos_tag(comments_tok)\n            src.pos_tagged_comments = [token for token, pos in comments_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"tokenize source code to tokens\"\"\"\n        for src in self.src_files.values():\n            src.all_content = nltk.wordpunct_tokenize(src.all_content)\n            src.comments = nltk.wordpunct_tokenize(src.comments)\n\n    def _split_camelcase(self, tokens):\n        # copy token\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camelcase defect for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        # Split camelcase indenti\n        for src in self.src_files.values():\n            src.all_content = self._split_camelcase(src.all_content)\n            src.comments = self._split_camelcase(src.comments)\n            src.class_names = self._split_camelcase(src.class_names)\n            src.attributes = self._split_camelcase(src.attributes)\n            src.method_names = self._split_camelcase(src.method_names)\n            src.variables = self._split_camelcase(src.variables)\n            src.pos_tagged_comments = self._split_camelcase(src.pos_tagged_comments)\n\n    def normalize(self):\n        \"remove punctuation, number and lowercase conversion\"\n        # build a translate table for punctuation and number\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n        for src in self.src_files.values():\n            content_punctnum_rem = [token.translate(punctnum_table) for token in src.all_content]\n            comments_punctnum_rem = [token.translate(punctnum_table) for token in src.comments]\n            classnames_punctnum_rem = [token.translate(punctnum_table) for token in src.class_names]\n            attributes_punctnum_rem = [token.translate(punctnum_table) for token in src.attributes]\n            methodnames_punctnum_rem = [token.translate(punctnum_table) for token in src.method_names]\n            variables_punctnum_rem = [token.translate(punctnum_table) for token in src.variables]\n            filename_punctnum_rem = [token.translate(punctnum_table) for token in src.file_name]\n            pos_comments_punctnum_rem = [token.translate(punctnum_table) for token in src.pos_tagged_comments]\n\n            src.all_content = [token.lower() for token in content_punctnum_rem if token]\n            src.comments = [token.lower() for token in comments_punctnum_rem if token]\n            src.class_names = [token.lower() for token in classnames_punctnum_rem if token]\n            src.attributes = [token.lower() for token in attributes_punctnum_rem if token]\n            src.method_names = [token.lower() for token in methodnames_punctnum_rem if token]\n            src.variables = [token.lower() for token in variables_punctnum_rem if token]\n            src.file_name = [token.lower() for token in filename_punctnum_rem if token]\n            src.pos_tagged_comments = [token.lower() for token in pos_comments_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in stop_words]\n            src.comments = [token for token in src.comments if token not in stop_words]\n            src.class_names = [token for token in src.class_names if token not in stop_words]\n            src.attributes = [token for token in src.attributes if token not in stop_words]\n            src.method_names = [token for token in src.method_names if token not in stop_words]\n            src.variables = [token for token in src.variables if token not in stop_words]\n            src.file_name = [token for token in src.file_name if token not in stop_words]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in stop_words]\n\n    def remove_javakeywords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in java_keywords]\n            src.comments = [token for token in src.comments if token not in java_keywords]\n            src.class_names = [token for token in src.class_names if token not in java_keywords]\n            src.attributes = [token for token in src.attributes if token not in java_keywords]\n            src.method_names = [token for token in src.method_names if token not in java_keywords]\n            src.variables = [token for token in src.variables if token not in java_keywords]\n            src.file_name = [token for token in src.file_name if token not in java_keywords]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for src in self.src_files.values():\n            src.all_content = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.all_content], src.all_content]))\n            src.comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.comments], src.comments]))\n            src.class_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.class_names], src.class_names]))\n            src.attributes = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.attributes], src.attributes]))\n            src.method_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.method_names], src.method_names]))\n            src.variables = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.variables], src.variables]))\n            src.file_name = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.file_name], src.file_name]))\n            src.pos_tagged_comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.pos_tagged_comments], src.pos_tagged_comments]))\n\n\n    def preprocess(self):\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_javakeywords()\n        self.stem()","metadata":{"id":"_22yeS4wcWpU","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:21.028599Z","iopub.execute_input":"2025-04-13T16:54:21.028975Z","iopub.status.idle":"2025-04-13T16:54:21.097773Z","shell.execute_reply.started":"2025-04-13T16:54:21.028954Z","shell.execute_reply":"2025-04-13T16:54:21.096577Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install inflection\nimport inflection\n","metadata":{"id":"ACZrz5Byh7Ur","outputId":"68b0fd14-1ac6-484a-cb84-2ab028bd5ed6","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:21.098990Z","iopub.execute_input":"2025-04-13T16:54:21.099328Z","iopub.status.idle":"2025-04-13T16:54:24.872052Z","shell.execute_reply.started":"2025-04-13T16:54:21.099294Z","shell.execute_reply":"2025-04-13T16:54:24.871102Z"}},"outputs":[{"name":"stdout","text":"Collecting inflection\n  Downloading inflection-0.5.1-py2.py3-none-any.whl.metadata (1.7 kB)\nDownloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\nInstalling collected packages: inflection\nSuccessfully installed inflection-0.5.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')\nimport pickle\nfrom google.colab import drive\nimport csv\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport re\nimport string\nfrom nltk.stem.porter import PorterStemmer","metadata":{"id":"P9emxprnStnP","outputId":"51497efe-74ae-4abd-d5a0-485188dab6f7","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:24.873235Z","iopub.execute_input":"2025-04-13T16:54:24.873911Z","iopub.status.idle":"2025-04-13T16:54:24.947940Z","shell.execute_reply.started":"2025-04-13T16:54:24.873885Z","shell.execute_reply":"2025-04-13T16:54:24.946932Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Load dữ liệu","metadata":{"id":"SC6LbKkWy8kR"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# Đường dẫn đến các file pickle\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_src_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_src_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_src_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_src_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_src_processed.pkl'\n}\n\n# Load từng file và lưu vào các biến tương ứng\ndatasets = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        datasets[name] = pickle.load(f)\n\n# Kiểm tra dữ liệu đã được load vào các biến\nfor name, data in datasets.items():\n    print(f\"Data for {name}:\")\n","metadata":{"id":"O3TGVN1KzAXg","outputId":"63f347a7-1b8d-41fe-98e5-105ca43233e2","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:24.948966Z","iopub.execute_input":"2025-04-13T16:54:24.949227Z","iopub.status.idle":"2025-04-13T16:54:41.001764Z","shell.execute_reply.started":"2025-04-13T16:54:24.949206Z","shell.execute_reply":"2025-04-13T16:54:41.000802Z"}},"outputs":[{"name":"stdout","text":"Data for aspectj:\nData for eclipse:\nData for swt:\nData for tomcat:\nData for birt:\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"eclipse_src = datasets['eclipse']\nbirt_src = datasets['birt']\nswt_src = datasets['swt']\ntomcat_src = datasets['tomcat']\naspectj_src = datasets['aspectj']","metadata":{"id":"b91231aHzUu_","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:41.002722Z","iopub.execute_input":"2025-04-13T16:54:41.002969Z","iopub.status.idle":"2025-04-13T16:54:41.007515Z","shell.execute_reply.started":"2025-04-13T16:54:41.002949Z","shell.execute_reply":"2025-04-13T16:54:41.006704Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load dữ liệu từ các file pickle đã lưu\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_reports_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_reports_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_reports_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_reports_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_reports_processed.pkl'\n}\n\n# Load từng dataset và lưu vào các biến\nall_processed_reports = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        all_processed_reports[name] = pickle.load(f)\n\n# Kiểm tra dữ liệu đã load vào\nfor dataset, reports in all_processed_reports.items():\n    print(f\"Processed reports for {dataset}:\")","metadata":{"id":"kXiYZuNgzv0M","outputId":"053bc2e9-ef09-4aea-9431-acfda03d3300","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:41.010241Z","iopub.execute_input":"2025-04-13T16:54:41.010541Z","iopub.status.idle":"2025-04-13T16:54:45.774181Z","shell.execute_reply.started":"2025-04-13T16:54:41.010515Z","shell.execute_reply":"2025-04-13T16:54:45.773207Z"}},"outputs":[{"name":"stdout","text":"Processed reports for aspectj:\nProcessed reports for eclipse:\nProcessed reports for swt:\nProcessed reports for tomcat:\nProcessed reports for birt:\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"eclipse_reports = all_processed_reports['eclipse']\nbirt_reports = all_processed_reports['birt']\nswt_reports = all_processed_reports['swt']\ntomcat_reports = all_processed_reports['tomcat']\naspectj_reports = all_processed_reports['aspectj']","metadata":{"id":"iAUv0Bnv0FcI","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:45.775190Z","iopub.execute_input":"2025-04-13T16:54:45.775483Z","iopub.status.idle":"2025-04-13T16:54:45.780097Z","shell.execute_reply.started":"2025-04-13T16:54:45.775459Z","shell.execute_reply":"2025-04-13T16:54:45.779443Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 2. Xử lí data, gán nhãn\n- Sắp xếp bug report theo thời gian (report_time)\n- Chia thành 10 folds\n- Tạo training/test dataset theo kiểu fold i → fold i+1\n- Gán nhãn cho từng cặp (bug report, source file)","metadata":{"id":"-hwWTIRJ9PXd"}},{"cell_type":"code","source":"# B1: Lấy danh sách (bug_id, bug_report), sau đó sắp xếp theo report_time\nsorted_bug_reports = sorted(aspectj_reports.items(), key=lambda x: x[1].report_time)\ndata_src = aspectj_src","metadata":{"id":"HrKZiEgO9X16","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:45.781009Z","iopub.execute_input":"2025-04-13T16:54:45.781318Z","iopub.status.idle":"2025-04-13T16:54:45.798156Z","shell.execute_reply.started":"2025-04-13T16:54:45.781293Z","shell.execute_reply":"2025-04-13T16:54:45.797389Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def split_into_folds(sorted_reports, num_folds=10):\n    fold_size = len(sorted_reports) // num_folds\n    folds = [sorted_reports[i*fold_size:(i+1)*fold_size] for i in range(num_folds)]\n\n    # Nếu còn dư, rải đều vào các fold đầu\n    remainder = sorted_reports[num_folds*fold_size:]\n    for i, extra in enumerate(remainder):\n        folds[i].append(extra)\n    return folds\n\ndata_folds = split_into_folds(sorted_bug_reports, num_folds=3)\n","metadata":{"id":"fgRpuQKE9aA2","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:45.799172Z","iopub.execute_input":"2025-04-13T16:54:45.799506Z","iopub.status.idle":"2025-04-13T16:54:45.817328Z","shell.execute_reply.started":"2025-04-13T16:54:45.799482Z","shell.execute_reply":"2025-04-13T16:54:45.816441Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"i = 0 # thử với fold 0 → 1\ntrain_fold = data_folds[i]\ntest_fold = data_folds[i+1]","metadata":{"id":"0itPPQ5O9cDT","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:55:55.754000Z","iopub.execute_input":"2025-04-13T16:55:55.754433Z","iopub.status.idle":"2025-04-13T16:55:55.760252Z","shell.execute_reply.started":"2025-04-13T16:55:55.754404Z","shell.execute_reply":"2025-04-13T16:55:55.758989Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\ndef generate_balanced_pairs(bug_fold, source_files, num_negatives_per_positive=50):\n    data = []\n    for bug_id, bug in bug_fold:\n        # Danh sách file chứa bug (poszqitive)\n        positive_paths = set(bug.fixed_files)\n        positive = [\n            (bug_id, bug, src_path, source_files[src_path], 1)\n            for src_path in positive_paths if src_path in source_files\n        ]\n\n        # Danh sách file còn lại để lấy negative\n        all_paths = list(source_files.keys())\n        negative_paths = list(set(all_paths) - positive_paths)\n        sampled_negatives = random.sample(negative_paths, min(num_negatives_per_positive * len(positive), len(negative_paths)))\n\n        negative = [\n            (bug_id, bug, src_path, source_files[src_path], 0)\n            for src_path in sampled_negatives if src_path in source_files\n        ]\n\n        data.extend(positive + negative)\n    return data\ndef generate_all_negatives_pairs(bug_fold, source_files):\n    data = []\n    for bug_id, bug in bug_fold:\n        positive_paths = set(bug.fixed_files)\n        positive = [\n            (bug_id, bug, src_path, source_files[src_path], 1)\n            for src_path in positive_paths if src_path in source_files\n        ]\n\n        all_paths = list(source_files.keys())\n        negative_paths = list(set(all_paths) - positive_paths)\n\n        negative = [\n            (bug_id, bug, src_path, source_files[src_path], 0)\n            for src_path in negative_paths if src_path in source_files\n        ]\n\n        data.extend(positive + negative)\n    return data\n\n\ntrain_pairs = generate_balanced_pairs(train_fold, data_src, num_negatives_per_positive=50)\n#test_pairs = generate_balanced_pairs(test_fold, data_src, num_negatives_per_positive=50)\ntest_pairs = generate_all_negatives_pairs(test_fold, data_src)\n\n\n","metadata":{"id":"wjLEL9mR9fPv","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:55:55.762179Z","iopub.execute_input":"2025-04-13T16:55:55.762519Z","iopub.status.idle":"2025-04-13T16:56:01.489192Z","shell.execute_reply.started":"2025-04-13T16:55:55.762496Z","shell.execute_reply":"2025-04-13T16:56:01.488275Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"Xử lí mất cân bằng","metadata":{"id":"phVI5NPj-GMs"}},{"cell_type":"code","source":"def compute_stats(pairs):\n    total = len(pairs)\n    pos = sum(1 for _, _, _, _, label in pairs if label == 1)\n    neg = total - pos\n    ratio = pos / total if total > 0 else 0\n    return total, pos, neg, ratio\n\n  \ntotal, pos, neg, ratio = compute_stats(train_pairs)\nprint(\"📊 Train Set:\")\nprint(f\"  ➤ Tổng cặp: {total}\")\nprint(f\"  ✅ Positive (label=1): {pos}\")\nprint(f\"  ❌ Negative (label=0): {neg}\")\nprint(f\"  ⚖️ Tỷ lệ positive: {ratio:.4f}\")\n\ntotal, pos, neg, ratio = compute_stats(test_pairs)\nprint(\"\\n🧪 Test Set:\")\nprint(f\"  ➤ Tổng cặp: {total}\")\nprint(f\"  ✅ Positive (label=1): {pos}\")\nprint(f\"  ❌ Negative (label=0): {neg}\")\nprint(f\"  ⚖️ Tỷ lệ positive: {ratio:.4f}\")\n","metadata":{"id":"G-m4H5d79uzp","outputId":"1a4a413d-189b-4b5d-ca55-6552e56ce33f","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:01.490161Z","iopub.execute_input":"2025-04-13T16:56:01.490606Z","iopub.status.idle":"2025-04-13T16:56:01.608236Z","shell.execute_reply.started":"2025-04-13T16:56:01.490579Z","shell.execute_reply":"2025-04-13T16:56:01.607294Z"}},"outputs":[{"name":"stdout","text":"📊 Train Set:\n  ➤ Tổng cặp: 8619\n  ✅ Positive (label=1): 169\n  ❌ Negative (label=0): 8450\n  ⚖️ Tỷ lệ positive: 0.0196\n\n🧪 Test Set:\n  ➤ Tổng cặp: 1368180\n  ✅ Positive (label=1): 168\n  ❌ Negative (label=0): 1368012\n  ⚖️ Tỷ lệ positive: 0.0001\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### Hàm 1: Tạo batches có bootstrapping (luôn chứa ít nhất 1 positive sample)","metadata":{}},{"cell_type":"code","source":"def create_bootstrapped_batches(pairs, batch_size=128, pos_ratio=0.1):\n    positives = [p for p in pairs if p[-1] == 1]\n    negatives = [p for p in pairs if p[-1] == 0]\n\n    pos_per_batch = max(1, int(batch_size * pos_ratio))\n    neg_per_batch = batch_size - pos_per_batch\n\n    random.shuffle(positives)\n    random.shuffle(negatives)\n\n    batches = []\n    pos_idx, neg_idx = 0, 0\n\n    while neg_idx + neg_per_batch <= len(negatives):\n        pos_batch = []\n        for _ in range(pos_per_batch):\n            pos_batch.append(positives[pos_idx % len(positives)])\n            pos_idx += 1\n\n        neg_batch = negatives[neg_idx:neg_idx + neg_per_batch]\n        neg_idx += neg_per_batch\n\n        batch = pos_batch + neg_batch\n        random.shuffle(batch)\n        batches.append(batch)\n\n    return batches\n","metadata":{"id":"VVi_cbQD-a1b","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:01.610589Z","iopub.execute_input":"2025-04-13T16:56:01.610848Z","iopub.status.idle":"2025-04-13T16:56:01.618424Z","shell.execute_reply.started":"2025-04-13T16:56:01.610828Z","shell.execute_reply":"2025-04-13T16:56:01.617463Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### Focal Loss Function","metadata":{}},{"cell_type":"code","source":"\ndef focal_loss(predictions, targets, alpha=0.999, gamma=2.0, eps=1e-6):\n    \"\"\"\n    predictions: tensor (batch_size,) - output sigmoid from model\n    targets: tensor (batch_size,) - true labels (0 or 1)\n    \"\"\"\n    # Avoid log(0)\n    predictions = predictions.clamp(min=eps, max=1.0 - eps)\n\n    # Compute focal loss\n    loss = -alpha * (1 - predictions)**gamma * targets * predictions.log() \\\n           - (1 - alpha) * predictions**gamma * (1 - targets) * (1 - predictions).log()\n    return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:01.619219Z","iopub.execute_input":"2025-04-13T16:56:01.619523Z","iopub.status.idle":"2025-04-13T16:56:01.635480Z","shell.execute_reply.started":"2025-04-13T16:56:01.619503Z","shell.execute_reply":"2025-04-13T16:56:01.634584Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# 4. Trích xuất đặc trưng","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Hàm xử lý text gộp lại từ bug report\ndef bug_to_text(bug):\n    summary = bug.summary['unstemmed'] if isinstance(bug.summary, dict) else bug.summary\n    desc = bug.description['unstemmed'] if isinstance(bug.description, dict) else bug.description\n    return \" \".join(summary + desc)\n\n# Hàm xử lý text từ source file\ndef src_to_text(src):\n    content = src.all_content['unstemmed'] if isinstance(src.all_content, dict) else src.all_content\n    comments = src.comments['unstemmed'] if isinstance(src.comments, dict) else src.comments\n    return \" \".join(content + comments)\n","metadata":{"id":"4en89sb6-s54","outputId":"5c14ad2a-da00-495a-ec81-38687265bcfe","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:01.636316Z","iopub.execute_input":"2025-04-13T16:56:01.636722Z","iopub.status.idle":"2025-04-13T16:56:01.648852Z","shell.execute_reply.started":"2025-04-13T16:56:01.636690Z","shell.execute_reply":"2025-04-13T16:56:01.647913Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Đặc trưng 1: Tính toán độ tương đồng từ vựng (lexical similarity)\n- Phương pháp: sử dụng TF-IDF và cosine similarity.\n- Input: Cặp dữ liệu (bug report, source file)\n- Output: mảng numpy chứa các giá trị độ tương đồng cosine giữa bug report và source file cho mỗi cặp.","metadata":{}},{"cell_type":"code","source":"def compute_lexical_similarity(pairs):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # Gộp cả bug + src lại để fit chung vectorizer\n    combined = bug_texts + src_texts\n    tfidf = TfidfVectorizer()\n    tfidf_matrix = tfidf.fit_transform(combined)\n\n    # Tách riêng lại từng phần\n    bug_vecs = tfidf_matrix[:len(pairs)]\n    src_vecs = tfidf_matrix[len(pairs):]\n\n    # Tính cosine cho từng cặp (theo hàng tương ứng)\n    similarities = cosine_similarity(bug_vecs, src_vecs).diagonal()\n\n    return similarities\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:01.649947Z","iopub.execute_input":"2025-04-13T16:56:01.650212Z","iopub.status.idle":"2025-04-13T16:56:01.672533Z","shell.execute_reply.started":"2025-04-13T16:56:01.650193Z","shell.execute_reply":"2025-04-13T16:56:01.671472Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"glove_path = \"/kaggle/input/glove-embedding/glove.6B.100d.txt\"\n# Load GloVe 100d vào dictionary\nimport numpy as np\n\ndef load_glove_embeddings(filepath):\n    embeddings = {}\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.strip().split()\n            word = values[0]\n            vector = np.array(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n    \nglove_embeddings = load_glove_embeddings(glove_path)","metadata":{"id":"6FexoFrK_LTE","outputId":"b7e7b200-954e-4541-a27b-dc43607ae8f2","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:01.673542Z","iopub.execute_input":"2025-04-13T16:56:01.673865Z","iopub.status.idle":"2025-04-13T16:56:13.594800Z","shell.execute_reply.started":"2025-04-13T16:56:01.673837Z","shell.execute_reply":"2025-04-13T16:56:13.594063Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Đặc trưng 2: Tính toán độ tương đồng ngữ nghĩa (semantic similarity)\n- Phương pháp: TF-IDF weighted average của GloVe vectors và cosine similarity\n- Input:  (bug report, source file).\n- Output: Một mảng numpy chứa các giá trị độ tương đồng cosine giữa bug report và source file cho mỗi cặp, dựa trên GloVe vectors và trọng số TF-IDF.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef compute_semantic_similarity(pairs, glove_dict, dim=100):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # Dùng TF-IDF để lấy trọng số từ\n    tfidf = TfidfVectorizer()\n    tfidf.fit(bug_texts + src_texts)\n    vocab = tfidf.vocabulary_\n    idf_weights = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n\n    def embed_text(text):\n        tokens = text.split()\n        vecs = []\n        weights = []\n        for token in tokens:\n            if token in glove_dict and token in vocab:\n                vecs.append(glove_dict[token])\n                weights.append(idf_weights[token])\n        if not vecs:\n            return np.zeros(dim)\n        vecs = np.array(vecs)\n        weights = np.array(weights).reshape(-1, 1)\n        weighted_vecs = vecs * weights\n        return weighted_vecs.sum(axis=0) / weights.sum()\n\n    # Tính vector trung bình cho bug và src\n    bug_vecs = [embed_text(text) for text in bug_texts]\n    src_vecs = [embed_text(text) for text in src_texts]\n\n    # Tính cosine similarity giữa từng cặp\n    similarities = [cosine_similarity([b], [s])[0][0] for b, s in zip(bug_vecs, src_vecs)]\n\n    return np.array(similarities)\n","metadata":{"id":"BmKfMQ9oAFSQ","outputId":"e0bf60b5-8bc7-401f-c8ee-5e8ac04ebe01","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:13.595668Z","iopub.execute_input":"2025-04-13T16:56:13.595971Z","iopub.status.idle":"2025-04-13T16:56:13.604727Z","shell.execute_reply.started":"2025-04-13T16:56:13.595945Z","shell.execute_reply":"2025-04-13T16:56:13.603896Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Đặc trưng 3: Similar Bug Report Score ","metadata":{}},{"cell_type":"markdown","source":"→ Kiểm tra xem bug report này có giống **những bug report cũ từng sửa cùng file đó** không?\n\n- `build_bug_fix_history(pairs)` → XD lịch sử chỉnh sửa theo từng file\n- `compute_similar_bug_score(pairs, history)`\n    - Input: pairs, history\n    - So sánh bug hiện tại và bug cũ:\n    \n    cosine_similarity(TfidfVectorizer().fit_transform([bug_now, bug_old]))[0, 1]\n    \n    - Lấy giá trị tương đồng cao nhất vừa tìm được","metadata":{}},{"cell_type":"code","source":"def build_bug_fix_history(pairs):\n    history = {}\n    for bug_id, bug, src_path, _, label in pairs:\n        if label == 1:  # chỉ tính các bug thật sự sửa file\n            if src_path not in history:\n                history[src_path] = []\n            history[src_path].append((bug_id, bug.report_time, bug_to_text(bug)))\n    return history\n\n# Đặc trưng 3: Similar Bug Report Score\ndef compute_similar_bug_score(pairs, history):\n    scores = []\n    for bug_id, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        current_text = bug_to_text(bug)\n\n        sim_scores = []\n        if src_path in history:\n            for hist_bug_id, hist_time, hist_text in history[src_path]:\n                if hist_time < current_time:  # chỉ tính bug trong quá khứ\n                    sim = cosine_similarity(\n                        TfidfVectorizer().fit_transform([current_text, hist_text])\n                    )[0, 1]\n                    sim_scores.append(sim)\n        scores.append(max(sim_scores) if sim_scores else 0.0)\n    return np.array(scores)","metadata":{"id":"imb4_du_Bgjz","outputId":"8c08c8e9-7047-4e5d-de85-8b70e30cbe28","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:13.607967Z","iopub.execute_input":"2025-04-13T16:56:13.608549Z","iopub.status.idle":"2025-04-13T16:56:13.632150Z","shell.execute_reply.started":"2025-04-13T16:56:13.608520Z","shell.execute_reply":"2025-04-13T16:56:13.631247Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Đặc trung 4: Time Since Last Fix (ngày, normalize)\n- Kiểm tra với mỗi `(bug report, source file)` xem từng được sửa trước đó không và lần cuối khi nào\n    - Đã lâu k sửa → Ít lỗi → Điểm thấp\n    - Mới sửa → có thể liên quan tới lỗi → Điểm cao\n- Cách hđ:\n    - Tìm thời điểm bug current_time\n    - Tìm history các lần sửa file trong quá khứ\n    - Tính khoảng cách time giữa current và history gần nhất\n    - Chưa sửa → Gán số delta_days=9999\n    - Chuẩn hoá","metadata":{}},{"cell_type":"code","source":"# Đặc trưng 4: Time Since Last Fix (ngày, normalize)\ndef compute_time_since_last_fix(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_times = [hist_time for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            if past_times:\n                delta_days = (current_time - max(past_times)).days\n            else:\n                delta_days = 9999  # Cực lớn nếu chưa từng sửa\n        else:\n            delta_days = 9999\n        scores.append(delta_days)\n\n    # Normalize về [0,1]\n    max_days = max(scores) if max(scores) != 0 else 1  # Tránh chia cho 0\n\n    return np.array([1 - (s / max_days) for s in scores])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:13.633154Z","iopub.execute_input":"2025-04-13T16:56:13.633803Z","iopub.status.idle":"2025-04-13T16:56:13.656387Z","shell.execute_reply.started":"2025-04-13T16:56:13.633781Z","shell.execute_reply":"2025-04-13T16:56:13.655537Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"### Đặc trưng 5: Fix Frequency (số lần bị sửa trong quá khứ, normalize)\n\n\n- Kiểm tra xme mỗi cặp được sửa bao nhiêu lần\n\n→ Sửa nhiều → File dễ dính lỗi → Điểm cao","metadata":{}},{"cell_type":"code","source":"# Đặc trưng 5: Fix Frequency (số lần bị sửa trong quá khứ, normalize)\ndef compute_fix_frequency(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_fixes = [1 for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            freq = len(past_fixes)\n        else:\n            freq = 0\n        scores.append(freq)\n    # Normalize về [0,1] an toàn\n    max_freq = max(scores)\n    max_freq = max(max_freq, 1)  # tránh chia 0\n    return np.array([s / max_freq for s in scores])\n\n\n# Dùng cho 500 cặp mẫu\nsampled_pairs = train_pairs[:5000]\nbug_history = build_bug_fix_history(train_pairs)\n\nsimilar_bug_score = compute_similar_bug_score(sampled_pairs, bug_history)\ntime_since_last_fix = compute_time_since_last_fix(sampled_pairs, bug_history)\nfix_frequency = compute_fix_frequency(sampled_pairs, bug_history)\n\n# Trích 5 giá trị đầu mỗi feature\nsimilar_bug_score[:50], time_since_last_fix[:50], fix_frequency[:50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:13.657172Z","iopub.execute_input":"2025-04-13T16:56:13.657465Z","iopub.status.idle":"2025-04-13T16:56:13.816496Z","shell.execute_reply.started":"2025-04-13T16:56:13.657444Z","shell.execute_reply":"2025-04-13T16:56:13.815723Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"def min_max_normalize(values):\n    min_val = min(values)\n    max_val = max(values)\n    denom = max_val - min_val if max_val != min_val else 1\n    return [(v - min_val) / denom for v in values]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:13.817283Z","iopub.execute_input":"2025-04-13T16:56:13.817631Z","iopub.status.idle":"2025-04-13T16:56:13.822871Z","shell.execute_reply.started":"2025-04-13T16:56:13.817606Z","shell.execute_reply":"2025-04-13T16:56:13.822019Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"Đặc trưng 6: Ngữ nghĩa học sâu từ CNN encoder","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n# CNN encoder dùng cho cả bug và source\nclass CNNEncoder(nn.Module):\n    def __init__(self, embed_dim=100, num_filters=64, kernel_sizes=(3, 5), dropout=0.5):\n        super(CNNEncoder, self).__init__()\n        self.convs = nn.ModuleList([\n            nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=k)\n            for k in kernel_sizes\n        ])\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):  # x: (batch_size, seq_len, embed_dim)\n        x = x.permute(0, 2, 1)  # (batch, embed_dim, seq_len)\n        conv_outs = [F.relu(conv(x)) for conv in self.convs]\n        pooled = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in conv_outs]\n        out = torch.cat(pooled, dim=1)\n        return self.dropout(out)  # shape: (batch_size, num_filters * len(kernel_sizes))\n\n# Token list → batch embedding tensor\ndef batch_tokens_to_embeddings(batch_tokens, glove_dict, dim=100, max_len=100):\n    batch_embeddings = []\n    for tokens in batch_tokens:\n        emb = []\n        for token in tokens[:max_len]:\n            if token in glove_dict:\n                emb.append(glove_dict[token])\n            else:\n                emb.append(np.zeros(dim))\n        while len(emb) < max_len:\n            emb.append(np.zeros(dim))\n        batch_embeddings.append(emb)\n    return torch.tensor(np.array(batch_embeddings), dtype=torch.float32)\n\n# 🆕 CNN feature extraction – batch, nhanh, GPU, normalized\ndef extract_cnn_features_batch(pairs, glove_dict, bug_encoder, src_encoder, device=\"cuda\", dim=100, max_len=100):\n    bug_encoder = bug_encoder.to(device)\n    src_encoder = src_encoder.to(device)\n    bug_encoder.eval()\n    src_encoder.eval()\n\n    bug_token_list = []\n    src_token_list = []\n\n    for _, bug, _, src, _ in pairs:\n        bug_tokens = bug.summary['stemmed'] + bug.description['stemmed']\n        src_tokens = src.all_content['stemmed'] + src.comments['stemmed'] + \\\n                     src.class_names['stemmed'] + src.method_names['stemmed']\n        bug_token_list.append(bug_tokens)\n        src_token_list.append(src_tokens)\n\n    # Embed → tensor\n    bug_tensor = batch_tokens_to_embeddings(bug_token_list, glove_dict, dim, max_len).to(device)\n    src_tensor = batch_tokens_to_embeddings(src_token_list, glove_dict, dim, max_len).to(device)\n\n    # Forward CNN\n    with torch.no_grad():\n        bug_tensor = bug_tensor.to(device)\n        src_tensor = src_tensor.to(device)\n    \n        bug_vec = bug_encoder(bug_tensor)\n        src_vec = src_encoder(src_tensor)\n    \n        combined = torch.cat([bug_vec, src_vec], dim=1).cpu().numpy()  # chuyển về CPU để convert sang NumPy\n\n\n    # Normalize từng chiều về [0, 1]\n    min_vals = combined.min(axis=0)\n    max_vals = combined.max(axis=0)\n    denom = np.where(max_vals - min_vals == 0, 1, max_vals - min_vals)\n    normalized = (combined - min_vals) / denom\n\n    return normalized  # shape: (n_samples, 2*filters)\nimport torch\n\ndef extract_cnn_features_batch(pairs, glove_dict, bug_encoder, src_encoder, device=None, dim=100, max_len=100):\n    # Tự động chọn thiết bị: GPU nếu có, không thì CPU\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    bug_encoder = bug_encoder.to(device)\n    src_encoder = src_encoder.to(device)\n    bug_encoder.eval()\n    src_encoder.eval()\n\n    bug_token_list = []\n    src_token_list = []\n\n    for _, bug, _, src, _ in pairs:\n        bug_tokens = bug.summary['stemmed'] + bug.description['stemmed']\n        src_tokens = src.all_content['stemmed'] + src.comments['stemmed'] + \\\n                     src.class_names['stemmed'] + src.method_names['stemmed']\n        bug_token_list.append(bug_tokens)\n        src_token_list.append(src_tokens)\n\n    # Embed → tensor\n    bug_tensor = batch_tokens_to_embeddings(bug_token_list, glove_dict, dim, max_len).to(device)\n    src_tensor = batch_tokens_to_embeddings(src_token_list, glove_dict, dim, max_len).to(device)\n\n    # Forward CNN\n    with torch.no_grad():\n        bug_vec = bug_encoder(bug_tensor)\n        src_vec = src_encoder(src_tensor)\n        combined = torch.cat([bug_vec, src_vec], dim=1).cpu().numpy()\n\n    # Normalize từng chiều về [0, 1]\n    min_vals = combined.min(axis=0)\n    max_vals = combined.max(axis=0)\n    denom = np.where(max_vals - min_vals == 0, 1, max_vals - min_vals)\n    normalized = (combined - min_vals) / denom\n\n    return normalized  # shape: (n_samples, 2 * filters)\n\n","metadata":{"id":"PmEZSTiyDEB4","outputId":"6db11041-f3a2-4931-f264-9480bfd7c2e6","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:04:26.414911Z","iopub.execute_input":"2025-04-13T17:04:26.415240Z","iopub.status.idle":"2025-04-13T17:04:26.435694Z","shell.execute_reply.started":"2025-04-13T17:04:26.415215Z","shell.execute_reply":"2025-04-13T17:04:26.434715Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"bug_encoder = CNNEncoder()\nsrc_encoder = CNNEncoder()\nglove_dict=glove_embeddings\ncnn_combined_vector = extract_cnn_features_batch(train_pairs[:5000], glove_dict, bug_encoder, src_encoder)\nprint(\"✅ CNN đặc trưng đã chuẩn hóa:\", cnn_combined_vector.shape)\nprint(cnn_combined_vector[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:04:28.778327Z","iopub.execute_input":"2025-04-13T17:04:28.779274Z","iopub.status.idle":"2025-04-13T17:04:33.889912Z","shell.execute_reply.started":"2025-04-13T17:04:28.779240Z","shell.execute_reply":"2025-04-13T17:04:33.888913Z"}},"outputs":[{"name":"stdout","text":"✅ CNN đặc trưng đã chuẩn hóa: (5000, 256)\n[[0.24267694 0.22376536 0.6680607  ... 0.781797   0.36746198 0.62862104]\n [0.24267694 0.22376536 0.6680607  ... 0.44357866 0.35221133 0.74948055]\n [0.24267694 0.22376536 0.6680607  ... 0.3792844  0.32195666 0.60165375]\n ...\n [0.24267694 0.22376536 0.6680607  ... 0.17033355 0.07380829 0.04349329]\n [0.24267694 0.22376536 0.6680607  ... 0.33772215 0.         0.2134291 ]\n [0.24267694 0.22376536 0.6680607  ... 0.60519177 0.39244118 0.36244106]]\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"ĐT7: identifier_overlap_count – Số tên hàm/biến trùng với từ trong bug report","metadata":{}},{"cell_type":"code","source":"def compute_identifier_overlap_count(pairs):\n    counts = []\n    for _, bug, _, src, _ in pairs:\n        bug_tokens = set(bug.summary['stemmed'] + bug.description['stemmed'])\n        identifiers = set(\n            src.class_names['stemmed'] + src.method_names['stemmed'] + src.variables['stemmed']\n        )\n        overlap = bug_tokens & identifiers\n        counts.append(len(overlap))\n    counts = np.array(counts)\n    # 🔄 Log normalization về [0,1]\n    return np.log1p(counts) / np.log1p(np.max(counts)) if counts.max() > 0 else np.zeros_like(counts)\n\n\ndef compute_shared_token_ratio(pairs):\n    ratios = []\n    for _, bug, _, src, _ in pairs:\n        bug_tokens = set(bug.summary['stemmed'] + bug.description['stemmed'])\n        src_tokens = set(\n            src.all_content['stemmed'] + src.comments['stemmed'] + \n            src.class_names['stemmed'] + src.method_names['stemmed']\n        )\n        if not src_tokens:\n            ratios.append(0.0)\n        else:\n            ratios.append(len(bug_tokens & src_tokens) / len(src_tokens))\n    ratios = np.array(ratios)\n    # 🔄 Min-max normalization về [0,1]\n    return (ratios - ratios.min()) / (ratios.max() - ratios.min()) if ratios.max() > ratios.min() else np.zeros_like(ratios)\n\nsampled_pairs = train_pairs[:5000]\nidf_overlap = compute_identifier_overlap_count(sampled_pairs)\nshared_ratio = compute_shared_token_ratio(sampled_pairs)\n\nprint(\"✅ identifier_overlap_count:\", idf_overlap[:10])\nprint(\"✅ shared_token_ratio:\", shared_ratio[:10])\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:59:39.950671Z","iopub.execute_input":"2025-04-13T16:59:39.950987Z","iopub.status.idle":"2025-04-13T16:59:40.152230Z","shell.execute_reply.started":"2025-04-13T16:59:39.950966Z","shell.execute_reply":"2025-04-13T16:59:40.151442Z"}},"outputs":[{"name":"stdout","text":"✅ identifier_overlap_count: [0.42061984 0.         0.         0.         0.         0.\n 0.         0.         0.         0.        ]\n✅ shared_token_ratio: [0.00472441 0.00925926 0.03773585 0.         0.         0.\n 0.02222222 0.14285714 0.         0.01960784]\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# 4. Quá trình huấn luyện","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Tạo ma trận train, test","metadata":{}},{"cell_type":"code","source":"def build_feature_matrix_batch(pairs_batch, glove_dict, bug_encoder, src_encoder, history, device=\"cuda\"):\n    # Đặc trưng vector thường (1 chiều)\n    lexical = compute_lexical_similarity(pairs_batch)                         # (N,)\n    semantic = compute_semantic_similarity(pairs_batch, glove_dict)          # (N,)\n    idf_overlap = compute_identifier_overlap_count(pairs_batch)              # (N,)\n    shared_ratio = compute_shared_token_ratio(pairs_batch)                   # (N,)\n    \n    # Đặc trưng dựa vào lịch sử sửa lỗi\n    similar_score = compute_similar_bug_score(pairs_batch, history)\n    recency = compute_time_since_last_fix(pairs_batch, history)\n    freq = compute_fix_frequency(pairs_batch, history)\n    # Đặc trưng học sâu (N, 256)\n    cnn_vec = extract_cnn_features_batch(pairs_batch, glove_dict, bug_encoder, src_encoder)\n\n    # Ghép toàn bộ lại\n    X = np.hstack([\n        lexical.reshape(-1, 1),         \n        semantic.reshape(-1, 1),        \n        idf_overlap.reshape(-1, 1),     \n        shared_ratio.reshape(-1, 1),    \n        similar_score.reshape(-1, 1),\n        recency.reshape(-1, 1),\n        freq.reshape(-1, 1),\n        cnn_vec                         \n    ])  # → Tổng cộng (N, 263)\n\n    return X\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:59:40.153694Z","iopub.execute_input":"2025-04-13T16:59:40.154014Z","iopub.status.idle":"2025-04-13T16:59:40.160357Z","shell.execute_reply.started":"2025-04-13T16:59:40.153994Z","shell.execute_reply":"2025-04-13T16:59:40.159481Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Tạo nhãn y\ndef get_labels(pairs):\n    return np.array([label for *_, label in pairs])\nglove_dict = glove_embeddings\nX_train = build_feature_matrix_batch(train_pairs, glove_dict, bug_encoder, src_encoder,bug_history, device=device)\ny_train = get_labels(train_pairs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:59:40.161185Z","iopub.execute_input":"2025-04-13T16:59:40.161523Z","iopub.status.idle":"2025-04-13T17:00:04.637915Z","shell.execute_reply.started":"2025-04-13T16:59:40.161501Z","shell.execute_reply":"2025-04-13T17:00:04.636849Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def pair_generator(pairs, batch_size, glove_dict, history, bug_encoder, src_encoder, device=\"cuda\"):\n    for i in range(0, len(pairs), batch_size):\n        batch = pairs[i:i + batch_size]\n        X = build_feature_matrix_batch(batch, glove_dict, bug_encoder, src_encoder, history, device)\n        y = np.array([label for *_, label in batch])\n\n        yield torch.tensor(X, dtype=torch.float32).to(device), torch.tensor(y, dtype=torch.float32).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:04.639028Z","iopub.execute_input":"2025-04-13T17:00:04.639419Z","iopub.status.idle":"2025-04-13T17:00:04.646446Z","shell.execute_reply.started":"2025-04-13T17:00:04.639367Z","shell.execute_reply":"2025-04-13T17:00:04.645287Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"print(\"✅ X_train shape:\", X_train.shape)  # (5000, 260)\nprint(\"✅ y_train shape:\", y_train.shape)  # (5000,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:04.649018Z","iopub.execute_input":"2025-04-13T17:00:04.649322Z","iopub.status.idle":"2025-04-13T17:00:04.673477Z","shell.execute_reply.started":"2025-04-13T17:00:04.649299Z","shell.execute_reply":"2025-04-13T17:00:04.672236Z"}},"outputs":[{"name":"stdout","text":"✅ X_train shape: (8619, 263)\n✅ y_train shape: (8619,)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"## 4.2 Xây dựng mô hình","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\n# Định nghĩa mô hình DNN giống bài báo\nimport torch\nimport torch.nn as nn\n\nclass BugLocalization(nn.Module):\n    def __init__(self, input_dim=5, hidden_dim=64, output_dim=1):\n        super(BugLocalization, self).__init__()\n        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = x.to(next(self.parameters()).device)  # auto move to model's device\n        # Nếu x chỉ có 2 chiều (batch_size, input_dim), hãy thêm một chiều giả định (sequence_length=1)\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)  # Thêm một chiều giả định (batch_size, 1, input_dim)\n\n        # x shape: (batch_size, sequence_length, input_dim)\n        rnn_out, _ = self.rnn(x)\n        # Lấy output của phần cuối cùng trong chuỗi\n        final_rnn_out = rnn_out[:, -1, :]\n        out = torch.sigmoid(self.fc(final_rnn_out)).squeeze()\n        return out\n\n\n        \n# Định nghĩa focal loss\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.999, gamma=2.0, eps=1e-6):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, preds, targets):\n        preds = preds.clamp(min=self.eps, max=1. - self.eps)\n        loss = -self.alpha * (1 - preds) ** self.gamma * targets * torch.log(preds) \\\n               - (1 - self.alpha) * preds ** self.gamma * (1 - targets) * torch.log(1 - preds)\n        return loss.mean()\n\n\n# Huấn luyện mô hình\ndef train_model_generator(model, train_gen, epochs=10, lr=1e-3):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = FocalLoss()\n\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch_X, batch_y in train_gen:\n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n            # 🧹 Dọn bộ nhớ mỗi batch\n            del batch_X, batch_y, outputs, loss\n            torch.cuda.empty_cache()\n            import gc; gc.collect()\n\n        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n\n","metadata":{"id":"LiD_wJxjDeQ8","outputId":"9821aee2-a66e-480b-9f69-6e4d207c7ba6","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:04.674551Z","iopub.execute_input":"2025-04-13T17:00:04.674829Z","iopub.status.idle":"2025-04-13T17:00:04.698136Z","shell.execute_reply.started":"2025-04-13T17:00:04.674808Z","shell.execute_reply":"2025-04-13T17:00:04.697424Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for X_batch, y_batch in pair_generator(\n    train_pairs, batch_size=128,\n    glove_dict=glove_embeddings,\n    history=bug_history,\n    bug_encoder=bug_encoder,\n    src_encoder=src_encoder, \n    device=device\n):\n    print(\"👉 Feature shape:\", X_batch.shape)\n    print(\"👉 Label shape:\", y_batch.shape)\n    print(\"👉 Feature Sample [2]:\", X_batch[2].cpu().numpy())\n    print(\"👉 Label Sample [2]:\", y_batch[2].item())\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:04.699150Z","iopub.execute_input":"2025-04-13T17:00:04.699788Z","iopub.status.idle":"2025-04-13T17:00:05.019680Z","shell.execute_reply.started":"2025-04-13T17:00:04.699765Z","shell.execute_reply":"2025-04-13T17:00:05.018785Z"}},"outputs":[{"name":"stdout","text":"👉 Feature shape: torch.Size([128, 263])\n👉 Label shape: torch.Size([128])\n👉 Feature Sample [2]: [0.01822121 0.7729522  0.         0.03773585 0.         0.\n 0.         0.         0.35706815 0.         1.         0.\n 0.15033144 0.         0.         0.         1.         0.\n 0.         0.67332315 0.         1.         0.         0.\n 0.         0.33636105 0.         0.47858703 0.5355421  0.\n 0.         1.         0.         0.         0.         1.\n 0.24859448 0.2735264  0.5301865  0.         0.         0.20633534\n 0.         0.         0.67322123 1.         0.         0.\n 0.8869989  0.         0.         1.         0.         0.\n 0.         0.01430868 0.25331688 1.         0.         1.\n 0.         1.         1.         1.         0.9065167  0.4161309\n 1.         0.         0.         1.         0.2813771  0.\n 0.7264074  1.         0.         1.         0.         1.\n 0.         0.60931265 0.         0.         0.         0.\n 0.         1.         1.         1.         0.23088759 1.\n 1.         0.         0.         0.         0.04415921 0.7522205\n 0.         1.         1.         0.         0.         0.\n 0.56480056 0.398972   0.49734566 0.3783361  0.         0.6363016\n 0.         0.         1.         1.         1.         0.\n 0.40044025 0.71484035 1.         0.         0.         1.\n 1.         0.7163564  0.         1.         0.         0.\n 1.         0.1789143  0.45158234 1.         1.         0.05963381\n 1.         0.         0.5933989  0.51485306 0.46111912 0.4638457\n 0.60739154 0.6909374  0.49349794 0.628727   0.73529917 0.36106527\n 0.544204   0.8340344  0.49787864 0.6363471  0.51470834 0.6289383\n 0.6077393  0.55292505 0.24723157 0.64401144 0.6058025  0.76647747\n 0.60857743 0.24984217 0.7978555  0.7290899  0.57184845 0.64680374\n 0.6226768  0.6971908  0.6897723  0.64800054 0.89020085 0.5317565\n 0.81717384 0.56141466 0.37485832 0.5951407  0.353841   0.64287305\n 0.6886488  0.37852833 0.8189771  0.43412474 0.3301495  0.6486478\n 0.4713482  0.6825522  0.8065349  0.830497   0.55990624 0.71873045\n 0.8211372  0.49257255 0.73755014 0.5655598  0.63362336 0.7271205\n 0.7428144  0.80085766 0.49639818 0.787129   0.5390462  0.54907846\n 0.6952124  0.4519555  0.34433374 0.55609554 0.517255   0.4382771\n 0.5417443  0.7699088  0.6674922  0.7516097  0.59542507 0.49202988\n 0.5299655  0.49817666 0.6772146  0.81480646 0.5632339  0.59829646\n 0.34206966 0.5361576  0.42315313 0.58462405 0.28554794 0.54797953\n 0.7285614  0.79223615 0.51648116 0.8109658  0.75147605 0.61094797\n 0.60865074 0.6013054  0.62486017 0.54014087 0.59285    0.6621813\n 0.63416296 0.4786988  0.91014993 0.93317586 0.7516453  0.47362208\n 0.6826647  0.5564992  0.632934   0.45528594 0.84474623 0.445405\n 0.44342214 0.47197255 0.68387014 0.56199497 0.6083353  0.71401286\n 0.5763397  0.36333787 0.5003047  0.65904355 0.58153814 0.6633591\n 0.57462    1.         0.5284378  0.59554756 0.63285166]\n👉 Label Sample [2]: 0.0\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def test_pair_generator(pairs, batch_size, glove_dict, history, bug_encoder, src_encoder, device=\"cuda\"):\n    for i in range(0, len(pairs), batch_size):\n        batch = pairs[i:i + batch_size]\n        X = build_feature_matrix_batch(batch, glove_dict, bug_encoder, src_encoder, history, device)\n\n        yield torch.tensor(X, dtype=torch.float32, device=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:05.021356Z","iopub.execute_input":"2025-04-13T17:00:05.021762Z","iopub.status.idle":"2025-04-13T17:00:05.027961Z","shell.execute_reply.started":"2025-04-13T17:00:05.021739Z","shell.execute_reply":"2025-04-13T17:00:05.026941Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom sklearn.metrics import average_precision_score\nfrom collections import defaultdict\nimport numpy as np\n\n# Đánh giá các chỉ số (MAP, MRR, Top-k)\ndef compute_topk_accuracy(test_pairs, y_scores, k=10):\n    bug_to_scores = {}\n    for (bug_id, _, src_path, _, label), score in zip(test_pairs, y_scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    correct_at_k = 0\n    total = 0\n\n    for entries in bug_to_scores.values():\n        sorted_entries = sorted(entries, key=lambda x: x[0], reverse=True)\n        top_k = sorted_entries[:k]\n        if any(label == 1 for _, label in top_k):\n            correct_at_k += 1\n        total += 1\n\n    return correct_at_k / total if total > 0 else 0\n\n\n\ndef compute_MAP_per_bug(test_pairs, y_pred_probs):\n    # Gom nhãn và score theo bug_id\n    bug_to_ytrue = defaultdict(list)\n    bug_to_yscore = defaultdict(list)\n\n    for (bug_id, _, _, _, label), score in zip(test_pairs, y_pred_probs):\n        bug_to_ytrue[bug_id].append(label)\n        bug_to_yscore[bug_id].append(score)\n\n    # Tính AP cho từng bug, chỉ giữ bug có ít nhất 1 label = 1\n    ap_list = []\n    for bug_id in bug_to_ytrue:\n        y_true = np.array(bug_to_ytrue[bug_id])\n        y_score = np.array(bug_to_yscore[bug_id])\n\n        if np.sum(y_true) == 0:\n            continue  # bỏ qua bug không có file liên quan\n\n        ap = average_precision_score(y_true, y_score)\n        ap_list.append(ap)\n\n    # Tính MAP\n    MAP = np.mean(ap_list) if ap_list else 0.0\n    return MAP\n\n# MRR (Mean Reciprocal Rank)\ndef mean_reciprocal_rank(pairs, scores):\n    bug_to_scores = {}\n    for (bug_id, _, _, _, label), score in zip(pairs, scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    rr_sum = 0\n    count = 0\n    for bug_id, ranked in bug_to_scores.items():\n        ranked = sorted(ranked, key=lambda x: x[0], reverse=True)\n        for idx, (_, label) in enumerate(ranked):\n            if label == 1:\n                rr_sum += 1 / (idx + 1)\n                break\n        count += 1\n    return rr_sum / count if count > 0 else 0","metadata":{"id":"hvK-tE_MD4zq","outputId":"5444ce3e-4624-4f81-eb6b-b19a543b0d49","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:05.029065Z","iopub.execute_input":"2025-04-13T17:00:05.029490Z","iopub.status.idle":"2025-04-13T17:00:05.049095Z","shell.execute_reply.started":"2025-04-13T17:00:05.029466Z","shell.execute_reply":"2025-04-13T17:00:05.048284Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def build_feature_matrix_batch_large(\n    pairs, \n    glove_dict, \n    bug_encoder, \n    src_encoder, \n    history, \n    batch_size=20000, \n    device=\"cuda\"\n):\n    all_features = []\n\n    for i in range(0, len(pairs), batch_size):\n        sub_batch = pairs[i:i+batch_size]\n        \n        # 🔤 Các đặc trưng truyền thống\n        lexical = compute_lexical_similarity(sub_batch)\n        semantic = compute_semantic_similarity(sub_batch, glove_dict)\n        idf_overlap = compute_identifier_overlap_count(sub_batch)\n        shared_ratio = compute_shared_token_ratio(sub_batch)\n\n        # 🧠 Các đặc trưng từ lịch sử sửa lỗi\n        similar_score = compute_similar_bug_score(sub_batch, history)\n        recency = compute_time_since_last_fix(sub_batch, history)\n        freq = compute_fix_frequency(sub_batch, history)\n\n        # 🔍 Đặc trưng học sâu\n        cnn = extract_cnn_features_batch(sub_batch, glove_dict, bug_encoder, src_encoder)\n\n        # 🧩 Ghép toàn bộ đặc trưng\n        others = np.stack([\n            lexical, semantic, idf_overlap, shared_ratio, \n            similar_score, recency, freq\n        ], axis=1)\n\n        combined = np.concatenate([others, cnn], axis=1)\n        all_features.append(combined)\n\n        print(f\"✅ Done {i+len(sub_batch)}/{len(pairs)} samples\")\n\n    return np.vstack(all_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:04:49.693982Z","iopub.execute_input":"2025-04-13T17:04:49.694614Z","iopub.status.idle":"2025-04-13T17:04:49.702579Z","shell.execute_reply.started":"2025-04-13T17:04:49.694587Z","shell.execute_reply":"2025-04-13T17:04:49.701453Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def run_kfold_training_and_eval(\n    folds,\n    source_files,\n    glove_dict,\n    bug_encoder,\n    src_encoder,\n    model_class,\n    k=3,\n    device=\"cpu\",\n    cache_dir=\"/kaggle/working\"\n):\n    results = {\n        \"fold\": [],\n        \"MAP\": [],\n        \"MRR\": [],\n        \"Top1\": [],\n        \"Top2\": [],\n        \"Top3\": [],\n        \"Top4\": [],\n        \"Top5\": [],\n        \"Top10\": [],\n        \"Top15\": []\n    }\n\n    for i in range(k - 1):\n        train_fold = [pair for j in range(i + 1) for pair in folds[j]]\n        test_fold = folds[i + 1]\n        \n\n        print(f\"\\n📦 Fold 0..{i} ➤ {i+1}\")\n\n        train_pairs = generate_balanced_pairs(train_fold, source_files, num_negatives_per_positive=50)\n        test_pairs = generate_all_negatives_pairs(test_fold, source_files)\n\n        if sum(1 for p in train_pairs if p[-1] == 1) < 1:\n            print(\"⚠️ Bỏ qua do quá ít positive samples\")\n            continue\n\n        train_X_path = os.path.join(cache_dir, f\"X_train_aspectj_fold{i}_263.npy\")\n        train_y_path = os.path.join(cache_dir, f\"y_train_aspectj_fold{i}_263.npy\")\n        test_X_path = os.path.join(cache_dir, f\"X_test_aspectj_fold{i+1}_263.npy\")\n        test_y_path = os.path.join(cache_dir, f\"y_test_aspectj_fold{i+1}_263.npy\")\n\n        if os.path.exists(train_X_path) and os.path.exists(train_y_path):\n            print(\"✅ Load đặc trưng train từ cache\")\n            X_train = np.load(train_X_path)\n            y_train = np.load(train_y_path)\n        else:\n            print(\"🛠 Trích đặc trưng train...\")\n            X_train = build_feature_matrix_batch_large(train_pairs, glove_dict, bug_encoder, src_encoder, bug_history,batch_size=20000, device=device)\n            y_train = get_labels(train_pairs)\n            np.save(train_X_path, X_train)\n            np.save(train_y_path, y_train)\n\n        if os.path.exists(test_X_path) and os.path.exists(test_y_path):\n            print(\"✅ Load đặc trưng test từ cache\")\n            X_test = np.load(test_X_path)\n            y_test = np.load(test_y_path)\n        else:\n            print(\"🛠 Trích đặc trưng test...\")\n            X_test = build_feature_matrix_batch_large(test_pairs, glove_dict, bug_encoder, src_encoder, bug_history,batch_size=20000, device=device)\n            y_test = get_labels(test_pairs)\n            np.save(test_X_path, X_test)\n            np.save(test_y_path, y_test)\n\n        print(\"✅ Trích xuất test done\")\n\n        train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n                                      torch.tensor(y_train, dtype=torch.float32))\n        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n\n        model = model_class(input_dim=X_train.shape[1]).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n        criterion = FocalLoss(alpha=0.99)\n\n        model.train()\n        print(\"🚀 Bắt đầu huấn luyện\")\n        for epoch in range(10):\n            total_loss = 0\n            for batch_X, batch_y in train_loader:\n                batch_X = batch_X.to(device)\n                batch_y = batch_y.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(batch_X)\n                loss = criterion(outputs, batch_y)\n                loss.backward()\n                optimizer.step()\n\n                total_loss += loss.item()\n\n                del batch_X, batch_y, outputs, loss\n                torch.cuda.empty_cache()\n                import gc; gc.collect()\n\n            print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")\n\n        model.eval()\n        y_pred_probs = []\n        with torch.no_grad():\n            for i in range(0, len(X_test), 128):\n                batch_X = torch.tensor(X_test[i:i+128], dtype=torch.float32).to(device)\n                probs = model(batch_X).cpu().numpy()\n                y_pred_probs.extend(probs)\n\n        map_score = compute_MAP_per_bug(test_pairs, y_pred_probs)\n        mrr_score = mean_reciprocal_rank(test_pairs, y_pred_probs)\n        top1 = compute_topk_accuracy(test_pairs, y_pred_probs, k=1)\n        top2 = compute_topk_accuracy(test_pairs, y_pred_probs, k=2)\n        top3 = compute_topk_accuracy(test_pairs, y_pred_probs, k=3)\n        top4 = compute_topk_accuracy(test_pairs, y_pred_probs, k=4)\n        top5 = compute_topk_accuracy(test_pairs, y_pred_probs, k=5)\n        top10 = compute_topk_accuracy(test_pairs, y_pred_probs, k=10)\n        top15 = compute_topk_accuracy(test_pairs, y_pred_probs, k=15)\n\n        print(f\"✅ Results:\")\n        print(f\"  ➤ MAP:   {map_score:.4f}\")\n        print(f\"  ➤ MRR:   {mrr_score:.4f}\")\n        print(f\"  ➤ Top@1: {top1:.4f} | Top@2: {top2:.4f} | Top@3: {top3:.4f}\")\n        print(f\"  ➤ Top@4: {top4:.4f} | Top@5: {top5:.4f} | Top@10: {top10:.4f} | Top@15: {top15:.4f}\")\n\n        results[\"fold\"].append(i)\n        results[\"MAP\"].append(map_score)\n        results[\"MRR\"].append(mrr_score)\n        results[\"Top1\"].append(top1)\n        results[\"Top2\"].append(top2)\n        results[\"Top3\"].append(top3)\n        results[\"Top4\"].append(top4)\n        results[\"Top5\"].append(top5)\n        results[\"Top10\"].append(top10)\n        results[\"Top15\"].append(top15)\n\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:04:56.857007Z","iopub.execute_input":"2025-04-13T17:04:56.858076Z","iopub.status.idle":"2025-04-13T17:04:56.877527Z","shell.execute_reply.started":"2025-04-13T17:04:56.858042Z","shell.execute_reply":"2025-04-13T17:04:56.876539Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim  # ✅ phần bị thiếu\nfrom sklearn.metrics import average_precision_score\nimport os\n\n\n\n\nresults = run_kfold_training_and_eval(\n    folds=data_folds,\n    source_files=data_src,\n    glove_dict=glove_dict,\n    bug_encoder=bug_encoder,\n    src_encoder=src_encoder,\n    model_class=BugLocalization,  # hoặc ImprovedBugLocalization\n    k=3,\n    device=\"cuda\"\n)\n\n\n\n# Output full results\nprint(\"\\nFull Results:\")\nfor key, value in results.items():\n    print(f\"{key}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:05:00.784232Z","iopub.execute_input":"2025-04-13T17:05:00.784870Z"}},"outputs":[{"name":"stdout","text":"\n📦 Fold 0..0 ➤ 1\n🛠 Trích đặc trưng train...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom pathlib import Path\n\n# Tạo thư mục đích nếu chưa có\noutput_dir = Path(\"/kaggle/working/input\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Duyệt qua tất cả file .npy trong /kaggle/input và các thư mục con\nfor root, dirs, files in os.walk(\"/kaggle/input\"):\n    for file in files:\n        if file.endswith(\".npy\"):\n            src_path = os.path.join(root, file)\n            # Tạo đường dẫn tương ứng trong /kaggle/working/input\n            rel_path = os.path.relpath(src_path, \"/kaggle/input\")\n            dest_path = output_dir / rel_path\n            dest_path.parent.mkdir(parents=True, exist_ok=True)\n            shutil.copy2(src_path, dest_path)\n\n# Nén thư mục chứa các file .npy đã copy thành file zip\nshutil.make_archive(\"/kaggle/working/CNN_swt_263\", 'zip', \"/kaggle/working/input\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:30.128244Z","iopub.status.idle":"2025-04-13T17:00:30.128609Z","shell.execute_reply.started":"2025-04-13T17:00:30.128463Z","shell.execute_reply":"2025-04-13T17:00:30.128478Z"}},"outputs":[],"execution_count":null}]}