{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":11265211,"sourceType":"datasetVersion","datasetId":7041476},{"sourceId":11280721,"sourceType":"datasetVersion","datasetId":7052679}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Load dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ­","metadata":{"id":"-eUizc5QmpX4"}},{"cell_type":"code","source":"!pip install nltk\n","metadata":{"id":"9qfVWdges0io","outputId":"e6051f73-0867-4eb4-b5c0-984722d836d3","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:14.414045Z","iopub.execute_input":"2025-04-13T16:54:14.414318Z","iopub.status.idle":"2025-04-13T16:54:19.299304Z","shell.execute_reply.started":"2025-04-13T16:54:14.414290Z","shell.execute_reply":"2025-04-13T16:54:19.298201Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import nltk\nnltk.download('averaged_perceptron_tagger_eng')","metadata":{"id":"8SUNdusXgQ-3","outputId":"937573b4-53fb-4bb5-9c39-84eb53bc1890","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:19.300665Z","iopub.execute_input":"2025-04-13T16:54:19.300994Z","iopub.status.idle":"2025-04-13T16:54:21.026401Z","shell.execute_reply.started":"2025-04-13T16:54:19.300964Z","shell.execute_reply":"2025-04-13T16:54:21.025616Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# English stop words\nstop_words = set(\n    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n     'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',\n     'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',\n     'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n     'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n     'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n     'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n     'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n     'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n     'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n     'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n     'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n     's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o',\n     're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven',\n     'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won',\n     'wouldn', 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'n', 'p', 'q', 'u', 'v',\n     'w', 'x', 'z', 'us'])\n\n# Java language keywords\njava_keywords = set(\n    ['abstract', 'assert', 'boolean', 'break', 'byte', 'case',\n     'catch', 'char', 'class', 'const', 'continue', 'default', 'do', 'double',\n     'else', 'enum', 'extends', 'false', 'final', 'finally', 'float', 'for', 'goto',\n     'if', 'implements', 'import', 'instanceof', 'int', 'interface', 'long',\n     'native', 'new', 'null', 'package', 'private', 'protected', 'public', 'return',\n     'short', 'static', 'strictfp', 'super', 'switch', 'synchronized', 'this',\n     'throw', 'throws', 'transient', 'true', 'try', 'void', 'volatile', 'while'])\n\nfrom collections import namedtuple\nfrom pathlib import Path\n\n# Dataset root directory (Ä‘iá»u chá»‰nh Ä‘Æ°á»ng dáº«n náº¿u cáº§n)\n_DATASET_ROOT = Path('/content/drive/MyDrive/Colab Notebooks/NLP/Task bug localization/')\n\nDataset = namedtuple('Dataset', ['name', 'src', 'bug_repo', 'repo_url', 'features'])\n\n# CÃ¡c dataset Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a\naspectj = Dataset(\n    'aspectj',\n    _DATASET_ROOT / 'source files/org.aspectj',\n    _DATASET_ROOT / 'bug reports/AspectJ.txt',\n    \"https://github.com/eclipse/org.aspectj/tree/bug433351.git\",\n    _DATASET_ROOT / 'bug reports/AspectJ.xlsx'\n)\n\neclipse = Dataset(\n    'eclipse',\n    _DATASET_ROOT / 'source files/eclipse.platform.ui-johna-402445',\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.txt',\n    \"https://github.com/eclipse/eclipse.platform.ui.git\",\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.xlsx'\n)\n\nswt = Dataset(\n    'swt',\n    _DATASET_ROOT / 'source files/eclipse.platform.swt-xulrunner-31',\n    _DATASET_ROOT / 'bug reports/SWT.txt',\n    \"https://github.com/eclipse/eclipse.platform.swt.git\",\n    _DATASET_ROOT / 'bug reports/SWT.xlsx'\n)\n\ntomcat = Dataset(\n    'tomcat',\n    _DATASET_ROOT / 'source files/tomcat-7.0.51',\n    _DATASET_ROOT / 'bug reports/Tomcat.txt',\n    \"https://github.com/apache/tomcat.git\",\n    _DATASET_ROOT / 'bug reports/Tomcat.xlsx'\n)\n\nbirt = Dataset(\n    'birt',\n    _DATASET_ROOT / 'source files/birt-20140211-1400',\n    _DATASET_ROOT / 'bug reports/Birt.txt',\n    \"https://github.com/apache/birt.git\",\n    _DATASET_ROOT / 'bug reports/Birt.xlsx'\n)\n\n\n### Current dataset in use. (change this name to change the dataset)\nDATASET = tomcat\n\nclass BugReport:\n    \"\"\"Class representing each bug report\"\"\"\n    __slots__ = ['summary', 'description', 'fixed_files', 'report_time', 'pos_tagged_summary', 'pos_tagged_description','stack_traces','stack_traces_remove']\n\n    def __init__(self, summary, description, fixed_files, report_time):\n        self.summary = summary\n        self.description = description\n        self.fixed_files = fixed_files\n        self.report_time = report_time\n        self.pos_tagged_summary = None\n        self.pos_tagged_description = None\n        self.stack_traces = None\n        self.stack_traces_remove = None\n\nclass SourceFile:\n    \"\"\"Class representing each source file\"\"\"\n    __slots__ = ['all_content', 'comments', 'class_names', 'attributes', 'method_names', 'variables', 'file_name',\n                 'pos_tagged_comments', 'exact_file_name', 'package_name']\n\n    def __init__(self, all_content, comments, class_names, attributes, method_names, variables, file_name,\n                 package_name):\n        self.all_content = all_content\n        self.comments = comments\n        self.class_names = class_names\n        self.attributes = attributes\n        self.method_names = method_names\n        self.variables = variables\n        self.file_name = file_name\n        self.exact_file_name = file_name[0]\n        self.package_name = package_name\n        self.pos_tagged_comments = None\n\n\nclass Parser:\n    \"\"\"Class containing different parsers\"\"\"\n    __slots__ = ['name', 'src', 'bug_repo']\n\n    def __init__(self, pro):\n        self.name = pro.name\n        self.src = pro.src\n        self.bug_repo = pro.bug_repo\n\n    def report_parser(self):\n        reader = csv.DictReader(open(self.bug_repo, \"r\"), delimiter=\"\\t\")\n        bug_reports = OrderedDict()\n        # raw_texts = []\n        # fixed_files = []\n        for line in reader:\n            # line[\"raw_text\"] = line[\"summary\"] + ' ' + line[\"description\"]\n            line[\"report_time\"] = datetime.strptime(line[\"report_time\"], \"%Y-%m-%d %H:%M:%S\")\n            temp = line[\"files\"].strip().split(\".java\")\n            length = len(temp)\n            x = []\n            for i, f in enumerate(temp):\n                if i == (length - 1):\n                    x.append(os.path.normpath(f))\n                    continue\n                x.append(os.path.normpath(f + \".java\"))\n            line[\"files\"] = x\n            bug_reports[line[\"bug_id\"]] = BugReport(line[\"summary\"], line[\"description\"], line[\"files\"],\n                                                    line[\"report_time\"])\n        # bug_reports = tsv2dict(self.bug_repo)\n\n        return bug_reports\n\n    def src_parser(self):\n        \"\"\"Parse source code directory of a program and colect its java files\"\"\"\n\n        # Gettting the list of source files recursively from the source directory\n        src_addresses = glob.glob(str(self.src) + '/**/*.java', recursive=True)\n        print(src_addresses)\n        # Creating a java lexer instance for pygments.lex() method\n        java_lexer = JavaLexer()\n        src_files = OrderedDict()\n        # src_files = dict()\n        # Looping to parse each source file\n        for src_file in src_addresses:\n            with open(src_file, encoding='latin-1') as file:\n                src = file.read()\n\n            # Placeholder for different parts of a source file\n            comments = ''\n            class_names = []\n            attributes = []\n            method_names = []\n            variables = []\n\n            # Source parsing\n            parse_tree = None\n            try:\n                parse_tree = javalang.parse.parse(src)\n                for path, node in parse_tree.filter(javalang.tree.VariableDeclarator):\n                    if isinstance(path[-2], javalang.tree.FieldDeclaration):\n                        attributes.append(node.name)\n                    elif isinstance(path[-2], javalang.tree.VariableDeclaration):\n                        variables.append(node.name)\n            except:\n                pass\n\n            # Triming the source file\n            ind = False\n            if parse_tree:\n                if parse_tree.imports:\n                    last_imp_path = parse_tree.imports[-1].path\n                    src = src[src.index(last_imp_path) + len(last_imp_path) + 1:]\n                elif parse_tree.package:\n                    package_name = parse_tree.package.name\n                    src = src[src.index(package_name) + len(package_name) + 1:]\n                else:  # no import and no package declaration\n                    ind = True\n            # javalang can't parse the source file\n            else:\n                ind = True\n\n            # Lexically tokenize the source file\n            lexed_src = pygments.lex(src, java_lexer)\n\n            for i, token in enumerate(lexed_src):\n                if token[0] in Token.Comment:\n                    if ind and i == 0 and token[0] is Token.Comment.Multiline:\n                        src = src[src.index(token[1]) + len(token[1]):]\n                        continue\n                    comments = comments + token[1]\n                elif token[0] is Token.Name.Class:\n                    class_names.append(token[1])\n                elif token[0] is Token.Name.Function:\n                    method_names.append(token[1])\n\n            # get the package declaration if exists\n            if parse_tree and parse_tree.package:\n                package_name = parse_tree.package.name\n            else:\n                package_name = None\n\n            if self.name == 'aspectj' or 'tomcat' or 'eclipse' or 'swt':\n                src_files[os.path.relpath(src_file, start=self.src)] = SourceFile(src, comments, class_names,\n                                                                                  attributes, method_names, variables, [\n                                                                                      os.path.basename(src_file).split(\n                                                                                          '.')[0]], package_name)\n            else:\n                # If source files has package declaration\n                if package_name:\n                    src_id = (package_name + '.' + os.path.basename(src_file))\n                else:\n                    src_id = os.path.basename(src_file)\n                src_files[src_id] = SourceFile(src, comments, class_names, attributes, method_names, variables,\n                                               [os.path.basename(src_file).split('.')[0]], package_name)\n            # print(src_files)\n            # print(\"===========\")\n        return src_files\n\n\nclass ReportPreprocessing:\n    \"\"\"Class preprocess bug reports\"\"\"\n    __slots__ = ['bug_reports']\n\n    def __init__(self, bug_reports):\n        self.bug_reports = bug_reports\n\n    def extract_stack_traces(self):\n        \"\"\"Extract stack traces from bug reports\"\"\"\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            report.stack_traces = st\n\n    def extract_stack_traces_remove(self):\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            at = []\n            for x in st:\n                if (x[1] == 'Unknown Source'):\n                    temp = 'Unknown'\n                    y = x[0]+ '(' + temp\n                else:\n                    y = x[0] + '(' + x[1] + ')'\n                at.append(y)\n            report.stack_traces_remove = at\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from bug reports raw_text\"\"\"\n        for report in self.bug_reports.values():\n            # Tokenizing using word_tokeize for more accurate pos-tagging\n            sum_tok = nltk.word_tokenize(report.summary)\n            desc_tok = nltk.word_tokenize(report.description)\n            sum_pos = nltk.pos_tag(sum_tok)\n            desc_pos = nltk.pos_tag(desc_tok)\n            report.pos_tagged_summary = [token for token, pos in sum_pos if 'NN' in pos or 'VB' in pos]\n            report.pos_tagged_description = [token for token, pos in desc_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"Tokenize bug report intro tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = nltk.wordpunct_tokenize(report.summary)\n            report.description = nltk.wordpunct_tokenize(report.description)\n\n    def _split_camelcase(self, tokens):\n        # copy tokens\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camel case detection for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        \"\"\"Split camelcase indentifiers\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = self._split_camelcase(report.summary)\n            report.description = self._split_camelcase(report.description)\n            report.pos_tagged_summary = self._split_camelcase(report.pos_tagged_summary)\n            report.pos_tagged_description = self._split_camelcase(report.pos_tagged_description)\n\n    def normalize(self):\n        \"\"\"remove punctuation, numbers and lowecase conversion\"\"\"\n        # build a translate table for punctuation and number removal\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n\n        for report in self.bug_reports.values():\n            summary_punctnum_rem = [token.translate(punctnum_table) for token in report.summary]\n            desc_punctnum_rem = [token.translate(punctnum_table) for token in report.description]\n            pos_sum_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_summary]\n            pos_desc_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_description]\n            report.summary = [token.lower() for token in summary_punctnum_rem if token]\n            report.description = [token.lower() for token in desc_punctnum_rem if token]\n            report.pos_tagged_summary = [token.lower() for token in pos_sum_punctnum_rem if token]\n            report.pos_tagged_description = [token.lower() for token in pos_desc_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        \"\"\"removing stop word from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in stop_words]\n            report.description = [token for token in report.description if token not in stop_words]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in stop_words]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in stop_words]\n\n    def remove_java_keywords(self):\n        \"\"\"removing java language keywords from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in java_keywords]\n            report.description = [token for token in report.description if token not in java_keywords]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in java_keywords]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for report in self.bug_reports.values():\n            report.summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.summary], report.summary]))\n            report.description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.description], report.description]))\n            report.pos_tagged_summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_summary], report.pos_tagged_summary]))\n            report.pos_tagged_description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_description], report.pos_tagged_description]))\n\n    def preprocess(self):\n        self.extract_stack_traces()\n        self.extract_stack_traces_remove()\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_java_keywords()\n        self.stem()\n\nclass SrcPreprocessing:\n    \"\"\"class to preprocess source code\"\"\"\n    __slots__ = ['src_files']\n\n    def __init__(self, src_files):\n        self.src_files = src_files\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from comments\"\"\"\n        for src in self.src_files.values():\n            # tokenize using word_tokenize\n            comments_tok = nltk.word_tokenize(src.comments)\n            comments_pos = nltk.pos_tag(comments_tok)\n            src.pos_tagged_comments = [token for token, pos in comments_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"tokenize source code to tokens\"\"\"\n        for src in self.src_files.values():\n            src.all_content = nltk.wordpunct_tokenize(src.all_content)\n            src.comments = nltk.wordpunct_tokenize(src.comments)\n\n    def _split_camelcase(self, tokens):\n        # copy token\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camelcase defect for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        # Split camelcase indenti\n        for src in self.src_files.values():\n            src.all_content = self._split_camelcase(src.all_content)\n            src.comments = self._split_camelcase(src.comments)\n            src.class_names = self._split_camelcase(src.class_names)\n            src.attributes = self._split_camelcase(src.attributes)\n            src.method_names = self._split_camelcase(src.method_names)\n            src.variables = self._split_camelcase(src.variables)\n            src.pos_tagged_comments = self._split_camelcase(src.pos_tagged_comments)\n\n    def normalize(self):\n        \"remove punctuation, number and lowercase conversion\"\n        # build a translate table for punctuation and number\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n        for src in self.src_files.values():\n            content_punctnum_rem = [token.translate(punctnum_table) for token in src.all_content]\n            comments_punctnum_rem = [token.translate(punctnum_table) for token in src.comments]\n            classnames_punctnum_rem = [token.translate(punctnum_table) for token in src.class_names]\n            attributes_punctnum_rem = [token.translate(punctnum_table) for token in src.attributes]\n            methodnames_punctnum_rem = [token.translate(punctnum_table) for token in src.method_names]\n            variables_punctnum_rem = [token.translate(punctnum_table) for token in src.variables]\n            filename_punctnum_rem = [token.translate(punctnum_table) for token in src.file_name]\n            pos_comments_punctnum_rem = [token.translate(punctnum_table) for token in src.pos_tagged_comments]\n\n            src.all_content = [token.lower() for token in content_punctnum_rem if token]\n            src.comments = [token.lower() for token in comments_punctnum_rem if token]\n            src.class_names = [token.lower() for token in classnames_punctnum_rem if token]\n            src.attributes = [token.lower() for token in attributes_punctnum_rem if token]\n            src.method_names = [token.lower() for token in methodnames_punctnum_rem if token]\n            src.variables = [token.lower() for token in variables_punctnum_rem if token]\n            src.file_name = [token.lower() for token in filename_punctnum_rem if token]\n            src.pos_tagged_comments = [token.lower() for token in pos_comments_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in stop_words]\n            src.comments = [token for token in src.comments if token not in stop_words]\n            src.class_names = [token for token in src.class_names if token not in stop_words]\n            src.attributes = [token for token in src.attributes if token not in stop_words]\n            src.method_names = [token for token in src.method_names if token not in stop_words]\n            src.variables = [token for token in src.variables if token not in stop_words]\n            src.file_name = [token for token in src.file_name if token not in stop_words]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in stop_words]\n\n    def remove_javakeywords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in java_keywords]\n            src.comments = [token for token in src.comments if token not in java_keywords]\n            src.class_names = [token for token in src.class_names if token not in java_keywords]\n            src.attributes = [token for token in src.attributes if token not in java_keywords]\n            src.method_names = [token for token in src.method_names if token not in java_keywords]\n            src.variables = [token for token in src.variables if token not in java_keywords]\n            src.file_name = [token for token in src.file_name if token not in java_keywords]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for src in self.src_files.values():\n            src.all_content = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.all_content], src.all_content]))\n            src.comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.comments], src.comments]))\n            src.class_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.class_names], src.class_names]))\n            src.attributes = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.attributes], src.attributes]))\n            src.method_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.method_names], src.method_names]))\n            src.variables = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.variables], src.variables]))\n            src.file_name = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.file_name], src.file_name]))\n            src.pos_tagged_comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.pos_tagged_comments], src.pos_tagged_comments]))\n\n\n    def preprocess(self):\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_javakeywords()\n        self.stem()","metadata":{"id":"_22yeS4wcWpU","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:21.028599Z","iopub.execute_input":"2025-04-13T16:54:21.028975Z","iopub.status.idle":"2025-04-13T16:54:21.097773Z","shell.execute_reply.started":"2025-04-13T16:54:21.028954Z","shell.execute_reply":"2025-04-13T16:54:21.096577Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install inflection\nimport inflection\n","metadata":{"id":"ACZrz5Byh7Ur","outputId":"68b0fd14-1ac6-484a-cb84-2ab028bd5ed6","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:21.098990Z","iopub.execute_input":"2025-04-13T16:54:21.099328Z","iopub.status.idle":"2025-04-13T16:54:24.872052Z","shell.execute_reply.started":"2025-04-13T16:54:21.099294Z","shell.execute_reply":"2025-04-13T16:54:24.871102Z"}},"outputs":[{"name":"stdout","text":"Collecting inflection\n  Downloading inflection-0.5.1-py2.py3-none-any.whl.metadata (1.7 kB)\nDownloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\nInstalling collected packages: inflection\nSuccessfully installed inflection-0.5.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')\nimport pickle\nfrom google.colab import drive\nimport csv\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport re\nimport string\nfrom nltk.stem.porter import PorterStemmer","metadata":{"id":"P9emxprnStnP","outputId":"51497efe-74ae-4abd-d5a0-485188dab6f7","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:24.873235Z","iopub.execute_input":"2025-04-13T16:54:24.873911Z","iopub.status.idle":"2025-04-13T16:54:24.947940Z","shell.execute_reply.started":"2025-04-13T16:54:24.873885Z","shell.execute_reply":"2025-04-13T16:54:24.946932Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Load dá»¯ liá»‡u","metadata":{"id":"SC6LbKkWy8kR"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# ÄÆ°á»ng dáº«n Ä‘áº¿n cÃ¡c file pickle\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_src_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_src_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_src_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_src_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_src_processed.pkl'\n}\n\n# Load tá»«ng file vÃ  lÆ°u vÃ o cÃ¡c biáº¿n tÆ°Æ¡ng á»©ng\ndatasets = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        datasets[name] = pickle.load(f)\n\n# Kiá»ƒm tra dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c load vÃ o cÃ¡c biáº¿n\nfor name, data in datasets.items():\n    print(f\"Data for {name}:\")\n","metadata":{"id":"O3TGVN1KzAXg","outputId":"63f347a7-1b8d-41fe-98e5-105ca43233e2","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:24.948966Z","iopub.execute_input":"2025-04-13T16:54:24.949227Z","iopub.status.idle":"2025-04-13T16:54:41.001764Z","shell.execute_reply.started":"2025-04-13T16:54:24.949206Z","shell.execute_reply":"2025-04-13T16:54:41.000802Z"}},"outputs":[{"name":"stdout","text":"Data for aspectj:\nData for eclipse:\nData for swt:\nData for tomcat:\nData for birt:\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"eclipse_src = datasets['eclipse']\nbirt_src = datasets['birt']\nswt_src = datasets['swt']\ntomcat_src = datasets['tomcat']\naspectj_src = datasets['aspectj']","metadata":{"id":"b91231aHzUu_","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:41.002722Z","iopub.execute_input":"2025-04-13T16:54:41.002969Z","iopub.status.idle":"2025-04-13T16:54:41.007515Z","shell.execute_reply.started":"2025-04-13T16:54:41.002949Z","shell.execute_reply":"2025-04-13T16:54:41.006704Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load dá»¯ liá»‡u tá»« cÃ¡c file pickle Ä‘Ã£ lÆ°u\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_reports_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_reports_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_reports_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_reports_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_reports_processed.pkl'\n}\n\n# Load tá»«ng dataset vÃ  lÆ°u vÃ o cÃ¡c biáº¿n\nall_processed_reports = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        all_processed_reports[name] = pickle.load(f)\n\n# Kiá»ƒm tra dá»¯ liá»‡u Ä‘Ã£ load vÃ o\nfor dataset, reports in all_processed_reports.items():\n    print(f\"Processed reports for {dataset}:\")","metadata":{"id":"kXiYZuNgzv0M","outputId":"053bc2e9-ef09-4aea-9431-acfda03d3300","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:41.010241Z","iopub.execute_input":"2025-04-13T16:54:41.010541Z","iopub.status.idle":"2025-04-13T16:54:45.774181Z","shell.execute_reply.started":"2025-04-13T16:54:41.010515Z","shell.execute_reply":"2025-04-13T16:54:45.773207Z"}},"outputs":[{"name":"stdout","text":"Processed reports for aspectj:\nProcessed reports for eclipse:\nProcessed reports for swt:\nProcessed reports for tomcat:\nProcessed reports for birt:\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"eclipse_reports = all_processed_reports['eclipse']\nbirt_reports = all_processed_reports['birt']\nswt_reports = all_processed_reports['swt']\ntomcat_reports = all_processed_reports['tomcat']\naspectj_reports = all_processed_reports['aspectj']","metadata":{"id":"iAUv0Bnv0FcI","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:45.775190Z","iopub.execute_input":"2025-04-13T16:54:45.775483Z","iopub.status.idle":"2025-04-13T16:54:45.780097Z","shell.execute_reply.started":"2025-04-13T16:54:45.775459Z","shell.execute_reply":"2025-04-13T16:54:45.779443Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 2. Xá»­ lÃ­ data, gÃ¡n nhÃ£n\n- Sáº¯p xáº¿p bug report theo thá»i gian (report_time)\n- Chia thÃ nh 10 folds\n- Táº¡o training/test dataset theo kiá»ƒu fold i â†’ fold i+1\n- GÃ¡n nhÃ£n cho tá»«ng cáº·p (bug report, source file)","metadata":{"id":"-hwWTIRJ9PXd"}},{"cell_type":"code","source":"# B1: Láº¥y danh sÃ¡ch (bug_id, bug_report), sau Ä‘Ã³ sáº¯p xáº¿p theo report_time\nsorted_bug_reports = sorted(aspectj_reports.items(), key=lambda x: x[1].report_time)\ndata_src = aspectj_src","metadata":{"id":"HrKZiEgO9X16","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:45.781009Z","iopub.execute_input":"2025-04-13T16:54:45.781318Z","iopub.status.idle":"2025-04-13T16:54:45.798156Z","shell.execute_reply.started":"2025-04-13T16:54:45.781293Z","shell.execute_reply":"2025-04-13T16:54:45.797389Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def split_into_folds(sorted_reports, num_folds=10):\n    fold_size = len(sorted_reports) // num_folds\n    folds = [sorted_reports[i*fold_size:(i+1)*fold_size] for i in range(num_folds)]\n\n    # Náº¿u cÃ²n dÆ°, ráº£i Ä‘á»u vÃ o cÃ¡c fold Ä‘áº§u\n    remainder = sorted_reports[num_folds*fold_size:]\n    for i, extra in enumerate(remainder):\n        folds[i].append(extra)\n    return folds\n\ndata_folds = split_into_folds(sorted_bug_reports, num_folds=3)\n","metadata":{"id":"fgRpuQKE9aA2","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:54:45.799172Z","iopub.execute_input":"2025-04-13T16:54:45.799506Z","iopub.status.idle":"2025-04-13T16:54:45.817328Z","shell.execute_reply.started":"2025-04-13T16:54:45.799482Z","shell.execute_reply":"2025-04-13T16:54:45.816441Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"i = 0 # thá»­ vá»›i fold 0 â†’ 1\ntrain_fold = data_folds[i]\ntest_fold = data_folds[i+1]","metadata":{"id":"0itPPQ5O9cDT","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:55:55.754000Z","iopub.execute_input":"2025-04-13T16:55:55.754433Z","iopub.status.idle":"2025-04-13T16:55:55.760252Z","shell.execute_reply.started":"2025-04-13T16:55:55.754404Z","shell.execute_reply":"2025-04-13T16:55:55.758989Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\ndef generate_balanced_pairs(bug_fold, source_files, num_negatives_per_positive=50):\n    data = []\n    for bug_id, bug in bug_fold:\n        # Danh sÃ¡ch file chá»©a bug (poszqitive)\n        positive_paths = set(bug.fixed_files)\n        positive = [\n            (bug_id, bug, src_path, source_files[src_path], 1)\n            for src_path in positive_paths if src_path in source_files\n        ]\n\n        # Danh sÃ¡ch file cÃ²n láº¡i Ä‘á»ƒ láº¥y negative\n        all_paths = list(source_files.keys())\n        negative_paths = list(set(all_paths) - positive_paths)\n        sampled_negatives = random.sample(negative_paths, min(num_negatives_per_positive * len(positive), len(negative_paths)))\n\n        negative = [\n            (bug_id, bug, src_path, source_files[src_path], 0)\n            for src_path in sampled_negatives if src_path in source_files\n        ]\n\n        data.extend(positive + negative)\n    return data\ndef generate_all_negatives_pairs(bug_fold, source_files):\n    data = []\n    for bug_id, bug in bug_fold:\n        positive_paths = set(bug.fixed_files)\n        positive = [\n            (bug_id, bug, src_path, source_files[src_path], 1)\n            for src_path in positive_paths if src_path in source_files\n        ]\n\n        all_paths = list(source_files.keys())\n        negative_paths = list(set(all_paths) - positive_paths)\n\n        negative = [\n            (bug_id, bug, src_path, source_files[src_path], 0)\n            for src_path in negative_paths if src_path in source_files\n        ]\n\n        data.extend(positive + negative)\n    return data\n\n\ntrain_pairs = generate_balanced_pairs(train_fold, data_src, num_negatives_per_positive=50)\n#test_pairs = generate_balanced_pairs(test_fold, data_src, num_negatives_per_positive=50)\ntest_pairs = generate_all_negatives_pairs(test_fold, data_src)\n\n\n","metadata":{"id":"wjLEL9mR9fPv","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:55:55.762179Z","iopub.execute_input":"2025-04-13T16:55:55.762519Z","iopub.status.idle":"2025-04-13T16:56:01.489192Z","shell.execute_reply.started":"2025-04-13T16:55:55.762496Z","shell.execute_reply":"2025-04-13T16:56:01.488275Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"Xá»­ lÃ­ máº¥t cÃ¢n báº±ng","metadata":{"id":"phVI5NPj-GMs"}},{"cell_type":"code","source":"def compute_stats(pairs):\n    total = len(pairs)\n    pos = sum(1 for _, _, _, _, label in pairs if label == 1)\n    neg = total - pos\n    ratio = pos / total if total > 0 else 0\n    return total, pos, neg, ratio\n\n  \ntotal, pos, neg, ratio = compute_stats(train_pairs)\nprint(\"ğŸ“Š Train Set:\")\nprint(f\"  â¤ Tá»•ng cáº·p: {total}\")\nprint(f\"  âœ… Positive (label=1): {pos}\")\nprint(f\"  âŒ Negative (label=0): {neg}\")\nprint(f\"  âš–ï¸ Tá»· lá»‡ positive: {ratio:.4f}\")\n\ntotal, pos, neg, ratio = compute_stats(test_pairs)\nprint(\"\\nğŸ§ª Test Set:\")\nprint(f\"  â¤ Tá»•ng cáº·p: {total}\")\nprint(f\"  âœ… Positive (label=1): {pos}\")\nprint(f\"  âŒ Negative (label=0): {neg}\")\nprint(f\"  âš–ï¸ Tá»· lá»‡ positive: {ratio:.4f}\")\n","metadata":{"id":"G-m4H5d79uzp","outputId":"1a4a413d-189b-4b5d-ca55-6552e56ce33f","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:01.490161Z","iopub.execute_input":"2025-04-13T16:56:01.490606Z","iopub.status.idle":"2025-04-13T16:56:01.608236Z","shell.execute_reply.started":"2025-04-13T16:56:01.490579Z","shell.execute_reply":"2025-04-13T16:56:01.607294Z"}},"outputs":[{"name":"stdout","text":"ğŸ“Š Train Set:\n  â¤ Tá»•ng cáº·p: 8619\n  âœ… Positive (label=1): 169\n  âŒ Negative (label=0): 8450\n  âš–ï¸ Tá»· lá»‡ positive: 0.0196\n\nğŸ§ª Test Set:\n  â¤ Tá»•ng cáº·p: 1368180\n  âœ… Positive (label=1): 168\n  âŒ Negative (label=0): 1368012\n  âš–ï¸ Tá»· lá»‡ positive: 0.0001\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### HÃ m 1: Táº¡o batches cÃ³ bootstrapping (luÃ´n chá»©a Ã­t nháº¥t 1 positive sample)","metadata":{}},{"cell_type":"code","source":"def create_bootstrapped_batches(pairs, batch_size=128, pos_ratio=0.1):\n    positives = [p for p in pairs if p[-1] == 1]\n    negatives = [p for p in pairs if p[-1] == 0]\n\n    pos_per_batch = max(1, int(batch_size * pos_ratio))\n    neg_per_batch = batch_size - pos_per_batch\n\n    random.shuffle(positives)\n    random.shuffle(negatives)\n\n    batches = []\n    pos_idx, neg_idx = 0, 0\n\n    while neg_idx + neg_per_batch <= len(negatives):\n        pos_batch = []\n        for _ in range(pos_per_batch):\n            pos_batch.append(positives[pos_idx % len(positives)])\n            pos_idx += 1\n\n        neg_batch = negatives[neg_idx:neg_idx + neg_per_batch]\n        neg_idx += neg_per_batch\n\n        batch = pos_batch + neg_batch\n        random.shuffle(batch)\n        batches.append(batch)\n\n    return batches\n","metadata":{"id":"VVi_cbQD-a1b","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:01.610589Z","iopub.execute_input":"2025-04-13T16:56:01.610848Z","iopub.status.idle":"2025-04-13T16:56:01.618424Z","shell.execute_reply.started":"2025-04-13T16:56:01.610828Z","shell.execute_reply":"2025-04-13T16:56:01.617463Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### Focal Loss Function","metadata":{}},{"cell_type":"code","source":"\ndef focal_loss(predictions, targets, alpha=0.999, gamma=2.0, eps=1e-6):\n    \"\"\"\n    predictions: tensor (batch_size,) - output sigmoid from model\n    targets: tensor (batch_size,) - true labels (0 or 1)\n    \"\"\"\n    # Avoid log(0)\n    predictions = predictions.clamp(min=eps, max=1.0 - eps)\n\n    # Compute focal loss\n    loss = -alpha * (1 - predictions)**gamma * targets * predictions.log() \\\n           - (1 - alpha) * predictions**gamma * (1 - targets) * (1 - predictions).log()\n    return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:01.619219Z","iopub.execute_input":"2025-04-13T16:56:01.619523Z","iopub.status.idle":"2025-04-13T16:56:01.635480Z","shell.execute_reply.started":"2025-04-13T16:56:01.619503Z","shell.execute_reply":"2025-04-13T16:56:01.634584Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# 4. TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# HÃ m xá»­ lÃ½ text gá»™p láº¡i tá»« bug report\ndef bug_to_text(bug):\n    summary = bug.summary['unstemmed'] if isinstance(bug.summary, dict) else bug.summary\n    desc = bug.description['unstemmed'] if isinstance(bug.description, dict) else bug.description\n    return \" \".join(summary + desc)\n\n# HÃ m xá»­ lÃ½ text tá»« source file\ndef src_to_text(src):\n    content = src.all_content['unstemmed'] if isinstance(src.all_content, dict) else src.all_content\n    comments = src.comments['unstemmed'] if isinstance(src.comments, dict) else src.comments\n    return \" \".join(content + comments)\n","metadata":{"id":"4en89sb6-s54","outputId":"5c14ad2a-da00-495a-ec81-38687265bcfe","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:01.636316Z","iopub.execute_input":"2025-04-13T16:56:01.636722Z","iopub.status.idle":"2025-04-13T16:56:01.648852Z","shell.execute_reply.started":"2025-04-13T16:56:01.636690Z","shell.execute_reply":"2025-04-13T16:56:01.647913Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Äáº·c trÆ°ng 1: TÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng tá»« vá»±ng (lexical similarity)\n- PhÆ°Æ¡ng phÃ¡p: sá»­ dá»¥ng TF-IDF vÃ  cosine similarity.\n- Input: Cáº·p dá»¯ liá»‡u (bug report, source file)\n- Output: máº£ng numpy chá»©a cÃ¡c giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine giá»¯a bug report vÃ  source file cho má»—i cáº·p.","metadata":{}},{"cell_type":"code","source":"def compute_lexical_similarity(pairs):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # Gá»™p cáº£ bug + src láº¡i Ä‘á»ƒ fit chung vectorizer\n    combined = bug_texts + src_texts\n    tfidf = TfidfVectorizer()\n    tfidf_matrix = tfidf.fit_transform(combined)\n\n    # TÃ¡ch riÃªng láº¡i tá»«ng pháº§n\n    bug_vecs = tfidf_matrix[:len(pairs)]\n    src_vecs = tfidf_matrix[len(pairs):]\n\n    # TÃ­nh cosine cho tá»«ng cáº·p (theo hÃ ng tÆ°Æ¡ng á»©ng)\n    similarities = cosine_similarity(bug_vecs, src_vecs).diagonal()\n\n    return similarities\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:01.649947Z","iopub.execute_input":"2025-04-13T16:56:01.650212Z","iopub.status.idle":"2025-04-13T16:56:01.672533Z","shell.execute_reply.started":"2025-04-13T16:56:01.650193Z","shell.execute_reply":"2025-04-13T16:56:01.671472Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"glove_path = \"/kaggle/input/glove-embedding/glove.6B.100d.txt\"\n# Load GloVe 100d vÃ o dictionary\nimport numpy as np\n\ndef load_glove_embeddings(filepath):\n    embeddings = {}\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.strip().split()\n            word = values[0]\n            vector = np.array(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n    \nglove_embeddings = load_glove_embeddings(glove_path)","metadata":{"id":"6FexoFrK_LTE","outputId":"b7e7b200-954e-4541-a27b-dc43607ae8f2","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:01.673542Z","iopub.execute_input":"2025-04-13T16:56:01.673865Z","iopub.status.idle":"2025-04-13T16:56:13.594800Z","shell.execute_reply.started":"2025-04-13T16:56:01.673837Z","shell.execute_reply":"2025-04-13T16:56:13.594063Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Äáº·c trÆ°ng 2: TÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a (semantic similarity)\n- PhÆ°Æ¡ng phÃ¡p: TF-IDF weighted average cá»§a GloVe vectors vÃ  cosine similarity\n- Input:  (bug report, source file).\n- Output: Má»™t máº£ng numpy chá»©a cÃ¡c giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine giá»¯a bug report vÃ  source file cho má»—i cáº·p, dá»±a trÃªn GloVe vectors vÃ  trá»ng sá»‘ TF-IDF.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef compute_semantic_similarity(pairs, glove_dict, dim=100):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # DÃ¹ng TF-IDF Ä‘á»ƒ láº¥y trá»ng sá»‘ tá»«\n    tfidf = TfidfVectorizer()\n    tfidf.fit(bug_texts + src_texts)\n    vocab = tfidf.vocabulary_\n    idf_weights = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n\n    def embed_text(text):\n        tokens = text.split()\n        vecs = []\n        weights = []\n        for token in tokens:\n            if token in glove_dict and token in vocab:\n                vecs.append(glove_dict[token])\n                weights.append(idf_weights[token])\n        if not vecs:\n            return np.zeros(dim)\n        vecs = np.array(vecs)\n        weights = np.array(weights).reshape(-1, 1)\n        weighted_vecs = vecs * weights\n        return weighted_vecs.sum(axis=0) / weights.sum()\n\n    # TÃ­nh vector trung bÃ¬nh cho bug vÃ  src\n    bug_vecs = [embed_text(text) for text in bug_texts]\n    src_vecs = [embed_text(text) for text in src_texts]\n\n    # TÃ­nh cosine similarity giá»¯a tá»«ng cáº·p\n    similarities = [cosine_similarity([b], [s])[0][0] for b, s in zip(bug_vecs, src_vecs)]\n\n    return np.array(similarities)\n","metadata":{"id":"BmKfMQ9oAFSQ","outputId":"e0bf60b5-8bc7-401f-c8ee-5e8ac04ebe01","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:13.595668Z","iopub.execute_input":"2025-04-13T16:56:13.595971Z","iopub.status.idle":"2025-04-13T16:56:13.604727Z","shell.execute_reply.started":"2025-04-13T16:56:13.595945Z","shell.execute_reply":"2025-04-13T16:56:13.603896Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Äáº·c trÆ°ng 3: Similar Bug Report Score ","metadata":{}},{"cell_type":"markdown","source":"â†’ Kiá»ƒm tra xem bug report nÃ y cÃ³ giá»‘ngÂ **nhá»¯ng bug report cÅ© tá»«ng sá»­a cÃ¹ng file Ä‘Ã³**Â khÃ´ng?\n\n- `build_bug_fix_history(pairs)` â†’ XD lá»‹ch sá»­ chá»‰nh sá»­a theo tá»«ng file\n- `compute_similar_bug_score(pairs, history)`\n    - Input: pairs, history\n    - So sÃ¡nh bug hiá»‡n táº¡i vÃ  bug cÅ©:\n    \n    cosine_similarity(TfidfVectorizer().fit_transform([bug_now, bug_old]))[0, 1]\n    \n    - Láº¥y giÃ¡ trá»‹ tÆ°Æ¡ng Ä‘á»“ng cao nháº¥t vá»«a tÃ¬m Ä‘Æ°á»£c","metadata":{}},{"cell_type":"code","source":"def build_bug_fix_history(pairs):\n    history = {}\n    for bug_id, bug, src_path, _, label in pairs:\n        if label == 1:  # chá»‰ tÃ­nh cÃ¡c bug tháº­t sá»± sá»­a file\n            if src_path not in history:\n                history[src_path] = []\n            history[src_path].append((bug_id, bug.report_time, bug_to_text(bug)))\n    return history\n\n# Äáº·c trÆ°ng 3: Similar Bug Report Score\ndef compute_similar_bug_score(pairs, history):\n    scores = []\n    for bug_id, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        current_text = bug_to_text(bug)\n\n        sim_scores = []\n        if src_path in history:\n            for hist_bug_id, hist_time, hist_text in history[src_path]:\n                if hist_time < current_time:  # chá»‰ tÃ­nh bug trong quÃ¡ khá»©\n                    sim = cosine_similarity(\n                        TfidfVectorizer().fit_transform([current_text, hist_text])\n                    )[0, 1]\n                    sim_scores.append(sim)\n        scores.append(max(sim_scores) if sim_scores else 0.0)\n    return np.array(scores)","metadata":{"id":"imb4_du_Bgjz","outputId":"8c08c8e9-7047-4e5d-de85-8b70e30cbe28","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:13.607967Z","iopub.execute_input":"2025-04-13T16:56:13.608549Z","iopub.status.idle":"2025-04-13T16:56:13.632150Z","shell.execute_reply.started":"2025-04-13T16:56:13.608520Z","shell.execute_reply":"2025-04-13T16:56:13.631247Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Äáº·c trung 4: Time Since Last Fix (ngÃ y, normalize)\n- Kiá»ƒm tra vá»›i má»—i `(bug report, source file)` xem tá»«ng Ä‘Æ°á»£c sá»­a trÆ°á»›c Ä‘Ã³ khÃ´ng vÃ  láº§n cuá»‘i khi nÃ o\n    - ÄÃ£ lÃ¢u k sá»­a â†’ Ãt lá»—i â†’ Äiá»ƒm tháº¥p\n    - Má»›i sá»­a â†’ cÃ³ thá»ƒ liÃªn quan tá»›i lá»—i â†’ Äiá»ƒm cao\n- CÃ¡ch hÄ‘:\n    - TÃ¬m thá»i Ä‘iá»ƒm bug current_time\n    - TÃ¬m history cÃ¡c láº§n sá»­a file trong quÃ¡ khá»©\n    - TÃ­nh khoáº£ng cÃ¡ch time giá»¯a current vÃ  history gáº§n nháº¥t\n    - ChÆ°a sá»­a â†’ GÃ¡n sá»‘ delta_days=9999\n    - Chuáº©n hoÃ¡","metadata":{}},{"cell_type":"code","source":"# Äáº·c trÆ°ng 4: Time Since Last Fix (ngÃ y, normalize)\ndef compute_time_since_last_fix(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_times = [hist_time for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            if past_times:\n                delta_days = (current_time - max(past_times)).days\n            else:\n                delta_days = 9999  # Cá»±c lá»›n náº¿u chÆ°a tá»«ng sá»­a\n        else:\n            delta_days = 9999\n        scores.append(delta_days)\n\n    # Normalize vá» [0,1]\n    max_days = max(scores) if max(scores) != 0 else 1  # TrÃ¡nh chia cho 0\n\n    return np.array([1 - (s / max_days) for s in scores])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:13.633154Z","iopub.execute_input":"2025-04-13T16:56:13.633803Z","iopub.status.idle":"2025-04-13T16:56:13.656387Z","shell.execute_reply.started":"2025-04-13T16:56:13.633781Z","shell.execute_reply":"2025-04-13T16:56:13.655537Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"### Äáº·c trÆ°ng 5: Fix Frequency (sá»‘ láº§n bá»‹ sá»­a trong quÃ¡ khá»©, normalize)\n\n\n- Kiá»ƒm tra xme má»—i cáº·p Ä‘Æ°á»£c sá»­a bao nhiÃªu láº§n\n\nâ†’ Sá»­a nhiá»u â†’ File dá»… dÃ­nh lá»—i â†’ Äiá»ƒm cao","metadata":{}},{"cell_type":"code","source":"# Äáº·c trÆ°ng 5: Fix Frequency (sá»‘ láº§n bá»‹ sá»­a trong quÃ¡ khá»©, normalize)\ndef compute_fix_frequency(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_fixes = [1 for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            freq = len(past_fixes)\n        else:\n            freq = 0\n        scores.append(freq)\n    # Normalize vá» [0,1] an toÃ n\n    max_freq = max(scores)\n    max_freq = max(max_freq, 1)  # trÃ¡nh chia 0\n    return np.array([s / max_freq for s in scores])\n\n\n# DÃ¹ng cho 500 cáº·p máº«u\nsampled_pairs = train_pairs[:5000]\nbug_history = build_bug_fix_history(train_pairs)\n\nsimilar_bug_score = compute_similar_bug_score(sampled_pairs, bug_history)\ntime_since_last_fix = compute_time_since_last_fix(sampled_pairs, bug_history)\nfix_frequency = compute_fix_frequency(sampled_pairs, bug_history)\n\n# TrÃ­ch 5 giÃ¡ trá»‹ Ä‘áº§u má»—i feature\nsimilar_bug_score[:50], time_since_last_fix[:50], fix_frequency[:50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:13.657172Z","iopub.execute_input":"2025-04-13T16:56:13.657465Z","iopub.status.idle":"2025-04-13T16:56:13.816496Z","shell.execute_reply.started":"2025-04-13T16:56:13.657444Z","shell.execute_reply":"2025-04-13T16:56:13.815723Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"def min_max_normalize(values):\n    min_val = min(values)\n    max_val = max(values)\n    denom = max_val - min_val if max_val != min_val else 1\n    return [(v - min_val) / denom for v in values]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:56:13.817283Z","iopub.execute_input":"2025-04-13T16:56:13.817631Z","iopub.status.idle":"2025-04-13T16:56:13.822871Z","shell.execute_reply.started":"2025-04-13T16:56:13.817606Z","shell.execute_reply":"2025-04-13T16:56:13.822019Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"Äáº·c trÆ°ng 6: Ngá»¯ nghÄ©a há»c sÃ¢u tá»« CNN encoder","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n# CNN encoder dÃ¹ng cho cáº£ bug vÃ  source\nclass CNNEncoder(nn.Module):\n    def __init__(self, embed_dim=100, num_filters=64, kernel_sizes=(3, 5), dropout=0.5):\n        super(CNNEncoder, self).__init__()\n        self.convs = nn.ModuleList([\n            nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=k)\n            for k in kernel_sizes\n        ])\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):  # x: (batch_size, seq_len, embed_dim)\n        x = x.permute(0, 2, 1)  # (batch, embed_dim, seq_len)\n        conv_outs = [F.relu(conv(x)) for conv in self.convs]\n        pooled = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in conv_outs]\n        out = torch.cat(pooled, dim=1)\n        return self.dropout(out)  # shape: (batch_size, num_filters * len(kernel_sizes))\n\n# Token list â†’ batch embedding tensor\ndef batch_tokens_to_embeddings(batch_tokens, glove_dict, dim=100, max_len=100):\n    batch_embeddings = []\n    for tokens in batch_tokens:\n        emb = []\n        for token in tokens[:max_len]:\n            if token in glove_dict:\n                emb.append(glove_dict[token])\n            else:\n                emb.append(np.zeros(dim))\n        while len(emb) < max_len:\n            emb.append(np.zeros(dim))\n        batch_embeddings.append(emb)\n    return torch.tensor(np.array(batch_embeddings), dtype=torch.float32)\n\n# ğŸ†• CNN feature extraction â€“ batch, nhanh, GPU, normalized\ndef extract_cnn_features_batch(pairs, glove_dict, bug_encoder, src_encoder, device=\"cuda\", dim=100, max_len=100):\n    bug_encoder = bug_encoder.to(device)\n    src_encoder = src_encoder.to(device)\n    bug_encoder.eval()\n    src_encoder.eval()\n\n    bug_token_list = []\n    src_token_list = []\n\n    for _, bug, _, src, _ in pairs:\n        bug_tokens = bug.summary['stemmed'] + bug.description['stemmed']\n        src_tokens = src.all_content['stemmed'] + src.comments['stemmed'] + \\\n                     src.class_names['stemmed'] + src.method_names['stemmed']\n        bug_token_list.append(bug_tokens)\n        src_token_list.append(src_tokens)\n\n    # Embed â†’ tensor\n    bug_tensor = batch_tokens_to_embeddings(bug_token_list, glove_dict, dim, max_len).to(device)\n    src_tensor = batch_tokens_to_embeddings(src_token_list, glove_dict, dim, max_len).to(device)\n\n    # Forward CNN\n    with torch.no_grad():\n        bug_tensor = bug_tensor.to(device)\n        src_tensor = src_tensor.to(device)\n    \n        bug_vec = bug_encoder(bug_tensor)\n        src_vec = src_encoder(src_tensor)\n    \n        combined = torch.cat([bug_vec, src_vec], dim=1).cpu().numpy()  # chuyá»ƒn vá» CPU Ä‘á»ƒ convert sang NumPy\n\n\n    # Normalize tá»«ng chiá»u vá» [0, 1]\n    min_vals = combined.min(axis=0)\n    max_vals = combined.max(axis=0)\n    denom = np.where(max_vals - min_vals == 0, 1, max_vals - min_vals)\n    normalized = (combined - min_vals) / denom\n\n    return normalized  # shape: (n_samples, 2*filters)\nimport torch\n\ndef extract_cnn_features_batch(pairs, glove_dict, bug_encoder, src_encoder, device=None, dim=100, max_len=100):\n    # Tá»± Ä‘á»™ng chá»n thiáº¿t bá»‹: GPU náº¿u cÃ³, khÃ´ng thÃ¬ CPU\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    bug_encoder = bug_encoder.to(device)\n    src_encoder = src_encoder.to(device)\n    bug_encoder.eval()\n    src_encoder.eval()\n\n    bug_token_list = []\n    src_token_list = []\n\n    for _, bug, _, src, _ in pairs:\n        bug_tokens = bug.summary['stemmed'] + bug.description['stemmed']\n        src_tokens = src.all_content['stemmed'] + src.comments['stemmed'] + \\\n                     src.class_names['stemmed'] + src.method_names['stemmed']\n        bug_token_list.append(bug_tokens)\n        src_token_list.append(src_tokens)\n\n    # Embed â†’ tensor\n    bug_tensor = batch_tokens_to_embeddings(bug_token_list, glove_dict, dim, max_len).to(device)\n    src_tensor = batch_tokens_to_embeddings(src_token_list, glove_dict, dim, max_len).to(device)\n\n    # Forward CNN\n    with torch.no_grad():\n        bug_vec = bug_encoder(bug_tensor)\n        src_vec = src_encoder(src_tensor)\n        combined = torch.cat([bug_vec, src_vec], dim=1).cpu().numpy()\n\n    # Normalize tá»«ng chiá»u vá» [0, 1]\n    min_vals = combined.min(axis=0)\n    max_vals = combined.max(axis=0)\n    denom = np.where(max_vals - min_vals == 0, 1, max_vals - min_vals)\n    normalized = (combined - min_vals) / denom\n\n    return normalized  # shape: (n_samples, 2 * filters)\n\n","metadata":{"id":"PmEZSTiyDEB4","outputId":"6db11041-f3a2-4931-f264-9480bfd7c2e6","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:04:26.414911Z","iopub.execute_input":"2025-04-13T17:04:26.415240Z","iopub.status.idle":"2025-04-13T17:04:26.435694Z","shell.execute_reply.started":"2025-04-13T17:04:26.415215Z","shell.execute_reply":"2025-04-13T17:04:26.434715Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"bug_encoder = CNNEncoder()\nsrc_encoder = CNNEncoder()\nglove_dict=glove_embeddings\ncnn_combined_vector = extract_cnn_features_batch(train_pairs[:5000], glove_dict, bug_encoder, src_encoder)\nprint(\"âœ… CNN Ä‘áº·c trÆ°ng Ä‘Ã£ chuáº©n hÃ³a:\", cnn_combined_vector.shape)\nprint(cnn_combined_vector[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:04:28.778327Z","iopub.execute_input":"2025-04-13T17:04:28.779274Z","iopub.status.idle":"2025-04-13T17:04:33.889912Z","shell.execute_reply.started":"2025-04-13T17:04:28.779240Z","shell.execute_reply":"2025-04-13T17:04:33.888913Z"}},"outputs":[{"name":"stdout","text":"âœ… CNN Ä‘áº·c trÆ°ng Ä‘Ã£ chuáº©n hÃ³a: (5000, 256)\n[[0.24267694 0.22376536 0.6680607  ... 0.781797   0.36746198 0.62862104]\n [0.24267694 0.22376536 0.6680607  ... 0.44357866 0.35221133 0.74948055]\n [0.24267694 0.22376536 0.6680607  ... 0.3792844  0.32195666 0.60165375]\n ...\n [0.24267694 0.22376536 0.6680607  ... 0.17033355 0.07380829 0.04349329]\n [0.24267694 0.22376536 0.6680607  ... 0.33772215 0.         0.2134291 ]\n [0.24267694 0.22376536 0.6680607  ... 0.60519177 0.39244118 0.36244106]]\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"ÄT7: identifier_overlap_count â€“ Sá»‘ tÃªn hÃ m/biáº¿n trÃ¹ng vá»›i tá»« trong bug report","metadata":{}},{"cell_type":"code","source":"def compute_identifier_overlap_count(pairs):\n    counts = []\n    for _, bug, _, src, _ in pairs:\n        bug_tokens = set(bug.summary['stemmed'] + bug.description['stemmed'])\n        identifiers = set(\n            src.class_names['stemmed'] + src.method_names['stemmed'] + src.variables['stemmed']\n        )\n        overlap = bug_tokens & identifiers\n        counts.append(len(overlap))\n    counts = np.array(counts)\n    # ğŸ”„ Log normalization vá» [0,1]\n    return np.log1p(counts) / np.log1p(np.max(counts)) if counts.max() > 0 else np.zeros_like(counts)\n\n\ndef compute_shared_token_ratio(pairs):\n    ratios = []\n    for _, bug, _, src, _ in pairs:\n        bug_tokens = set(bug.summary['stemmed'] + bug.description['stemmed'])\n        src_tokens = set(\n            src.all_content['stemmed'] + src.comments['stemmed'] + \n            src.class_names['stemmed'] + src.method_names['stemmed']\n        )\n        if not src_tokens:\n            ratios.append(0.0)\n        else:\n            ratios.append(len(bug_tokens & src_tokens) / len(src_tokens))\n    ratios = np.array(ratios)\n    # ğŸ”„ Min-max normalization vá» [0,1]\n    return (ratios - ratios.min()) / (ratios.max() - ratios.min()) if ratios.max() > ratios.min() else np.zeros_like(ratios)\n\nsampled_pairs = train_pairs[:5000]\nidf_overlap = compute_identifier_overlap_count(sampled_pairs)\nshared_ratio = compute_shared_token_ratio(sampled_pairs)\n\nprint(\"âœ… identifier_overlap_count:\", idf_overlap[:10])\nprint(\"âœ… shared_token_ratio:\", shared_ratio[:10])\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:59:39.950671Z","iopub.execute_input":"2025-04-13T16:59:39.950987Z","iopub.status.idle":"2025-04-13T16:59:40.152230Z","shell.execute_reply.started":"2025-04-13T16:59:39.950966Z","shell.execute_reply":"2025-04-13T16:59:40.151442Z"}},"outputs":[{"name":"stdout","text":"âœ… identifier_overlap_count: [0.42061984 0.         0.         0.         0.         0.\n 0.         0.         0.         0.        ]\nâœ… shared_token_ratio: [0.00472441 0.00925926 0.03773585 0.         0.         0.\n 0.02222222 0.14285714 0.         0.01960784]\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# 4. QuÃ¡ trÃ¬nh huáº¥n luyá»‡n","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Táº¡o ma tráº­n train, test","metadata":{}},{"cell_type":"code","source":"def build_feature_matrix_batch(pairs_batch, glove_dict, bug_encoder, src_encoder, history, device=\"cuda\"):\n    # Äáº·c trÆ°ng vector thÆ°á»ng (1 chiá»u)\n    lexical = compute_lexical_similarity(pairs_batch)                         # (N,)\n    semantic = compute_semantic_similarity(pairs_batch, glove_dict)          # (N,)\n    idf_overlap = compute_identifier_overlap_count(pairs_batch)              # (N,)\n    shared_ratio = compute_shared_token_ratio(pairs_batch)                   # (N,)\n    \n    # Äáº·c trÆ°ng dá»±a vÃ o lá»‹ch sá»­ sá»­a lá»—i\n    similar_score = compute_similar_bug_score(pairs_batch, history)\n    recency = compute_time_since_last_fix(pairs_batch, history)\n    freq = compute_fix_frequency(pairs_batch, history)\n    # Äáº·c trÆ°ng há»c sÃ¢u (N, 256)\n    cnn_vec = extract_cnn_features_batch(pairs_batch, glove_dict, bug_encoder, src_encoder)\n\n    # GhÃ©p toÃ n bá»™ láº¡i\n    X = np.hstack([\n        lexical.reshape(-1, 1),         \n        semantic.reshape(-1, 1),        \n        idf_overlap.reshape(-1, 1),     \n        shared_ratio.reshape(-1, 1),    \n        similar_score.reshape(-1, 1),\n        recency.reshape(-1, 1),\n        freq.reshape(-1, 1),\n        cnn_vec                         \n    ])  # â†’ Tá»•ng cá»™ng (N, 263)\n\n    return X\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:59:40.153694Z","iopub.execute_input":"2025-04-13T16:59:40.154014Z","iopub.status.idle":"2025-04-13T16:59:40.160357Z","shell.execute_reply.started":"2025-04-13T16:59:40.153994Z","shell.execute_reply":"2025-04-13T16:59:40.159481Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Táº¡o nhÃ£n y\ndef get_labels(pairs):\n    return np.array([label for *_, label in pairs])\nglove_dict = glove_embeddings\nX_train = build_feature_matrix_batch(train_pairs, glove_dict, bug_encoder, src_encoder,bug_history, device=device)\ny_train = get_labels(train_pairs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:59:40.161185Z","iopub.execute_input":"2025-04-13T16:59:40.161523Z","iopub.status.idle":"2025-04-13T17:00:04.637915Z","shell.execute_reply.started":"2025-04-13T16:59:40.161501Z","shell.execute_reply":"2025-04-13T17:00:04.636849Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def pair_generator(pairs, batch_size, glove_dict, history, bug_encoder, src_encoder, device=\"cuda\"):\n    for i in range(0, len(pairs), batch_size):\n        batch = pairs[i:i + batch_size]\n        X = build_feature_matrix_batch(batch, glove_dict, bug_encoder, src_encoder, history, device)\n        y = np.array([label for *_, label in batch])\n\n        yield torch.tensor(X, dtype=torch.float32).to(device), torch.tensor(y, dtype=torch.float32).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:04.639028Z","iopub.execute_input":"2025-04-13T17:00:04.639419Z","iopub.status.idle":"2025-04-13T17:00:04.646446Z","shell.execute_reply.started":"2025-04-13T17:00:04.639367Z","shell.execute_reply":"2025-04-13T17:00:04.645287Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"print(\"âœ… X_train shape:\", X_train.shape)  # (5000, 260)\nprint(\"âœ… y_train shape:\", y_train.shape)  # (5000,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:04.649018Z","iopub.execute_input":"2025-04-13T17:00:04.649322Z","iopub.status.idle":"2025-04-13T17:00:04.673477Z","shell.execute_reply.started":"2025-04-13T17:00:04.649299Z","shell.execute_reply":"2025-04-13T17:00:04.672236Z"}},"outputs":[{"name":"stdout","text":"âœ… X_train shape: (8619, 263)\nâœ… y_train shape: (8619,)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"## 4.2 XÃ¢y dá»±ng mÃ´ hÃ¬nh","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\n# Äá»‹nh nghÄ©a mÃ´ hÃ¬nh DNN giá»‘ng bÃ i bÃ¡o\nimport torch\nimport torch.nn as nn\n\nclass BugLocalization(nn.Module):\n    def __init__(self, input_dim=5, hidden_dim=64, output_dim=1):\n        super(BugLocalization, self).__init__()\n        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = x.to(next(self.parameters()).device)  # auto move to model's device\n        # Náº¿u x chá»‰ cÃ³ 2 chiá»u (batch_size, input_dim), hÃ£y thÃªm má»™t chiá»u giáº£ Ä‘á»‹nh (sequence_length=1)\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)  # ThÃªm má»™t chiá»u giáº£ Ä‘á»‹nh (batch_size, 1, input_dim)\n\n        # x shape: (batch_size, sequence_length, input_dim)\n        rnn_out, _ = self.rnn(x)\n        # Láº¥y output cá»§a pháº§n cuá»‘i cÃ¹ng trong chuá»—i\n        final_rnn_out = rnn_out[:, -1, :]\n        out = torch.sigmoid(self.fc(final_rnn_out)).squeeze()\n        return out\n\n\n        \n# Äá»‹nh nghÄ©a focal loss\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.999, gamma=2.0, eps=1e-6):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, preds, targets):\n        preds = preds.clamp(min=self.eps, max=1. - self.eps)\n        loss = -self.alpha * (1 - preds) ** self.gamma * targets * torch.log(preds) \\\n               - (1 - self.alpha) * preds ** self.gamma * (1 - targets) * torch.log(1 - preds)\n        return loss.mean()\n\n\n# Huáº¥n luyá»‡n mÃ´ hÃ¬nh\ndef train_model_generator(model, train_gen, epochs=10, lr=1e-3):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = FocalLoss()\n\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch_X, batch_y in train_gen:\n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n            # ğŸ§¹ Dá»n bá»™ nhá»› má»—i batch\n            del batch_X, batch_y, outputs, loss\n            torch.cuda.empty_cache()\n            import gc; gc.collect()\n\n        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n\n","metadata":{"id":"LiD_wJxjDeQ8","outputId":"9821aee2-a66e-480b-9f69-6e4d207c7ba6","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:04.674551Z","iopub.execute_input":"2025-04-13T17:00:04.674829Z","iopub.status.idle":"2025-04-13T17:00:04.698136Z","shell.execute_reply.started":"2025-04-13T17:00:04.674808Z","shell.execute_reply":"2025-04-13T17:00:04.697424Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for X_batch, y_batch in pair_generator(\n    train_pairs, batch_size=128,\n    glove_dict=glove_embeddings,\n    history=bug_history,\n    bug_encoder=bug_encoder,\n    src_encoder=src_encoder, \n    device=device\n):\n    print(\"ğŸ‘‰ Feature shape:\", X_batch.shape)\n    print(\"ğŸ‘‰ Label shape:\", y_batch.shape)\n    print(\"ğŸ‘‰ Feature Sample [2]:\", X_batch[2].cpu().numpy())\n    print(\"ğŸ‘‰ Label Sample [2]:\", y_batch[2].item())\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:04.699150Z","iopub.execute_input":"2025-04-13T17:00:04.699788Z","iopub.status.idle":"2025-04-13T17:00:05.019680Z","shell.execute_reply.started":"2025-04-13T17:00:04.699765Z","shell.execute_reply":"2025-04-13T17:00:05.018785Z"}},"outputs":[{"name":"stdout","text":"ğŸ‘‰ Feature shape: torch.Size([128, 263])\nğŸ‘‰ Label shape: torch.Size([128])\nğŸ‘‰ Feature Sample [2]: [0.01822121 0.7729522  0.         0.03773585 0.         0.\n 0.         0.         0.35706815 0.         1.         0.\n 0.15033144 0.         0.         0.         1.         0.\n 0.         0.67332315 0.         1.         0.         0.\n 0.         0.33636105 0.         0.47858703 0.5355421  0.\n 0.         1.         0.         0.         0.         1.\n 0.24859448 0.2735264  0.5301865  0.         0.         0.20633534\n 0.         0.         0.67322123 1.         0.         0.\n 0.8869989  0.         0.         1.         0.         0.\n 0.         0.01430868 0.25331688 1.         0.         1.\n 0.         1.         1.         1.         0.9065167  0.4161309\n 1.         0.         0.         1.         0.2813771  0.\n 0.7264074  1.         0.         1.         0.         1.\n 0.         0.60931265 0.         0.         0.         0.\n 0.         1.         1.         1.         0.23088759 1.\n 1.         0.         0.         0.         0.04415921 0.7522205\n 0.         1.         1.         0.         0.         0.\n 0.56480056 0.398972   0.49734566 0.3783361  0.         0.6363016\n 0.         0.         1.         1.         1.         0.\n 0.40044025 0.71484035 1.         0.         0.         1.\n 1.         0.7163564  0.         1.         0.         0.\n 1.         0.1789143  0.45158234 1.         1.         0.05963381\n 1.         0.         0.5933989  0.51485306 0.46111912 0.4638457\n 0.60739154 0.6909374  0.49349794 0.628727   0.73529917 0.36106527\n 0.544204   0.8340344  0.49787864 0.6363471  0.51470834 0.6289383\n 0.6077393  0.55292505 0.24723157 0.64401144 0.6058025  0.76647747\n 0.60857743 0.24984217 0.7978555  0.7290899  0.57184845 0.64680374\n 0.6226768  0.6971908  0.6897723  0.64800054 0.89020085 0.5317565\n 0.81717384 0.56141466 0.37485832 0.5951407  0.353841   0.64287305\n 0.6886488  0.37852833 0.8189771  0.43412474 0.3301495  0.6486478\n 0.4713482  0.6825522  0.8065349  0.830497   0.55990624 0.71873045\n 0.8211372  0.49257255 0.73755014 0.5655598  0.63362336 0.7271205\n 0.7428144  0.80085766 0.49639818 0.787129   0.5390462  0.54907846\n 0.6952124  0.4519555  0.34433374 0.55609554 0.517255   0.4382771\n 0.5417443  0.7699088  0.6674922  0.7516097  0.59542507 0.49202988\n 0.5299655  0.49817666 0.6772146  0.81480646 0.5632339  0.59829646\n 0.34206966 0.5361576  0.42315313 0.58462405 0.28554794 0.54797953\n 0.7285614  0.79223615 0.51648116 0.8109658  0.75147605 0.61094797\n 0.60865074 0.6013054  0.62486017 0.54014087 0.59285    0.6621813\n 0.63416296 0.4786988  0.91014993 0.93317586 0.7516453  0.47362208\n 0.6826647  0.5564992  0.632934   0.45528594 0.84474623 0.445405\n 0.44342214 0.47197255 0.68387014 0.56199497 0.6083353  0.71401286\n 0.5763397  0.36333787 0.5003047  0.65904355 0.58153814 0.6633591\n 0.57462    1.         0.5284378  0.59554756 0.63285166]\nğŸ‘‰ Label Sample [2]: 0.0\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def test_pair_generator(pairs, batch_size, glove_dict, history, bug_encoder, src_encoder, device=\"cuda\"):\n    for i in range(0, len(pairs), batch_size):\n        batch = pairs[i:i + batch_size]\n        X = build_feature_matrix_batch(batch, glove_dict, bug_encoder, src_encoder, history, device)\n\n        yield torch.tensor(X, dtype=torch.float32, device=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:05.021356Z","iopub.execute_input":"2025-04-13T17:00:05.021762Z","iopub.status.idle":"2025-04-13T17:00:05.027961Z","shell.execute_reply.started":"2025-04-13T17:00:05.021739Z","shell.execute_reply":"2025-04-13T17:00:05.026941Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom sklearn.metrics import average_precision_score\nfrom collections import defaultdict\nimport numpy as np\n\n# ÄÃ¡nh giÃ¡ cÃ¡c chá»‰ sá»‘ (MAP, MRR, Top-k)\ndef compute_topk_accuracy(test_pairs, y_scores, k=10):\n    bug_to_scores = {}\n    for (bug_id, _, src_path, _, label), score in zip(test_pairs, y_scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    correct_at_k = 0\n    total = 0\n\n    for entries in bug_to_scores.values():\n        sorted_entries = sorted(entries, key=lambda x: x[0], reverse=True)\n        top_k = sorted_entries[:k]\n        if any(label == 1 for _, label in top_k):\n            correct_at_k += 1\n        total += 1\n\n    return correct_at_k / total if total > 0 else 0\n\n\n\ndef compute_MAP_per_bug(test_pairs, y_pred_probs):\n    # Gom nhÃ£n vÃ  score theo bug_id\n    bug_to_ytrue = defaultdict(list)\n    bug_to_yscore = defaultdict(list)\n\n    for (bug_id, _, _, _, label), score in zip(test_pairs, y_pred_probs):\n        bug_to_ytrue[bug_id].append(label)\n        bug_to_yscore[bug_id].append(score)\n\n    # TÃ­nh AP cho tá»«ng bug, chá»‰ giá»¯ bug cÃ³ Ã­t nháº¥t 1 label = 1\n    ap_list = []\n    for bug_id in bug_to_ytrue:\n        y_true = np.array(bug_to_ytrue[bug_id])\n        y_score = np.array(bug_to_yscore[bug_id])\n\n        if np.sum(y_true) == 0:\n            continue  # bá» qua bug khÃ´ng cÃ³ file liÃªn quan\n\n        ap = average_precision_score(y_true, y_score)\n        ap_list.append(ap)\n\n    # TÃ­nh MAP\n    MAP = np.mean(ap_list) if ap_list else 0.0\n    return MAP\n\n# MRR (Mean Reciprocal Rank)\ndef mean_reciprocal_rank(pairs, scores):\n    bug_to_scores = {}\n    for (bug_id, _, _, _, label), score in zip(pairs, scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    rr_sum = 0\n    count = 0\n    for bug_id, ranked in bug_to_scores.items():\n        ranked = sorted(ranked, key=lambda x: x[0], reverse=True)\n        for idx, (_, label) in enumerate(ranked):\n            if label == 1:\n                rr_sum += 1 / (idx + 1)\n                break\n        count += 1\n    return rr_sum / count if count > 0 else 0","metadata":{"id":"hvK-tE_MD4zq","outputId":"5444ce3e-4624-4f81-eb6b-b19a543b0d49","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:05.029065Z","iopub.execute_input":"2025-04-13T17:00:05.029490Z","iopub.status.idle":"2025-04-13T17:00:05.049095Z","shell.execute_reply.started":"2025-04-13T17:00:05.029466Z","shell.execute_reply":"2025-04-13T17:00:05.048284Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def build_feature_matrix_batch_large(\n    pairs, \n    glove_dict, \n    bug_encoder, \n    src_encoder, \n    history, \n    batch_size=20000, \n    device=\"cuda\"\n):\n    all_features = []\n\n    for i in range(0, len(pairs), batch_size):\n        sub_batch = pairs[i:i+batch_size]\n        \n        # ğŸ”¤ CÃ¡c Ä‘áº·c trÆ°ng truyá»n thá»‘ng\n        lexical = compute_lexical_similarity(sub_batch)\n        semantic = compute_semantic_similarity(sub_batch, glove_dict)\n        idf_overlap = compute_identifier_overlap_count(sub_batch)\n        shared_ratio = compute_shared_token_ratio(sub_batch)\n\n        # ğŸ§  CÃ¡c Ä‘áº·c trÆ°ng tá»« lá»‹ch sá»­ sá»­a lá»—i\n        similar_score = compute_similar_bug_score(sub_batch, history)\n        recency = compute_time_since_last_fix(sub_batch, history)\n        freq = compute_fix_frequency(sub_batch, history)\n\n        # ğŸ” Äáº·c trÆ°ng há»c sÃ¢u\n        cnn = extract_cnn_features_batch(sub_batch, glove_dict, bug_encoder, src_encoder)\n\n        # ğŸ§© GhÃ©p toÃ n bá»™ Ä‘áº·c trÆ°ng\n        others = np.stack([\n            lexical, semantic, idf_overlap, shared_ratio, \n            similar_score, recency, freq\n        ], axis=1)\n\n        combined = np.concatenate([others, cnn], axis=1)\n        all_features.append(combined)\n\n        print(f\"âœ… Done {i+len(sub_batch)}/{len(pairs)} samples\")\n\n    return np.vstack(all_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:04:49.693982Z","iopub.execute_input":"2025-04-13T17:04:49.694614Z","iopub.status.idle":"2025-04-13T17:04:49.702579Z","shell.execute_reply.started":"2025-04-13T17:04:49.694587Z","shell.execute_reply":"2025-04-13T17:04:49.701453Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def run_kfold_training_and_eval(\n    folds,\n    source_files,\n    glove_dict,\n    bug_encoder,\n    src_encoder,\n    model_class,\n    k=3,\n    device=\"cpu\",\n    cache_dir=\"/kaggle/working\"\n):\n    results = {\n        \"fold\": [],\n        \"MAP\": [],\n        \"MRR\": [],\n        \"Top1\": [],\n        \"Top2\": [],\n        \"Top3\": [],\n        \"Top4\": [],\n        \"Top5\": [],\n        \"Top10\": [],\n        \"Top15\": []\n    }\n\n    for i in range(k - 1):\n        train_fold = [pair for j in range(i + 1) for pair in folds[j]]\n        test_fold = folds[i + 1]\n        \n\n        print(f\"\\nğŸ“¦ Fold 0..{i} â¤ {i+1}\")\n\n        train_pairs = generate_balanced_pairs(train_fold, source_files, num_negatives_per_positive=50)\n        test_pairs = generate_all_negatives_pairs(test_fold, source_files)\n\n        if sum(1 for p in train_pairs if p[-1] == 1) < 1:\n            print(\"âš ï¸ Bá» qua do quÃ¡ Ã­t positive samples\")\n            continue\n\n        train_X_path = os.path.join(cache_dir, f\"X_train_aspectj_fold{i}_263.npy\")\n        train_y_path = os.path.join(cache_dir, f\"y_train_aspectj_fold{i}_263.npy\")\n        test_X_path = os.path.join(cache_dir, f\"X_test_aspectj_fold{i+1}_263.npy\")\n        test_y_path = os.path.join(cache_dir, f\"y_test_aspectj_fold{i+1}_263.npy\")\n\n        if os.path.exists(train_X_path) and os.path.exists(train_y_path):\n            print(\"âœ… Load Ä‘áº·c trÆ°ng train tá»« cache\")\n            X_train = np.load(train_X_path)\n            y_train = np.load(train_y_path)\n        else:\n            print(\"ğŸ›  TrÃ­ch Ä‘áº·c trÆ°ng train...\")\n            X_train = build_feature_matrix_batch_large(train_pairs, glove_dict, bug_encoder, src_encoder, bug_history,batch_size=20000, device=device)\n            y_train = get_labels(train_pairs)\n            np.save(train_X_path, X_train)\n            np.save(train_y_path, y_train)\n\n        if os.path.exists(test_X_path) and os.path.exists(test_y_path):\n            print(\"âœ… Load Ä‘áº·c trÆ°ng test tá»« cache\")\n            X_test = np.load(test_X_path)\n            y_test = np.load(test_y_path)\n        else:\n            print(\"ğŸ›  TrÃ­ch Ä‘áº·c trÆ°ng test...\")\n            X_test = build_feature_matrix_batch_large(test_pairs, glove_dict, bug_encoder, src_encoder, bug_history,batch_size=20000, device=device)\n            y_test = get_labels(test_pairs)\n            np.save(test_X_path, X_test)\n            np.save(test_y_path, y_test)\n\n        print(\"âœ… TrÃ­ch xuáº¥t test done\")\n\n        train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n                                      torch.tensor(y_train, dtype=torch.float32))\n        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n\n        model = model_class(input_dim=X_train.shape[1]).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n        criterion = FocalLoss(alpha=0.99)\n\n        model.train()\n        print(\"ğŸš€ Báº¯t Ä‘áº§u huáº¥n luyá»‡n\")\n        for epoch in range(10):\n            total_loss = 0\n            for batch_X, batch_y in train_loader:\n                batch_X = batch_X.to(device)\n                batch_y = batch_y.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(batch_X)\n                loss = criterion(outputs, batch_y)\n                loss.backward()\n                optimizer.step()\n\n                total_loss += loss.item()\n\n                del batch_X, batch_y, outputs, loss\n                torch.cuda.empty_cache()\n                import gc; gc.collect()\n\n            print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")\n\n        model.eval()\n        y_pred_probs = []\n        with torch.no_grad():\n            for i in range(0, len(X_test), 128):\n                batch_X = torch.tensor(X_test[i:i+128], dtype=torch.float32).to(device)\n                probs = model(batch_X).cpu().numpy()\n                y_pred_probs.extend(probs)\n\n        map_score = compute_MAP_per_bug(test_pairs, y_pred_probs)\n        mrr_score = mean_reciprocal_rank(test_pairs, y_pred_probs)\n        top1 = compute_topk_accuracy(test_pairs, y_pred_probs, k=1)\n        top2 = compute_topk_accuracy(test_pairs, y_pred_probs, k=2)\n        top3 = compute_topk_accuracy(test_pairs, y_pred_probs, k=3)\n        top4 = compute_topk_accuracy(test_pairs, y_pred_probs, k=4)\n        top5 = compute_topk_accuracy(test_pairs, y_pred_probs, k=5)\n        top10 = compute_topk_accuracy(test_pairs, y_pred_probs, k=10)\n        top15 = compute_topk_accuracy(test_pairs, y_pred_probs, k=15)\n\n        print(f\"âœ… Results:\")\n        print(f\"  â¤ MAP:   {map_score:.4f}\")\n        print(f\"  â¤ MRR:   {mrr_score:.4f}\")\n        print(f\"  â¤ Top@1: {top1:.4f} | Top@2: {top2:.4f} | Top@3: {top3:.4f}\")\n        print(f\"  â¤ Top@4: {top4:.4f} | Top@5: {top5:.4f} | Top@10: {top10:.4f} | Top@15: {top15:.4f}\")\n\n        results[\"fold\"].append(i)\n        results[\"MAP\"].append(map_score)\n        results[\"MRR\"].append(mrr_score)\n        results[\"Top1\"].append(top1)\n        results[\"Top2\"].append(top2)\n        results[\"Top3\"].append(top3)\n        results[\"Top4\"].append(top4)\n        results[\"Top5\"].append(top5)\n        results[\"Top10\"].append(top10)\n        results[\"Top15\"].append(top15)\n\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:04:56.857007Z","iopub.execute_input":"2025-04-13T17:04:56.858076Z","iopub.status.idle":"2025-04-13T17:04:56.877527Z","shell.execute_reply.started":"2025-04-13T17:04:56.858042Z","shell.execute_reply":"2025-04-13T17:04:56.876539Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim  # âœ… pháº§n bá»‹ thiáº¿u\nfrom sklearn.metrics import average_precision_score\nimport os\n\n\n\n\nresults = run_kfold_training_and_eval(\n    folds=data_folds,\n    source_files=data_src,\n    glove_dict=glove_dict,\n    bug_encoder=bug_encoder,\n    src_encoder=src_encoder,\n    model_class=BugLocalization,  # hoáº·c ImprovedBugLocalization\n    k=3,\n    device=\"cuda\"\n)\n\n\n\n# Output full results\nprint(\"\\nFull Results:\")\nfor key, value in results.items():\n    print(f\"{key}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:05:00.784232Z","iopub.execute_input":"2025-04-13T17:05:00.784870Z"}},"outputs":[{"name":"stdout","text":"\nğŸ“¦ Fold 0..0 â¤ 1\nğŸ›  TrÃ­ch Ä‘áº·c trÆ°ng train...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom pathlib import Path\n\n# Táº¡o thÆ° má»¥c Ä‘Ã­ch náº¿u chÆ°a cÃ³\noutput_dir = Path(\"/kaggle/working/input\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Duyá»‡t qua táº¥t cáº£ file .npy trong /kaggle/input vÃ  cÃ¡c thÆ° má»¥c con\nfor root, dirs, files in os.walk(\"/kaggle/input\"):\n    for file in files:\n        if file.endswith(\".npy\"):\n            src_path = os.path.join(root, file)\n            # Táº¡o Ä‘Æ°á»ng dáº«n tÆ°Æ¡ng á»©ng trong /kaggle/working/input\n            rel_path = os.path.relpath(src_path, \"/kaggle/input\")\n            dest_path = output_dir / rel_path\n            dest_path.parent.mkdir(parents=True, exist_ok=True)\n            shutil.copy2(src_path, dest_path)\n\n# NÃ©n thÆ° má»¥c chá»©a cÃ¡c file .npy Ä‘Ã£ copy thÃ nh file zip\nshutil.make_archive(\"/kaggle/working/CNN_swt_263\", 'zip', \"/kaggle/working/input\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:30.128244Z","iopub.status.idle":"2025-04-13T17:00:30.128609Z","shell.execute_reply.started":"2025-04-13T17:00:30.128463Z","shell.execute_reply":"2025-04-13T17:00:30.128478Z"}},"outputs":[],"execution_count":null}]}