{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11265211,"sourceType":"datasetVersion","datasetId":7041476}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Ti·ªÅn x·ª≠ l√≠ d·ªØ li·ªáu","metadata":{"id":"-eUizc5QmpX4"}},{"cell_type":"code","source":"!pip install nltk\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9qfVWdges0io","outputId":"e6051f73-0867-4eb4-b5c0-984722d836d3","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:26.714016Z","iopub.execute_input":"2025-04-04T06:34:26.714257Z","iopub.status.idle":"2025-04-04T06:34:31.007009Z","shell.execute_reply.started":"2025-04-04T06:34:26.714235Z","shell.execute_reply":"2025-04-04T06:34:31.005899Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import nltk\nnltk.download('averaged_perceptron_tagger_eng')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8SUNdusXgQ-3","outputId":"937573b4-53fb-4bb5-9c39-84eb53bc1890","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:31.008416Z","iopub.execute_input":"2025-04-04T06:34:31.008775Z","iopub.status.idle":"2025-04-04T06:34:32.941292Z","shell.execute_reply.started":"2025-04-04T06:34:31.008735Z","shell.execute_reply":"2025-04-04T06:34:32.940375Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# English stop words\nstop_words = set(\n    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n     'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',\n     'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',\n     'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n     'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n     'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n     'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n     'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n     'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n     'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n     'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n     'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n     's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o',\n     're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven',\n     'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won',\n     'wouldn', 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'n', 'p', 'q', 'u', 'v',\n     'w', 'x', 'z', 'us'])\n\n# Java language keywords\njava_keywords = set(\n    ['abstract', 'assert', 'boolean', 'break', 'byte', 'case',\n     'catch', 'char', 'class', 'const', 'continue', 'default', 'do', 'double',\n     'else', 'enum', 'extends', 'false', 'final', 'finally', 'float', 'for', 'goto',\n     'if', 'implements', 'import', 'instanceof', 'int', 'interface', 'long',\n     'native', 'new', 'null', 'package', 'private', 'protected', 'public', 'return',\n     'short', 'static', 'strictfp', 'super', 'switch', 'synchronized', 'this',\n     'throw', 'throws', 'transient', 'true', 'try', 'void', 'volatile', 'while'])\n\nfrom collections import namedtuple\nfrom pathlib import Path\n\n# Dataset root directory (ƒëi·ªÅu ch·ªânh ƒë∆∞·ªùng d·∫´n n·∫øu c·∫ßn)\n_DATASET_ROOT = Path('/content/drive/MyDrive/Colab Notebooks/NLP/Task bug localization/')\n\nDataset = namedtuple('Dataset', ['name', 'src', 'bug_repo', 'repo_url', 'features'])\n\n# C√°c dataset ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\naspectj = Dataset(\n    'aspectj',\n    _DATASET_ROOT / 'source files/org.aspectj',\n    _DATASET_ROOT / 'bug reports/AspectJ.txt',\n    \"https://github.com/eclipse/org.aspectj/tree/bug433351.git\",\n    _DATASET_ROOT / 'bug reports/AspectJ.xlsx'\n)\n\neclipse = Dataset(\n    'eclipse',\n    _DATASET_ROOT / 'source files/eclipse.platform.ui-johna-402445',\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.txt',\n    \"https://github.com/eclipse/eclipse.platform.ui.git\",\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.xlsx'\n)\n\nswt = Dataset(\n    'swt',\n    _DATASET_ROOT / 'source files/eclipse.platform.swt-xulrunner-31',\n    _DATASET_ROOT / 'bug reports/SWT.txt',\n    \"https://github.com/eclipse/eclipse.platform.swt.git\",\n    _DATASET_ROOT / 'bug reports/SWT.xlsx'\n)\n\ntomcat = Dataset(\n    'tomcat',\n    _DATASET_ROOT / 'source files/tomcat-7.0.51',\n    _DATASET_ROOT / 'bug reports/Tomcat.txt',\n    \"https://github.com/apache/tomcat.git\",\n    _DATASET_ROOT / 'bug reports/Tomcat.xlsx'\n)\n\nbirt = Dataset(\n    'birt',\n    _DATASET_ROOT / 'source files/birt-20140211-1400',\n    _DATASET_ROOT / 'bug reports/Birt.txt',\n    \"https://github.com/apache/birt.git\",\n    _DATASET_ROOT / 'bug reports/Birt.xlsx'\n)\n\n\n### Current dataset in use. (change this name to change the dataset)\nDATASET = tomcat\n\nclass BugReport:\n    \"\"\"Class representing each bug report\"\"\"\n    __slots__ = ['summary', 'description', 'fixed_files', 'report_time', 'pos_tagged_summary', 'pos_tagged_description','stack_traces','stack_traces_remove']\n\n    def __init__(self, summary, description, fixed_files, report_time):\n        self.summary = summary\n        self.description = description\n        self.fixed_files = fixed_files\n        self.report_time = report_time\n        self.pos_tagged_summary = None\n        self.pos_tagged_description = None\n        self.stack_traces = None\n        self.stack_traces_remove = None\n\nclass SourceFile:\n    \"\"\"Class representing each source file\"\"\"\n    __slots__ = ['all_content', 'comments', 'class_names', 'attributes', 'method_names', 'variables', 'file_name',\n                 'pos_tagged_comments', 'exact_file_name', 'package_name']\n\n    def __init__(self, all_content, comments, class_names, attributes, method_names, variables, file_name,\n                 package_name):\n        self.all_content = all_content\n        self.comments = comments\n        self.class_names = class_names\n        self.attributes = attributes\n        self.method_names = method_names\n        self.variables = variables\n        self.file_name = file_name\n        self.exact_file_name = file_name[0]\n        self.package_name = package_name\n        self.pos_tagged_comments = None\n\n\nclass Parser:\n    \"\"\"Class containing different parsers\"\"\"\n    __slots__ = ['name', 'src', 'bug_repo']\n\n    def __init__(self, pro):\n        self.name = pro.name\n        self.src = pro.src\n        self.bug_repo = pro.bug_repo\n\n    def report_parser(self):\n        reader = csv.DictReader(open(self.bug_repo, \"r\"), delimiter=\"\\t\")\n        bug_reports = OrderedDict()\n        # raw_texts = []\n        # fixed_files = []\n        for line in reader:\n            # line[\"raw_text\"] = line[\"summary\"] + ' ' + line[\"description\"]\n            line[\"report_time\"] = datetime.strptime(line[\"report_time\"], \"%Y-%m-%d %H:%M:%S\")\n            temp = line[\"files\"].strip().split(\".java\")\n            length = len(temp)\n            x = []\n            for i, f in enumerate(temp):\n                if i == (length - 1):\n                    x.append(os.path.normpath(f))\n                    continue\n                x.append(os.path.normpath(f + \".java\"))\n            line[\"files\"] = x\n            bug_reports[line[\"bug_id\"]] = BugReport(line[\"summary\"], line[\"description\"], line[\"files\"],\n                                                    line[\"report_time\"])\n        # bug_reports = tsv2dict(self.bug_repo)\n\n        return bug_reports\n\n    def src_parser(self):\n        \"\"\"Parse source code directory of a program and colect its java files\"\"\"\n\n        # Gettting the list of source files recursively from the source directory\n        src_addresses = glob.glob(str(self.src) + '/**/*.java', recursive=True)\n        print(src_addresses)\n        # Creating a java lexer instance for pygments.lex() method\n        java_lexer = JavaLexer()\n        src_files = OrderedDict()\n        # src_files = dict()\n        # Looping to parse each source file\n        for src_file in src_addresses:\n            with open(src_file, encoding='latin-1') as file:\n                src = file.read()\n\n            # Placeholder for different parts of a source file\n            comments = ''\n            class_names = []\n            attributes = []\n            method_names = []\n            variables = []\n\n            # Source parsing\n            parse_tree = None\n            try:\n                parse_tree = javalang.parse.parse(src)\n                for path, node in parse_tree.filter(javalang.tree.VariableDeclarator):\n                    if isinstance(path[-2], javalang.tree.FieldDeclaration):\n                        attributes.append(node.name)\n                    elif isinstance(path[-2], javalang.tree.VariableDeclaration):\n                        variables.append(node.name)\n            except:\n                pass\n\n            # Triming the source file\n            ind = False\n            if parse_tree:\n                if parse_tree.imports:\n                    last_imp_path = parse_tree.imports[-1].path\n                    src = src[src.index(last_imp_path) + len(last_imp_path) + 1:]\n                elif parse_tree.package:\n                    package_name = parse_tree.package.name\n                    src = src[src.index(package_name) + len(package_name) + 1:]\n                else:  # no import and no package declaration\n                    ind = True\n            # javalang can't parse the source file\n            else:\n                ind = True\n\n            # Lexically tokenize the source file\n            lexed_src = pygments.lex(src, java_lexer)\n\n            for i, token in enumerate(lexed_src):\n                if token[0] in Token.Comment:\n                    if ind and i == 0 and token[0] is Token.Comment.Multiline:\n                        src = src[src.index(token[1]) + len(token[1]):]\n                        continue\n                    comments = comments + token[1]\n                elif token[0] is Token.Name.Class:\n                    class_names.append(token[1])\n                elif token[0] is Token.Name.Function:\n                    method_names.append(token[1])\n\n            # get the package declaration if exists\n            if parse_tree and parse_tree.package:\n                package_name = parse_tree.package.name\n            else:\n                package_name = None\n\n            if self.name == 'aspectj' or 'tomcat' or 'eclipse' or 'swt':\n                src_files[os.path.relpath(src_file, start=self.src)] = SourceFile(src, comments, class_names,\n                                                                                  attributes, method_names, variables, [\n                                                                                      os.path.basename(src_file).split(\n                                                                                          '.')[0]], package_name)\n            else:\n                # If source files has package declaration\n                if package_name:\n                    src_id = (package_name + '.' + os.path.basename(src_file))\n                else:\n                    src_id = os.path.basename(src_file)\n                src_files[src_id] = SourceFile(src, comments, class_names, attributes, method_names, variables,\n                                               [os.path.basename(src_file).split('.')[0]], package_name)\n            # print(src_files)\n            # print(\"===========\")\n        return src_files\n\n\nclass ReportPreprocessing:\n    \"\"\"Class preprocess bug reports\"\"\"\n    __slots__ = ['bug_reports']\n\n    def __init__(self, bug_reports):\n        self.bug_reports = bug_reports\n\n    def extract_stack_traces(self):\n        \"\"\"Extract stack traces from bug reports\"\"\"\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            report.stack_traces = st\n\n    def extract_stack_traces_remove(self):\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            at = []\n            for x in st:\n                if (x[1] == 'Unknown Source'):\n                    temp = 'Unknown'\n                    y = x[0]+ '(' + temp\n                else:\n                    y = x[0] + '(' + x[1] + ')'\n                at.append(y)\n            report.stack_traces_remove = at\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from bug reports raw_text\"\"\"\n        for report in self.bug_reports.values():\n            # Tokenizing using word_tokeize for more accurate pos-tagging\n            sum_tok = nltk.word_tokenize(report.summary)\n            desc_tok = nltk.word_tokenize(report.description)\n            sum_pos = nltk.pos_tag(sum_tok)\n            desc_pos = nltk.pos_tag(desc_tok)\n            report.pos_tagged_summary = [token for token, pos in sum_pos if 'NN' in pos or 'VB' in pos]\n            report.pos_tagged_description = [token for token, pos in desc_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"Tokenize bug report intro tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = nltk.wordpunct_tokenize(report.summary)\n            report.description = nltk.wordpunct_tokenize(report.description)\n\n    def _split_camelcase(self, tokens):\n        # copy tokens\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camel case detection for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        \"\"\"Split camelcase indentifiers\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = self._split_camelcase(report.summary)\n            report.description = self._split_camelcase(report.description)\n            report.pos_tagged_summary = self._split_camelcase(report.pos_tagged_summary)\n            report.pos_tagged_description = self._split_camelcase(report.pos_tagged_description)\n\n    def normalize(self):\n        \"\"\"remove punctuation, numbers and lowecase conversion\"\"\"\n        # build a translate table for punctuation and number removal\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n\n        for report in self.bug_reports.values():\n            summary_punctnum_rem = [token.translate(punctnum_table) for token in report.summary]\n            desc_punctnum_rem = [token.translate(punctnum_table) for token in report.description]\n            pos_sum_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_summary]\n            pos_desc_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_description]\n            report.summary = [token.lower() for token in summary_punctnum_rem if token]\n            report.description = [token.lower() for token in desc_punctnum_rem if token]\n            report.pos_tagged_summary = [token.lower() for token in pos_sum_punctnum_rem if token]\n            report.pos_tagged_description = [token.lower() for token in pos_desc_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        \"\"\"removing stop word from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in stop_words]\n            report.description = [token for token in report.description if token not in stop_words]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in stop_words]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in stop_words]\n\n    def remove_java_keywords(self):\n        \"\"\"removing java language keywords from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in java_keywords]\n            report.description = [token for token in report.description if token not in java_keywords]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in java_keywords]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for report in self.bug_reports.values():\n            report.summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.summary], report.summary]))\n            report.description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.description], report.description]))\n            report.pos_tagged_summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_summary], report.pos_tagged_summary]))\n            report.pos_tagged_description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_description], report.pos_tagged_description]))\n\n    def preprocess(self):\n        self.extract_stack_traces()\n        self.extract_stack_traces_remove()\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_java_keywords()\n        self.stem()\n\nclass SrcPreprocessing:\n    \"\"\"class to preprocess source code\"\"\"\n    __slots__ = ['src_files']\n\n    def __init__(self, src_files):\n        self.src_files = src_files\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from comments\"\"\"\n        for src in self.src_files.values():\n            # tokenize using word_tokenize\n            comments_tok = nltk.word_tokenize(src.comments)\n            comments_pos = nltk.pos_tag(comments_tok)\n            src.pos_tagged_comments = [token for token, pos in comments_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"tokenize source code to tokens\"\"\"\n        for src in self.src_files.values():\n            src.all_content = nltk.wordpunct_tokenize(src.all_content)\n            src.comments = nltk.wordpunct_tokenize(src.comments)\n\n    def _split_camelcase(self, tokens):\n        # copy token\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camelcase defect for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        # Split camelcase indenti\n        for src in self.src_files.values():\n            src.all_content = self._split_camelcase(src.all_content)\n            src.comments = self._split_camelcase(src.comments)\n            src.class_names = self._split_camelcase(src.class_names)\n            src.attributes = self._split_camelcase(src.attributes)\n            src.method_names = self._split_camelcase(src.method_names)\n            src.variables = self._split_camelcase(src.variables)\n            src.pos_tagged_comments = self._split_camelcase(src.pos_tagged_comments)\n\n    def normalize(self):\n        \"remove punctuation, number and lowercase conversion\"\n        # build a translate table for punctuation and number\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n        for src in self.src_files.values():\n            content_punctnum_rem = [token.translate(punctnum_table) for token in src.all_content]\n            comments_punctnum_rem = [token.translate(punctnum_table) for token in src.comments]\n            classnames_punctnum_rem = [token.translate(punctnum_table) for token in src.class_names]\n            attributes_punctnum_rem = [token.translate(punctnum_table) for token in src.attributes]\n            methodnames_punctnum_rem = [token.translate(punctnum_table) for token in src.method_names]\n            variables_punctnum_rem = [token.translate(punctnum_table) for token in src.variables]\n            filename_punctnum_rem = [token.translate(punctnum_table) for token in src.file_name]\n            pos_comments_punctnum_rem = [token.translate(punctnum_table) for token in src.pos_tagged_comments]\n\n            src.all_content = [token.lower() for token in content_punctnum_rem if token]\n            src.comments = [token.lower() for token in comments_punctnum_rem if token]\n            src.class_names = [token.lower() for token in classnames_punctnum_rem if token]\n            src.attributes = [token.lower() for token in attributes_punctnum_rem if token]\n            src.method_names = [token.lower() for token in methodnames_punctnum_rem if token]\n            src.variables = [token.lower() for token in variables_punctnum_rem if token]\n            src.file_name = [token.lower() for token in filename_punctnum_rem if token]\n            src.pos_tagged_comments = [token.lower() for token in pos_comments_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in stop_words]\n            src.comments = [token for token in src.comments if token not in stop_words]\n            src.class_names = [token for token in src.class_names if token not in stop_words]\n            src.attributes = [token for token in src.attributes if token not in stop_words]\n            src.method_names = [token for token in src.method_names if token not in stop_words]\n            src.variables = [token for token in src.variables if token not in stop_words]\n            src.file_name = [token for token in src.file_name if token not in stop_words]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in stop_words]\n\n    def remove_javakeywords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in java_keywords]\n            src.comments = [token for token in src.comments if token not in java_keywords]\n            src.class_names = [token for token in src.class_names if token not in java_keywords]\n            src.attributes = [token for token in src.attributes if token not in java_keywords]\n            src.method_names = [token for token in src.method_names if token not in java_keywords]\n            src.variables = [token for token in src.variables if token not in java_keywords]\n            src.file_name = [token for token in src.file_name if token not in java_keywords]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for src in self.src_files.values():\n            src.all_content = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.all_content], src.all_content]))\n            src.comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.comments], src.comments]))\n            src.class_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.class_names], src.class_names]))\n            src.attributes = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.attributes], src.attributes]))\n            src.method_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.method_names], src.method_names]))\n            src.variables = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.variables], src.variables]))\n            src.file_name = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.file_name], src.file_name]))\n            src.pos_tagged_comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.pos_tagged_comments], src.pos_tagged_comments]))\n\n\n    def preprocess(self):\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_javakeywords()\n        self.stem()","metadata":{"id":"_22yeS4wcWpU","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:32.943048Z","iopub.execute_input":"2025-04-04T06:34:32.943412Z","iopub.status.idle":"2025-04-04T06:34:32.994445Z","shell.execute_reply.started":"2025-04-04T06:34:32.943365Z","shell.execute_reply":"2025-04-04T06:34:32.993706Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install inflection\nimport inflection\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ACZrz5Byh7Ur","outputId":"68b0fd14-1ac6-484a-cb84-2ab028bd5ed6","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:32.995725Z","iopub.execute_input":"2025-04-04T06:34:32.996037Z","iopub.status.idle":"2025-04-04T06:34:36.537829Z","shell.execute_reply.started":"2025-04-04T06:34:32.996017Z","shell.execute_reply":"2025-04-04T06:34:36.536817Z"}},"outputs":[{"name":"stdout","text":"Collecting inflection\n  Downloading inflection-0.5.1-py2.py3-none-any.whl.metadata (1.7 kB)\nDownloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\nInstalling collected packages: inflection\nSuccessfully installed inflection-0.5.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')\nimport pickle\nfrom google.colab import drive\nimport csv\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport re\nimport string\nfrom nltk.stem.porter import PorterStemmer","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P9emxprnStnP","outputId":"51497efe-74ae-4abd-d5a0-485188dab6f7","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:36.538945Z","iopub.execute_input":"2025-04-04T06:34:36.539193Z","iopub.status.idle":"2025-04-04T06:34:36.764435Z","shell.execute_reply.started":"2025-04-04T06:34:36.539172Z","shell.execute_reply":"2025-04-04T06:34:36.763832Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 2. Tr√≠ch xu·∫•t d·ªØ li·ªáu","metadata":{"id":"SC6LbKkWy8kR"}},{"cell_type":"code","source":"import pickle\n\n# ƒê∆∞·ªùng d·∫´n ƒë·∫øn c√°c file pickle\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_src_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_src_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_src_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_src_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_src_processed.pkl'\n}\n\n# Load t·ª´ng file v√† l∆∞u v√†o c√°c bi·∫øn t∆∞∆°ng ·ª©ng\ndatasets = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        datasets[name] = pickle.load(f)\n\n# Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c load v√†o c√°c bi·∫øn\nfor name, data in datasets.items():\n    print(f\"Data for {name}:\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O3TGVN1KzAXg","outputId":"63f347a7-1b8d-41fe-98e5-105ca43233e2","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:36.765423Z","iopub.execute_input":"2025-04-04T06:34:36.765683Z","iopub.status.idle":"2025-04-04T06:34:49.551275Z","shell.execute_reply.started":"2025-04-04T06:34:36.765663Z","shell.execute_reply":"2025-04-04T06:34:49.550563Z"}},"outputs":[{"name":"stdout","text":"Data for aspectj:\nData for eclipse:\nData for swt:\nData for tomcat:\nData for birt:\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"eclipse_src = datasets['eclipse']\nbirt_src = datasets['birt']\nswt_src = datasets['swt']\ntomcat_src = datasets['tomcat']\naspectj_src = datasets['aspectj']","metadata":{"id":"b91231aHzUu_","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:49.552217Z","iopub.execute_input":"2025-04-04T06:34:49.552577Z","iopub.status.idle":"2025-04-04T06:34:49.556350Z","shell.execute_reply.started":"2025-04-04T06:34:49.552545Z","shell.execute_reply":"2025-04-04T06:34:49.555735Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load d·ªØ li·ªáu t·ª´ c√°c file pickle ƒë√£ l∆∞u\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_reports_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_reports_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_reports_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_reports_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_reports_processed.pkl'\n}\n\n# Load t·ª´ng dataset v√† l∆∞u v√†o c√°c bi·∫øn\nall_processed_reports = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        all_processed_reports[name] = pickle.load(f)\n\n# Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ load v√†o\nfor dataset, reports in all_processed_reports.items():\n    print(f\"Processed reports for {dataset}:\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kXiYZuNgzv0M","outputId":"053bc2e9-ef09-4aea-9431-acfda03d3300","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:49.558595Z","iopub.execute_input":"2025-04-04T06:34:49.558816Z","iopub.status.idle":"2025-04-04T06:34:52.389639Z","shell.execute_reply.started":"2025-04-04T06:34:49.558796Z","shell.execute_reply":"2025-04-04T06:34:52.388927Z"}},"outputs":[{"name":"stdout","text":"Processed reports for aspectj:\nProcessed reports for eclipse:\nProcessed reports for swt:\nProcessed reports for tomcat:\nProcessed reports for birt:\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"eclipse_reports = all_processed_reports['eclipse']\nbirt_reports = all_processed_reports['birt']\nswt_reports = all_processed_reports['swt']\ntomcat_reports = all_processed_reports['tomcat']\naspectj_reports = all_processed_reports['aspectj']","metadata":{"id":"iAUv0Bnv0FcI","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:52.390997Z","iopub.execute_input":"2025-04-04T06:34:52.391221Z","iopub.status.idle":"2025-04-04T06:34:52.394864Z","shell.execute_reply.started":"2025-04-04T06:34:52.391202Z","shell.execute_reply":"2025-04-04T06:34:52.394090Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Gƒê1: Chu·∫©n b·ªã\n- S·∫Øp x·∫øp bug report theo th·ªùi gian (report_time)\n- Chia th√†nh 10 folds\n- T·∫°o training/test dataset theo ki·ªÉu fold i ‚Üí fold i+1\n- G√°n nh√£n cho t·ª´ng c·∫∑p (bug report, source file)","metadata":{"id":"-hwWTIRJ9PXd"}},{"cell_type":"code","source":"# B1: L·∫•y danh s√°ch (bug_id, bug_report), sau ƒë√≥ s·∫Øp x·∫øp theo report_time\nsorted_bug_reports = sorted(aspectj_reports.items(), key=lambda x: x[1].report_time)\ndata_src = aspectj_src","metadata":{"id":"HrKZiEgO9X16","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:52.395576Z","iopub.execute_input":"2025-04-04T06:34:52.395769Z","iopub.status.idle":"2025-04-04T06:34:52.406296Z","shell.execute_reply.started":"2025-04-04T06:34:52.395752Z","shell.execute_reply":"2025-04-04T06:34:52.405573Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def split_into_folds(sorted_reports, num_folds=10):\n    fold_size = len(sorted_reports) // num_folds\n    folds = [sorted_reports[i*fold_size:(i+1)*fold_size] for i in range(num_folds)]\n\n    # N·∫øu c√≤n d∆∞, r·∫£i ƒë·ªÅu v√†o c√°c fold ƒë·∫ßu\n    remainder = sorted_reports[num_folds*fold_size:]\n    for i, extra in enumerate(remainder):\n        folds[i].append(extra)\n    return folds\n\ndata_folds = split_into_folds(sorted_bug_reports, num_folds=3)\n","metadata":{"id":"fgRpuQKE9aA2","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:52.407133Z","iopub.execute_input":"2025-04-04T06:34:52.407411Z","iopub.status.idle":"2025-04-04T06:34:52.417554Z","shell.execute_reply.started":"2025-04-04T06:34:52.407366Z","shell.execute_reply":"2025-04-04T06:34:52.416874Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"i = 0 # th·ª≠ v·ªõi fold 0 ‚Üí 1\ntrain_fold = data_folds[i]\ntest_fold = data_folds[i+1]","metadata":{"id":"0itPPQ5O9cDT","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:52.418311Z","iopub.execute_input":"2025-04-04T06:34:52.418538Z","iopub.status.idle":"2025-04-04T06:34:52.428282Z","shell.execute_reply.started":"2025-04-04T06:34:52.418520Z","shell.execute_reply":"2025-04-04T06:34:52.427590Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import random\n\ndef generate_balanced_pairs(bug_fold, source_files, num_negatives_per_positive=50):\n    data = []\n    for bug_id, bug in bug_fold:\n        # Danh s√°ch file ch·ª©a bug (poszqitive)\n        positive_paths = set(bug.fixed_files)\n        positive = [\n            (bug_id, bug, src_path, source_files[src_path], 1)\n            for src_path in positive_paths if src_path in source_files\n        ]\n\n        # Danh s√°ch file c√≤n l·∫°i ƒë·ªÉ l·∫•y negative\n        all_paths = list(source_files.keys())\n        negative_paths = list(set(all_paths) - positive_paths)\n        sampled_negatives = random.sample(negative_paths, min(num_negatives_per_positive * len(positive), len(negative_paths)))\n\n        negative = [\n            (bug_id, bug, src_path, source_files[src_path], 0)\n            for src_path in sampled_negatives if src_path in source_files\n        ]\n\n        data.extend(positive + negative)\n    return data\ntrain_pairs = generate_balanced_pairs(train_fold, data_src, num_negatives_per_positive=50)\ntest_pairs = generate_balanced_pairs(test_fold, data_src, num_negatives_per_positive=50)\n","metadata":{"id":"wjLEL9mR9fPv","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:52.429169Z","iopub.execute_input":"2025-04-04T06:34:52.429461Z","iopub.status.idle":"2025-04-04T06:34:52.838324Z","shell.execute_reply.started":"2025-04-04T06:34:52.429435Z","shell.execute_reply":"2025-04-04T06:34:52.837458Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"X·ª≠ l√≠ m·∫•t c√¢n b·∫±ng","metadata":{"id":"phVI5NPj-GMs"}},{"cell_type":"code","source":"def compute_stats(pairs):\n    total = len(pairs)\n    pos = sum(1 for _, _, _, _, label in pairs if label == 1)\n    neg = total - pos\n    ratio = pos / total if total > 0 else 0\n    return total, pos, neg, ratio\n\ntotal, pos, neg, ratio = compute_stats(train_pairs)\nprint(\"üìä Train Set:\")\nprint(f\"  ‚û§ T·ªïng c·∫∑p: {total}\")\nprint(f\"  ‚úÖ Positive (label=1): {pos}\")\nprint(f\"  ‚ùå Negative (label=0): {neg}\")\nprint(f\"  ‚öñÔ∏è T·ª∑ l·ªá positive: {ratio:.4f}\")\n\ntotal, pos, neg, ratio = compute_stats(test_pairs)\nprint(\"\\nüß™ Test Set:\")\nprint(f\"  ‚û§ T·ªïng c·∫∑p: {total}\")\nprint(f\"  ‚úÖ Positive (label=1): {pos}\")\nprint(f\"  ‚ùå Negative (label=0): {neg}\")\nprint(f\"  ‚öñÔ∏è T·ª∑ l·ªá positive: {ratio:.4f}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-m4H5d79uzp","outputId":"1a4a413d-189b-4b5d-ca55-6552e56ce33f","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:52.839412Z","iopub.execute_input":"2025-04-04T06:34:52.839719Z","iopub.status.idle":"2025-04-04T06:34:52.849881Z","shell.execute_reply.started":"2025-04-04T06:34:52.839690Z","shell.execute_reply":"2025-04-04T06:34:52.849042Z"}},"outputs":[{"name":"stdout","text":"üìä Train Set:\n  ‚û§ T·ªïng c·∫∑p: 8619\n  ‚úÖ Positive (label=1): 169\n  ‚ùå Negative (label=0): 8450\n  ‚öñÔ∏è T·ª∑ l·ªá positive: 0.0196\n\nüß™ Test Set:\n  ‚û§ T·ªïng c·∫∑p: 8568\n  ‚úÖ Positive (label=1): 168\n  ‚ùå Negative (label=0): 8400\n  ‚öñÔ∏è T·ª∑ l·ªá positive: 0.0196\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import random\n\n# H√†m 1: T·∫°o batches c√≥ bootstrapping (lu√¥n ch·ª©a √≠t nh·∫•t 1 positive sample)\ndef create_bootstrapped_batches(pairs, batch_size=128):\n    # T√°ch positive v√† negative\n    positives = [p for p in pairs if p[-1] == 1]\n    negatives = [p for p in pairs if p[-1] == 0]\n\n    batches = []\n    # T√≠nh s·ªë batch c√≥ th·ªÉ t·∫°o\n    total_batches = len(pairs) // batch_size\n\n    for _ in range(total_batches):\n        # Lu√¥n ch·ªçn √≠t nh·∫•t 1 positive\n        pos_sample = random.choice(positives)\n\n        # Ch·ªçn ng·∫´u nhi√™n batch_size - 1 negative samples\n        neg_samples = random.sample(negatives, batch_size - 1)\n\n        # G·ªôp l·∫°i, shuffle ƒë·ªÉ positive kh√¥ng ƒë·ª©ng ƒë·∫ßu\n        batch = [pos_sample] + neg_samples\n        random.shuffle(batch)\n\n        batches.append(batch)\n\n    return batches\n\n# H√†m 2: Focal Loss Function (PyTorch-style pseudocode, kh√¥ng ch·∫°y ƒë∆∞·ª£c ·ªü ƒë√¢y)\ndef focal_loss(predictions, targets, alpha=0.999, gamma=2.0, eps=1e-6):\n    \"\"\"\n    predictions: tensor (batch_size,) - output sigmoid from model\n    targets: tensor (batch_size,) - true labels (0 or 1)\n    \"\"\"\n    # Avoid log(0)\n    predictions = predictions.clamp(min=eps, max=1.0 - eps)\n\n    # Compute focal loss\n    loss = -alpha * (1 - predictions)**gamma * targets * predictions.log() \\\n           - (1 - alpha) * predictions**gamma * (1 - targets) * (1 - predictions).log()\n    return loss.mean()","metadata":{"id":"VVi_cbQD-a1b","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:52.850689Z","iopub.execute_input":"2025-04-04T06:34:52.850899Z","iopub.status.idle":"2025-04-04T06:34:52.861662Z","shell.execute_reply.started":"2025-04-04T06:34:52.850880Z","shell.execute_reply":"2025-04-04T06:34:52.861054Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# B·∫Øt ƒë·∫ßu ph·∫ßn ti·∫øp theo: tr√≠ch xu·∫•t 5 ƒë·∫∑c tr∆∞ng cho t·ª´ng c·∫∑p (bug report, source file)\n\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# H√†m x·ª≠ l√Ω text g·ªôp l·∫°i t·ª´ bug report\ndef bug_to_text(bug):\n    summary = bug.summary['unstemmed'] if isinstance(bug.summary, dict) else bug.summary\n    desc = bug.description['unstemmed'] if isinstance(bug.description, dict) else bug.description\n    return \" \".join(summary + desc)\n\n# H√†m x·ª≠ l√Ω text t·ª´ source file\ndef src_to_text(src):\n    content = src.all_content['unstemmed'] if isinstance(src.all_content, dict) else src.all_content\n    comments = src.comments['unstemmed'] if isinstance(src.comments, dict) else src.comments\n    return \" \".join(content + comments)\n\n# H√†m t√≠nh feature 1: lexical similarity (TF-IDF cosine)\ndef compute_lexical_similarity(pairs):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # G·ªôp c·∫£ bug + src l·∫°i ƒë·ªÉ fit chung vectorizer\n    combined = bug_texts + src_texts\n    tfidf = TfidfVectorizer()\n    tfidf_matrix = tfidf.fit_transform(combined)\n\n    # T√°ch ri√™ng l·∫°i t·ª´ng ph·∫ßn\n    bug_vecs = tfidf_matrix[:len(pairs)]\n    src_vecs = tfidf_matrix[len(pairs):]\n\n    # T√≠nh cosine cho t·ª´ng c·∫∑p (theo h√†ng t∆∞∆°ng ·ª©ng)\n    similarities = cosine_similarity(bug_vecs, src_vecs).diagonal()\n\n    return similarities\n\n# Demo: t√≠nh feature lexical similarity cho train_pairs (gi·ªõi h·∫°n 500 m·∫´u v√¨ t·ªëc ƒë·ªô)\nsampled_pairs = train_pairs[:500]\nlexical_sim = compute_lexical_similarity(sampled_pairs)\n\n# Tr·∫£ v·ªÅ d∆∞·ªõi d·∫°ng numpy array\nlexical_sim[:10]  # Tr√≠ch 10 gi√° tr·ªã ƒë·∫ßu ti√™n ƒë·ªÉ test\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4en89sb6-s54","outputId":"5c14ad2a-da00-495a-ec81-38687265bcfe","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:52.862483Z","iopub.execute_input":"2025-04-04T06:34:52.862761Z","iopub.status.idle":"2025-04-04T06:34:53.041424Z","shell.execute_reply.started":"2025-04-04T06:34:52.862733Z","shell.execute_reply":"2025-04-04T06:34:53.040558Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"array([0.00139219, 0.        , 0.00147321, 0.0005491 , 0.        ,\n       0.03185508, 0.        , 0.        , 0.        , 0.02370916])"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# T·∫£i GloVe 6B (b·∫£n 100d ~160MB)\n!wget http://nlp.stanford.edu/data/glove.6B.zip -P /content/\n\n# Gi·∫£i n√©n\n!unzip /content/glove.6B.zip -d /content/glove\n\n# Load GloVe 100d v√†o dictionary\nimport numpy as np\n\ndef load_glove_embeddings(filepath):\n    embeddings = {}\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.strip().split()\n            word = values[0]\n            vector = np.array(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n\n# ƒê∆∞·ªùng d·∫´n t·ªõi GloVe 100d\nglove_path = \"/content/glove/glove.6B.100d.txt\"\nglove_embeddings = load_glove_embeddings(glove_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":632},"id":"6FexoFrK_LTE","outputId":"b7e7b200-954e-4541-a27b-dc43607ae8f2","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:34:53.042340Z","iopub.execute_input":"2025-04-04T06:34:53.042665Z","iopub.status.idle":"2025-04-04T06:38:04.385591Z","shell.execute_reply.started":"2025-04-04T06:34:53.042636Z","shell.execute_reply":"2025-04-04T06:38:04.384673Z"}},"outputs":[{"name":"stdout","text":"--2025-04-04 06:34:53--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2025-04-04 06:34:53--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2025-04-04 06:34:53--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: ‚Äò/content/glove.6B.zip‚Äô\n\nglove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 41s  \n\n2025-04-04 06:37:35 (5.09 MB/s) - ‚Äò/content/glove.6B.zip‚Äô saved [862182613/862182613]\n\nArchive:  /content/glove.6B.zip\n  inflating: /content/glove/glove.6B.50d.txt  \n  inflating: /content/glove/glove.6B.100d.txt  \n  inflating: /content/glove/glove.6B.200d.txt  \n  inflating: /content/glove/glove.6B.300d.txt  \n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# H√†m: T√≠nh semantic similarity d√πng TF-IDF weighted average of GloVe vectors\ndef compute_semantic_similarity(pairs, glove_dict, dim=100):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # D√πng TF-IDF ƒë·ªÉ l·∫•y tr·ªçng s·ªë t·ª´\n    tfidf = TfidfVectorizer()\n    tfidf.fit(bug_texts + src_texts)\n    vocab = tfidf.vocabulary_\n    idf_weights = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n\n    def embed_text(text):\n        tokens = text.split()\n        vecs = []\n        weights = []\n        for token in tokens:\n            if token in glove_dict and token in vocab:\n                vecs.append(glove_dict[token])\n                weights.append(idf_weights[token])\n        if not vecs:\n            return np.zeros(dim)\n        vecs = np.array(vecs)\n        weights = np.array(weights).reshape(-1, 1)\n        weighted_vecs = vecs * weights\n        return weighted_vecs.sum(axis=0) / weights.sum()\n\n    # T√≠nh vector trung b√¨nh cho bug v√† src\n    bug_vecs = [embed_text(text) for text in bug_texts]\n    src_vecs = [embed_text(text) for text in src_texts]\n\n    # T√≠nh cosine similarity gi·ªØa t·ª´ng c·∫∑p\n    similarities = [cosine_similarity([b], [s])[0][0] for b, s in zip(bug_vecs, src_vecs)]\n\n    return np.array(similarities)\nsampled_pairs = train_pairs[:500]\n# Demo tr√™n 500 c·∫∑p nh∆∞ lexical\nsemantic_sim = compute_semantic_similarity(sampled_pairs, glove_embeddings)\nsemantic_sim[:10]\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BmKfMQ9oAFSQ","outputId":"e0bf60b5-8bc7-401f-c8ee-5e8ac04ebe01","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:38:04.386880Z","iopub.execute_input":"2025-04-04T06:38:04.387221Z","iopub.status.idle":"2025-04-04T06:38:04.947629Z","shell.execute_reply.started":"2025-04-04T06:38:04.387189Z","shell.execute_reply":"2025-04-04T06:38:04.946703Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"array([0.77560948, 0.69013631, 0.74282996, 0.78909787, 0.73398599,\n       0.53522624, 0.64663719, 0.48919437, 0.61371504, 0.64618377])"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"from datetime import datetime\n\n# B·ªï tr·ª£: t·∫°o map file_path -> list of (bug_id, report_time, bug_text)\ndef build_bug_fix_history(pairs):\n    history = {}\n    for bug_id, bug, src_path, _, label in pairs:\n        if label == 1:  # ch·ªâ t√≠nh c√°c bug th·∫≠t s·ª± s·ª≠a file\n            if src_path not in history:\n                history[src_path] = []\n            history[src_path].append((bug_id, bug.report_time, bug_to_text(bug)))\n    return history\n\n# ƒê·∫∑c tr∆∞ng 3: Similar Bug Report Score\ndef compute_similar_bug_score(pairs, history):\n    scores = []\n    for bug_id, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        current_text = bug_to_text(bug)\n\n        sim_scores = []\n        if src_path in history:\n            for hist_bug_id, hist_time, hist_text in history[src_path]:\n                if hist_time < current_time:  # ch·ªâ t√≠nh bug trong qu√° kh·ª©\n                    sim = cosine_similarity(\n                        TfidfVectorizer().fit_transform([current_text, hist_text])\n                    )[0, 1]\n                    sim_scores.append(sim)\n        scores.append(max(sim_scores) if sim_scores else 0.0)\n    return np.array(scores)\n\n# ƒê·∫∑c tr∆∞ng 4: Time Since Last Fix (ng√†y, normalize)\n# ƒê·∫∑c tr∆∞ng 4: Time Since Last Fix (ng√†y, normalize)\ndef compute_time_since_last_fix(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_times = [hist_time for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            if past_times:\n                delta_days = (current_time - max(past_times)).days\n            else:\n                delta_days = 9999  # C·ª±c l·ªõn n·∫øu ch∆∞a t·ª´ng s·ª≠a\n        else:\n            delta_days = 9999\n        scores.append(delta_days)\n\n    # Normalize v·ªÅ [0,1]\n    max_days = max(scores) if max(scores) != 0 else 1  # Tr√°nh chia cho 0\n\n    return np.array([1 - (s / max_days) for s in scores])\n\n# ƒê·∫∑c tr∆∞ng 5: Fix Frequency (s·ªë l·∫ßn b·ªã s·ª≠a trong qu√° kh·ª©, normalize)\ndef compute_fix_frequency(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_fixes = [1 for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            freq = len(past_fixes)\n        else:\n            freq = 0\n        scores.append(freq)\n    # Normalize v·ªÅ [0,1] an to√†n\n    max_freq = max(scores)\n    max_freq = max(max_freq, 1)  # tr√°nh chia 0\n    return np.array([s / max_freq for s in scores])\n\n\n# D√πng cho 500 c·∫∑p m·∫´u\nsampled_pairs = train_pairs[:5000]\nbug_history = build_bug_fix_history(train_pairs)\n\nsimilar_bug_score = compute_similar_bug_score(sampled_pairs, bug_history)\ntime_since_last_fix = compute_time_since_last_fix(sampled_pairs, bug_history)\nfix_frequency = compute_fix_frequency(sampled_pairs, bug_history)\n\n# Tr√≠ch 5 gi√° tr·ªã ƒë·∫ßu m·ªói feature\nsimilar_bug_score[:50], time_since_last_fix[:50], fix_frequency[:50]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"imb4_du_Bgjz","outputId":"8c08c8e9-7047-4e5d-de85-8b70e30cbe28","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:38:04.948790Z","iopub.execute_input":"2025-04-04T06:38:04.949123Z","iopub.status.idle":"2025-04-04T06:38:05.123124Z","shell.execute_reply.started":"2025-04-04T06:38:04.949091Z","shell.execute_reply":"2025-04-04T06:38:05.122320Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# ‚úÖ B∆∞·ªõc 1: G·ªôp 5 ƒë·∫∑c tr∆∞ng l·∫°i th√†nh feature matrix X (n_samples, 5)\ndef build_feature_matrix(pairs, glove_dict, history):\n    lexical = compute_lexical_similarity(pairs)\n    semantic = compute_semantic_similarity(pairs, glove_dict)\n    similar_score = compute_similar_bug_score(pairs, history)\n    recency = compute_time_since_last_fix(pairs, history)\n    freq = compute_fix_frequency(pairs, history)\n\n    # G·ªôp l·∫°i theo chi·ªÅu d·ªçc ‚Üí ma tr·∫≠n (n_samples, 5)\n    X = np.stack([lexical, semantic, similar_score, recency, freq], axis=1)\n\n    return X\n\n# ‚úÖ B∆∞·ªõc 2: T·∫°o nh√£n y\ndef get_labels(pairs):\n    return np.array([label for *_, label in pairs])\n\n# ‚úÖ T·∫°o d·ªØ li·ªáu train t·ª´ sampled_pairs\nX_train = build_feature_matrix(train_pairs, glove_embeddings, bug_history)\ny_train = get_labels(train_pairs)\n\n# ‚úÖ In shape ƒë·ªÉ x√°c nh·∫≠n\nX_train.shape, y_train.shape\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PmEZSTiyDEB4","outputId":"6db11041-f3a2-4931-f264-9480bfd7c2e6","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:38:05.123991Z","iopub.execute_input":"2025-04-04T06:38:05.124244Z","iopub.status.idle":"2025-04-04T06:38:18.532586Z","shell.execute_reply.started":"2025-04-04T06:38:05.124212Z","shell.execute_reply":"2025-04-04T06:38:18.531829Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"((8619, 5), (8619,))"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\n# ‚úÖ B∆∞·ªõc 1: Chu·∫©n b·ªã Dataset & Dataloader t·ª´ numpy\ndef create_dataloader(X, y, batch_size=128):\n    X_tensor = torch.tensor(X, dtype=torch.float32)\n    y_tensor = torch.tensor(y, dtype=torch.float32)\n    dataset = TensorDataset(X_tensor, y_tensor)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# ‚úÖ B∆∞·ªõc 2: ƒê·ªãnh nghƒ©a m√¥ h√¨nh DNN gi·ªëng b√†i b√°o\nclass BugLocalizationDNN(nn.Module):\n    def __init__(self, input_dim=5, hidden_dim=64, output_dim=1):\n        super(BugLocalizationDNN, self).__init__()\n        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # N·∫øu x ch·ªâ c√≥ 2 chi·ªÅu (batch_size, input_dim), h√£y th√™m m·ªôt chi·ªÅu gi·∫£ ƒë·ªãnh (sequence_length=1)\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)  # Th√™m m·ªôt chi·ªÅu gi·∫£ ƒë·ªãnh (batch_size, 1, input_dim)\n\n        # x shape: (batch_size, sequence_length, input_dim)\n        rnn_out, _ = self.rnn(x)\n        # L·∫•y output c·ªßa ph·∫ßn cu·ªëi c√πng trong chu·ªói\n        final_rnn_out = rnn_out[:, -1, :]\n        out = torch.sigmoid(self.fc(final_rnn_out)).squeeze()\n        return out\n\n        \n# ‚úÖ B∆∞·ªõc 3: ƒê·ªãnh nghƒ©a focal loss\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.999, gamma=2.0, eps=1e-6):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, preds, targets):\n        preds = preds.clamp(min=self.eps, max=1. - self.eps)\n        loss = -self.alpha * (1 - preds) ** self.gamma * targets * torch.log(preds) \\\n               - (1 - self.alpha) * preds ** self.gamma * (1 - targets) * torch.log(1 - preds)\n        return loss.mean()\n\n# ‚úÖ B∆∞·ªõc 4: Hu·∫•n luy·ªán m√¥ h√¨nh\ndef train_model(model, dataloader, epochs=10, lr=1e-3):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = FocalLoss()\n\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch_X, batch_y in dataloader:\n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n\n# ‚úÖ T·∫°o dataloader & model, b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán\ntrain_loader = create_dataloader(X_train, y_train, batch_size=128)\nmodel = BugLocalizationDNN()\ntrain_model(model, train_loader, epochs=5)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LiD_wJxjDeQ8","outputId":"9821aee2-a66e-480b-9f69-6e4d207c7ba6","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:38:18.533648Z","iopub.execute_input":"2025-04-04T06:38:18.533965Z","iopub.status.idle":"2025-04-04T06:38:27.857878Z","shell.execute_reply.started":"2025-04-04T06:38:18.533933Z","shell.execute_reply":"2025-04-04T06:38:27.856836Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5 - Loss: 0.1110\nEpoch 2/5 - Loss: 0.0672\nEpoch 3/5 - Loss: 0.0662\nEpoch 4/5 - Loss: 0.0638\nEpoch 5/5 - Loss: 0.0625\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score, label_ranking_average_precision_score\nimport numpy as np\n\n# ‚úÖ B∆∞·ªõc 1: Chu·∫©n b·ªã t·∫≠p test\nsampled_test_pairs = test_pairs\nbug_history_test = build_bug_fix_history(test_pairs)\n\nX_test = build_feature_matrix(sampled_test_pairs, glove_embeddings, bug_history_test)\ny_test = get_labels(sampled_test_pairs)\n\n# ‚úÖ B∆∞·ªõc 2: D·ª± ƒëo√°n x√°c su·∫•t t·ª´ m√¥ h√¨nh\nmodel.eval()\nwith torch.no_grad():\n    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n    y_pred_probs = model(X_test_tensor).numpy()\n\n# ‚úÖ B∆∞·ªõc 3: ƒê√°nh gi√° c√°c ch·ªâ s·ªë (MAP, MRR, Top-k)\ndef compute_topk_accuracy(y_true, y_scores, k=10):\n    bug_to_scores = {}\n    for (bug_id, _, src_path, _, label), score in zip(sampled_test_pairs, y_scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    correct_at_k = 0\n    total = 0\n\n    for bug_id, entries in bug_to_scores.items():\n        sorted_entries = sorted(entries, key=lambda x: x[0], reverse=True)\n        top_k = sorted_entries[:k]\n        if any(label == 1 for _, label in top_k):\n            correct_at_k += 1\n        total += 1\n\n    return correct_at_k / total if total > 0 else 0\n\n# MAP (Mean Average Precision)\nmap_score = average_precision_score(y_test, y_pred_probs)\n\n# MRR (Mean Reciprocal Rank)\ndef mean_reciprocal_rank(pairs, scores):\n    bug_to_scores = {}\n    for (bug_id, _, _, _, label), score in zip(pairs, scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    rr_sum = 0\n    count = 0\n    for bug_id, ranked in bug_to_scores.items():\n        ranked = sorted(ranked, key=lambda x: x[0], reverse=True)\n        for idx, (_, label) in enumerate(ranked):\n            if label == 1:\n                rr_sum += 1 / (idx + 1)\n                break\n        count += 1\n    return rr_sum / count if count > 0 else 0\n\nmrr_score = mean_reciprocal_rank(sampled_test_pairs, y_pred_probs)\n\n# Top-k Accuracy (k = 1, 5, 10, 15)\ntop1 = compute_topk_accuracy(y_test, y_pred_probs, k=1)\ntop5 = compute_topk_accuracy(y_test, y_pred_probs, k=5)\ntop10 = compute_topk_accuracy(y_test, y_pred_probs, k=10)\ntop15 = compute_topk_accuracy(y_test, y_pred_probs, k=15)\n\n# Tr·∫£ v·ªÅ k·∫øt qu·∫£\nmap_score, mrr_score, top1, top5, top10, top15","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":207},"id":"hvK-tE_MD4zq","outputId":"5444ce3e-4624-4f81-eb6b-b19a543b0d49","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:38:27.859018Z","iopub.execute_input":"2025-04-04T06:38:27.859570Z","iopub.status.idle":"2025-04-04T06:38:44.574707Z","shell.execute_reply.started":"2025-04-04T06:38:27.859526Z","shell.execute_reply":"2025-04-04T06:38:44.573793Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(0.5349355575186959,\n 0.6928675607747492,\n 0.5952380952380952,\n 0.8035714285714286,\n 0.8511904761904762,\n 0.8690476190476191)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"import random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import average_precision_score\n\n# ‚úÖ Ki·ªÉm tra v√† x√°c ƒë·ªãnh thi·∫øt b·ªã (GPU ho·∫∑c CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ‚úÖ M√¥ h√¨nh DNN (ho·∫∑c b·∫•t k·ª≥ m√¥ h√¨nh n√†o b·∫°n ƒëang s·ª≠ d·ª•ng)\nclass BugLocalizationDNN(nn.Module):\n    def __init__(self, input_dim=5, hidden_dim=64, output_dim=1):\n        super(BugLocalizationDNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return torch.sigmoid(x).squeeze()\n\n# ‚úÖ Ch·∫°y ƒë√°nh gi√° to√†n b·ªô m√¥ h√¨nh theo ki·ªÉu k-fold nh∆∞ b√†i b√°o\ndef run_kfold_training_and_eval(folds, source_files, glove_dict, k=9):\n    results = {\n        \"fold\": [],\n        \"MAP\": [],\n        \"MRR\": [],\n        \"Top1\": [],\n        \"Top5\": [],\n        \"Top10\": [],\n        \"Top15\": []\n    }\n\n    for i in range(2):\n        print(f\"\\nüì¶ Fold {i} ‚û§ {i+1}\")\n        train_fold = folds[i]\n        test_fold = folds[i + 1]\n\n        train_pairs = generate_balanced_pairs(train_fold, source_files)\n        test_pairs = generate_balanced_pairs(test_fold, source_files)\n\n        # Skip if not enough positive samples\n        if sum(1 for p in train_pairs if p[-1] == 1) < 1:\n            print(\"‚ö†Ô∏è B·ªè qua do qu√° √≠t positive samples\")\n            continue\n\n        # Build history only from train set\n        bug_history = build_bug_fix_history(train_pairs)\n\n        # Feature extraction\n        X_train = build_feature_matrix(train_pairs, glove_dict, bug_history)\n        y_train = get_labels(train_pairs)\n\n        X_test = build_feature_matrix(test_pairs, glove_dict, bug_history)\n        y_test = get_labels(test_pairs)\n\n        # Create batches with bootstrapping\n        train_batches = create_bootstrapped_batches(train_pairs, batch_size=128)\n\n        # Model initialization\n        model = BugLocalizationDNN().to(device)  # Chuy·ªÉn m√¥ h√¨nh sang GPU n·∫øu c√≥\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n        # Training loop\n        model.train()\n        for epoch in range(15):  # Hu·∫•n luy·ªán trong 5 epoch\n            total_loss = 0\n            for batch in train_batches:\n                # L·∫•y ƒë·∫∑c tr∆∞ng (feature) t·ª´ batch v√† chuy·ªÉn sang GPU\n                X_batch = torch.tensor([build_feature_matrix([x], glove_dict, bug_history) for x in batch], dtype=torch.float32).to(device)\n                y_batch = torch.tensor([x[-1] for x in batch], dtype=torch.float32).to(device)\n\n                optimizer.zero_grad()\n                y_pred = model(X_batch)\n                loss = focal_loss(y_pred, y_batch)  # S·ª≠ d·ª•ng Focal Loss\n                loss.backward()\n                optimizer.step()\n\n                total_loss += loss.item()\n\n            print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_batches)}\")\n\n        # Predict\n        model.eval()\n        with torch.no_grad():\n            X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)  # Chuy·ªÉn d·ªØ li·ªáu v√†o GPU\n            y_pred_probs = model(X_test_tensor).cpu().numpy()  # D·ª± ƒëo√°n x√°c su·∫•t v√† chuy·ªÉn v·ªÅ CPU\n\n        # Evaluate\n        sampled_test_pairs = test_pairs\n        map_score = average_precision_score(y_test, y_pred_probs)\n        mrr_score = mean_reciprocal_rank(sampled_test_pairs, y_pred_probs)\n        top1 = compute_topk_accuracy(y_test, y_pred_probs, k=1)\n        top5 = compute_topk_accuracy(y_test, y_pred_probs, k=5)\n        top10 = compute_topk_accuracy(y_test, y_pred_probs, k=10)\n        top15 = compute_topk_accuracy(y_test, y_pred_probs, k=15)\n\n        print(f\"Fold {i + 1} Results:\")\n        print(f\"  ‚û§ MAP: {map_score:.4f}\")\n        print(f\"  ‚û§ MRR: {mrr_score:.4f}\")\n        print(f\"  ‚û§ Top1: {top1:.4f}\")\n        print(f\"  ‚û§ Top5: {top5:.4f}\")\n        print(f\"  ‚û§ Top10: {top10:.4f}\")\n        print(f\"  ‚û§ Top15: {top15:.4f}\")\n\n        # Save results\n        results[\"fold\"].append(i)\n        results[\"MAP\"].append(map_score)\n        results[\"MRR\"].append(mrr_score)\n        results[\"Top1\"].append(top1)\n        results[\"Top5\"].append(top5)\n        results[\"Top10\"].append(top10)\n        results[\"Top15\"].append(top15)\n\n    return results\n\n# ‚úÖ Run K-fold training and evaluation\nfull_results = run_kfold_training_and_eval(data_folds, data_src, glove_embeddings)\n\n# ‚úÖ Output full results\nprint(\"\\nFull Results:\")\nfor key, value in full_results.items():\n    print(f\"{key}: {value}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0esoYB7ME-At","outputId":"cbd0b93b-371f-4886-b013-7532bccb8b96","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:38:44.577479Z","iopub.execute_input":"2025-04-04T06:38:44.577730Z","iopub.status.idle":"2025-04-04T07:01:43.478372Z","shell.execute_reply.started":"2025-04-04T06:38:44.577708Z","shell.execute_reply":"2025-04-04T07:01:43.477642Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nüì¶ Fold 0 ‚û§ 1\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-23-613f724b1215>:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  X_batch = torch.tensor([build_feature_matrix([x], glove_dict, bug_history) for x in batch], dtype=torch.float32).to(device)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss = 0.0009728731698731877\nEpoch 2: Loss = 0.000723954526457324\nEpoch 3: Loss = 0.0006985710725299459\nEpoch 4: Loss = 0.0006789352506774463\nEpoch 5: Loss = 0.0006624745607285746\nEpoch 6: Loss = 0.0006480038647345309\nEpoch 7: Loss = 0.0006351519137996235\nEpoch 8: Loss = 0.0006256753443253796\nEpoch 9: Loss = 0.0006185072568008926\nEpoch 10: Loss = 0.0006127596056130506\nEpoch 11: Loss = 0.0006081714775281222\nEpoch 12: Loss = 0.0006042969523435597\nEpoch 13: Loss = 0.0006010557473602413\nEpoch 14: Loss = 0.0005982917813417405\nEpoch 15: Loss = 0.0005959163392970422\nFold 1 Results:\n  ‚û§ MAP: 0.5400\n  ‚û§ MRR: 0.6886\n  ‚û§ Top1: 0.5952\n  ‚û§ Top5: 0.8155\n  ‚û§ Top10: 0.8512\n  ‚û§ Top15: 0.8988\n\nüì¶ Fold 1 ‚û§ 2\nEpoch 1: Loss = 0.0009712243847774737\nEpoch 2: Loss = 0.0007326290475303364\nEpoch 3: Loss = 0.0006974687372955183\nEpoch 4: Loss = 0.0006689022499785731\nEpoch 5: Loss = 0.0006486464117188007\nEpoch 6: Loss = 0.0006369266577001491\nEpoch 7: Loss = 0.0006295818783462781\nEpoch 8: Loss = 0.0006249125444534884\nEpoch 9: Loss = 0.0006217288592421082\nEpoch 10: Loss = 0.0006196219787163885\nEpoch 11: Loss = 0.00061810224239404\nEpoch 12: Loss = 0.0006170164141980367\nEpoch 13: Loss = 0.0006162180357161119\nEpoch 14: Loss = 0.0006155318419173868\nEpoch 15: Loss = 0.0006150651425934594\nFold 2 Results:\n  ‚û§ MAP: 0.5334\n  ‚û§ MRR: 0.7332\n  ‚û§ Top1: 0.6429\n  ‚û§ Top5: 0.8571\n  ‚û§ Top10: 0.9226\n  ‚û§ Top15: 0.9226\n\nFull Results:\nfold: [0, 1]\nMAP: [0.5399566173819796, 0.5333512925272238]\nMRR: [0.6886239204042872, 0.7332203875650558]\nTop1: [0.5952380952380952, 0.6428571428571429]\nTop5: [0.8154761904761905, 0.8571428571428571]\nTop10: [0.8511904761904762, 0.9226190476190477]\nTop15: [0.8988095238095238, 0.9226190476190477]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"\n# In k·∫øt qu·∫£ t·ªïng h·ª£p sau khi ch·∫°y t·∫•t c·∫£ folds\nprint(\"\\nüìä K·∫øt qu·∫£ t·ªïng h·ª£p:\")\nfor i in range(len(full_results[\"fold\"])):\n    print(f\"Fold {full_results['fold'][i]}:\")\n    print(f\"  ‚û§ MAP: {full_results['MAP'][i]:.4f}\")\n    print(f\"  ‚û§ MRR: {full_results['MRR'][i]:.4f}\")\n    print(f\"  ‚û§ Top1: {full_results['Top1'][i]:.4f}\")\n    print(f\"  ‚û§ Top5: {full_results['Top5'][i]:.4f}\")\n    print(f\"  ‚û§ Top10: {full_results['Top10'][i]:.4f}\")\n    print(f\"  ‚û§ Top15: {full_results['Top15'][i]:.4f}\")\n\n# T√≠nh trung b√¨nh cho t·∫•t c·∫£ c√°c ch·ªâ s·ªë\nmean_map = np.mean(full_results[\"MAP\"])\nmean_mrr = np.mean(full_results[\"MRR\"])\nmean_top1 = np.mean(full_results[\"Top1\"])\nmean_top5 = np.mean(full_results[\"Top5\"])\nmean_top10 = np.mean(full_results[\"Top10\"])\nmean_top15 = np.mean(full_results[\"Top15\"])\n\n# In k·∫øt qu·∫£ trung b√¨nh\nprint(\"\\nüìä K·∫øt qu·∫£ trung b√¨nh tr√™n to√†n b·ªô k-folds:\")\nprint(f\"  ‚û§ MAP: {mean_map:.4f}\")\nprint(f\"  ‚û§ MRR: {mean_mrr:.4f}\")\nprint(f\"  ‚û§ Top1: {mean_top1:.4f}\")\nprint(f\"  ‚û§ Top5: {mean_top5:.4f}\")\nprint(f\"  ‚û§ Top10: {mean_top10:.4f}\")\nprint(f\"  ‚û§ Top15: {mean_top15:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:01:43.479770Z","iopub.execute_input":"2025-04-04T07:01:43.480142Z","iopub.status.idle":"2025-04-04T07:01:43.491176Z","shell.execute_reply.started":"2025-04-04T07:01:43.480099Z","shell.execute_reply":"2025-04-04T07:01:43.490257Z"}},"outputs":[{"name":"stdout","text":"\nüìä K·∫øt qu·∫£ t·ªïng h·ª£p:\nFold 0:\n  ‚û§ MAP: 0.5400\n  ‚û§ MRR: 0.6886\n  ‚û§ Top1: 0.5952\n  ‚û§ Top5: 0.8155\n  ‚û§ Top10: 0.8512\n  ‚û§ Top15: 0.8988\nFold 1:\n  ‚û§ MAP: 0.5334\n  ‚û§ MRR: 0.7332\n  ‚û§ Top1: 0.6429\n  ‚û§ Top5: 0.8571\n  ‚û§ Top10: 0.9226\n  ‚û§ Top15: 0.9226\n\nüìä K·∫øt qu·∫£ trung b√¨nh tr√™n to√†n b·ªô k-folds:\n  ‚û§ MAP: 0.5367\n  ‚û§ MRR: 0.7109\n  ‚û§ Top1: 0.6190\n  ‚û§ Top5: 0.8363\n  ‚û§ Top10: 0.8869\n  ‚û§ Top15: 0.9107\n","output_type":"stream"}],"execution_count":24}]}