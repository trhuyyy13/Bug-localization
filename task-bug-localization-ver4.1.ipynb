{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":11265211,"sourceType":"datasetVersion","datasetId":7041476},{"sourceId":11280721,"sourceType":"datasetVersion","datasetId":7052679}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Load d·ªØ li·ªáu ƒë√£ x·ª≠ l√≠","metadata":{"id":"-eUizc5QmpX4"}},{"cell_type":"code","source":"!pip install nltk\n","metadata":{"id":"9qfVWdges0io","outputId":"e6051f73-0867-4eb4-b5c0-984722d836d3","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:18:56.166825Z","iopub.execute_input":"2025-04-09T13:18:56.167211Z","iopub.status.idle":"2025-04-09T13:19:02.708598Z","shell.execute_reply.started":"2025-04-09T13:18:56.167180Z","shell.execute_reply":"2025-04-09T13:19:02.707290Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import nltk\nnltk.download('averaged_perceptron_tagger_eng')","metadata":{"id":"8SUNdusXgQ-3","outputId":"937573b4-53fb-4bb5-9c39-84eb53bc1890","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:02.710434Z","iopub.execute_input":"2025-04-09T13:19:02.710894Z","iopub.status.idle":"2025-04-09T13:19:05.612794Z","shell.execute_reply.started":"2025-04-09T13:19:02.710850Z","shell.execute_reply":"2025-04-09T13:19:05.611684Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# English stop words\nstop_words = set(\n    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n     'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',\n     'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',\n     'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n     'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n     'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n     'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n     'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n     'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n     'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n     'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n     'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n     's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o',\n     're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven',\n     'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won',\n     'wouldn', 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'n', 'p', 'q', 'u', 'v',\n     'w', 'x', 'z', 'us'])\n\n# Java language keywords\njava_keywords = set(\n    ['abstract', 'assert', 'boolean', 'break', 'byte', 'case',\n     'catch', 'char', 'class', 'const', 'continue', 'default', 'do', 'double',\n     'else', 'enum', 'extends', 'false', 'final', 'finally', 'float', 'for', 'goto',\n     'if', 'implements', 'import', 'instanceof', 'int', 'interface', 'long',\n     'native', 'new', 'null', 'package', 'private', 'protected', 'public', 'return',\n     'short', 'static', 'strictfp', 'super', 'switch', 'synchronized', 'this',\n     'throw', 'throws', 'transient', 'true', 'try', 'void', 'volatile', 'while'])\n\nfrom collections import namedtuple\nfrom pathlib import Path\n\n# Dataset root directory (ƒëi·ªÅu ch·ªânh ƒë∆∞·ªùng d·∫´n n·∫øu c·∫ßn)\n_DATASET_ROOT = Path('/content/drive/MyDrive/Colab Notebooks/NLP/Task bug localization/')\n\nDataset = namedtuple('Dataset', ['name', 'src', 'bug_repo', 'repo_url', 'features'])\n\n# C√°c dataset ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\naspectj = Dataset(\n    'aspectj',\n    _DATASET_ROOT / 'source files/org.aspectj',\n    _DATASET_ROOT / 'bug reports/AspectJ.txt',\n    \"https://github.com/eclipse/org.aspectj/tree/bug433351.git\",\n    _DATASET_ROOT / 'bug reports/AspectJ.xlsx'\n)\n\neclipse = Dataset(\n    'eclipse',\n    _DATASET_ROOT / 'source files/eclipse.platform.ui-johna-402445',\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.txt',\n    \"https://github.com/eclipse/eclipse.platform.ui.git\",\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.xlsx'\n)\n\nswt = Dataset(\n    'swt',\n    _DATASET_ROOT / 'source files/eclipse.platform.swt-xulrunner-31',\n    _DATASET_ROOT / 'bug reports/SWT.txt',\n    \"https://github.com/eclipse/eclipse.platform.swt.git\",\n    _DATASET_ROOT / 'bug reports/SWT.xlsx'\n)\n\ntomcat = Dataset(\n    'tomcat',\n    _DATASET_ROOT / 'source files/tomcat-7.0.51',\n    _DATASET_ROOT / 'bug reports/Tomcat.txt',\n    \"https://github.com/apache/tomcat.git\",\n    _DATASET_ROOT / 'bug reports/Tomcat.xlsx'\n)\n\nbirt = Dataset(\n    'birt',\n    _DATASET_ROOT / 'source files/birt-20140211-1400',\n    _DATASET_ROOT / 'bug reports/Birt.txt',\n    \"https://github.com/apache/birt.git\",\n    _DATASET_ROOT / 'bug reports/Birt.xlsx'\n)\n\n\n### Current dataset in use. (change this name to change the dataset)\nDATASET = tomcat\n\nclass BugReport:\n    \"\"\"Class representing each bug report\"\"\"\n    __slots__ = ['summary', 'description', 'fixed_files', 'report_time', 'pos_tagged_summary', 'pos_tagged_description','stack_traces','stack_traces_remove']\n\n    def __init__(self, summary, description, fixed_files, report_time):\n        self.summary = summary\n        self.description = description\n        self.fixed_files = fixed_files\n        self.report_time = report_time\n        self.pos_tagged_summary = None\n        self.pos_tagged_description = None\n        self.stack_traces = None\n        self.stack_traces_remove = None\n\nclass SourceFile:\n    \"\"\"Class representing each source file\"\"\"\n    __slots__ = ['all_content', 'comments', 'class_names', 'attributes', 'method_names', 'variables', 'file_name',\n                 'pos_tagged_comments', 'exact_file_name', 'package_name']\n\n    def __init__(self, all_content, comments, class_names, attributes, method_names, variables, file_name,\n                 package_name):\n        self.all_content = all_content\n        self.comments = comments\n        self.class_names = class_names\n        self.attributes = attributes\n        self.method_names = method_names\n        self.variables = variables\n        self.file_name = file_name\n        self.exact_file_name = file_name[0]\n        self.package_name = package_name\n        self.pos_tagged_comments = None\n\n\nclass Parser:\n    \"\"\"Class containing different parsers\"\"\"\n    __slots__ = ['name', 'src', 'bug_repo']\n\n    def __init__(self, pro):\n        self.name = pro.name\n        self.src = pro.src\n        self.bug_repo = pro.bug_repo\n\n    def report_parser(self):\n        reader = csv.DictReader(open(self.bug_repo, \"r\"), delimiter=\"\\t\")\n        bug_reports = OrderedDict()\n        # raw_texts = []\n        # fixed_files = []\n        for line in reader:\n            # line[\"raw_text\"] = line[\"summary\"] + ' ' + line[\"description\"]\n            line[\"report_time\"] = datetime.strptime(line[\"report_time\"], \"%Y-%m-%d %H:%M:%S\")\n            temp = line[\"files\"].strip().split(\".java\")\n            length = len(temp)\n            x = []\n            for i, f in enumerate(temp):\n                if i == (length - 1):\n                    x.append(os.path.normpath(f))\n                    continue\n                x.append(os.path.normpath(f + \".java\"))\n            line[\"files\"] = x\n            bug_reports[line[\"bug_id\"]] = BugReport(line[\"summary\"], line[\"description\"], line[\"files\"],\n                                                    line[\"report_time\"])\n        # bug_reports = tsv2dict(self.bug_repo)\n\n        return bug_reports\n\n    def src_parser(self):\n        \"\"\"Parse source code directory of a program and colect its java files\"\"\"\n\n        # Gettting the list of source files recursively from the source directory\n        src_addresses = glob.glob(str(self.src) + '/**/*.java', recursive=True)\n        print(src_addresses)\n        # Creating a java lexer instance for pygments.lex() method\n        java_lexer = JavaLexer()\n        src_files = OrderedDict()\n        # src_files = dict()\n        # Looping to parse each source file\n        for src_file in src_addresses:\n            with open(src_file, encoding='latin-1') as file:\n                src = file.read()\n\n            # Placeholder for different parts of a source file\n            comments = ''\n            class_names = []\n            attributes = []\n            method_names = []\n            variables = []\n\n            # Source parsing\n            parse_tree = None\n            try:\n                parse_tree = javalang.parse.parse(src)\n                for path, node in parse_tree.filter(javalang.tree.VariableDeclarator):\n                    if isinstance(path[-2], javalang.tree.FieldDeclaration):\n                        attributes.append(node.name)\n                    elif isinstance(path[-2], javalang.tree.VariableDeclaration):\n                        variables.append(node.name)\n            except:\n                pass\n\n            # Triming the source file\n            ind = False\n            if parse_tree:\n                if parse_tree.imports:\n                    last_imp_path = parse_tree.imports[-1].path\n                    src = src[src.index(last_imp_path) + len(last_imp_path) + 1:]\n                elif parse_tree.package:\n                    package_name = parse_tree.package.name\n                    src = src[src.index(package_name) + len(package_name) + 1:]\n                else:  # no import and no package declaration\n                    ind = True\n            # javalang can't parse the source file\n            else:\n                ind = True\n\n            # Lexically tokenize the source file\n            lexed_src = pygments.lex(src, java_lexer)\n\n            for i, token in enumerate(lexed_src):\n                if token[0] in Token.Comment:\n                    if ind and i == 0 and token[0] is Token.Comment.Multiline:\n                        src = src[src.index(token[1]) + len(token[1]):]\n                        continue\n                    comments = comments + token[1]\n                elif token[0] is Token.Name.Class:\n                    class_names.append(token[1])\n                elif token[0] is Token.Name.Function:\n                    method_names.append(token[1])\n\n            # get the package declaration if exists\n            if parse_tree and parse_tree.package:\n                package_name = parse_tree.package.name\n            else:\n                package_name = None\n\n            if self.name == 'aspectj' or 'tomcat' or 'eclipse' or 'swt':\n                src_files[os.path.relpath(src_file, start=self.src)] = SourceFile(src, comments, class_names,\n                                                                                  attributes, method_names, variables, [\n                                                                                      os.path.basename(src_file).split(\n                                                                                          '.')[0]], package_name)\n            else:\n                # If source files has package declaration\n                if package_name:\n                    src_id = (package_name + '.' + os.path.basename(src_file))\n                else:\n                    src_id = os.path.basename(src_file)\n                src_files[src_id] = SourceFile(src, comments, class_names, attributes, method_names, variables,\n                                               [os.path.basename(src_file).split('.')[0]], package_name)\n            # print(src_files)\n            # print(\"===========\")\n        return src_files\n\n\nclass ReportPreprocessing:\n    \"\"\"Class preprocess bug reports\"\"\"\n    __slots__ = ['bug_reports']\n\n    def __init__(self, bug_reports):\n        self.bug_reports = bug_reports\n\n    def extract_stack_traces(self):\n        \"\"\"Extract stack traces from bug reports\"\"\"\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            report.stack_traces = st\n\n    def extract_stack_traces_remove(self):\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            at = []\n            for x in st:\n                if (x[1] == 'Unknown Source'):\n                    temp = 'Unknown'\n                    y = x[0]+ '(' + temp\n                else:\n                    y = x[0] + '(' + x[1] + ')'\n                at.append(y)\n            report.stack_traces_remove = at\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from bug reports raw_text\"\"\"\n        for report in self.bug_reports.values():\n            # Tokenizing using word_tokeize for more accurate pos-tagging\n            sum_tok = nltk.word_tokenize(report.summary)\n            desc_tok = nltk.word_tokenize(report.description)\n            sum_pos = nltk.pos_tag(sum_tok)\n            desc_pos = nltk.pos_tag(desc_tok)\n            report.pos_tagged_summary = [token for token, pos in sum_pos if 'NN' in pos or 'VB' in pos]\n            report.pos_tagged_description = [token for token, pos in desc_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"Tokenize bug report intro tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = nltk.wordpunct_tokenize(report.summary)\n            report.description = nltk.wordpunct_tokenize(report.description)\n\n    def _split_camelcase(self, tokens):\n        # copy tokens\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camel case detection for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        \"\"\"Split camelcase indentifiers\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = self._split_camelcase(report.summary)\n            report.description = self._split_camelcase(report.description)\n            report.pos_tagged_summary = self._split_camelcase(report.pos_tagged_summary)\n            report.pos_tagged_description = self._split_camelcase(report.pos_tagged_description)\n\n    def normalize(self):\n        \"\"\"remove punctuation, numbers and lowecase conversion\"\"\"\n        # build a translate table for punctuation and number removal\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n\n        for report in self.bug_reports.values():\n            summary_punctnum_rem = [token.translate(punctnum_table) for token in report.summary]\n            desc_punctnum_rem = [token.translate(punctnum_table) for token in report.description]\n            pos_sum_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_summary]\n            pos_desc_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_description]\n            report.summary = [token.lower() for token in summary_punctnum_rem if token]\n            report.description = [token.lower() for token in desc_punctnum_rem if token]\n            report.pos_tagged_summary = [token.lower() for token in pos_sum_punctnum_rem if token]\n            report.pos_tagged_description = [token.lower() for token in pos_desc_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        \"\"\"removing stop word from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in stop_words]\n            report.description = [token for token in report.description if token not in stop_words]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in stop_words]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in stop_words]\n\n    def remove_java_keywords(self):\n        \"\"\"removing java language keywords from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in java_keywords]\n            report.description = [token for token in report.description if token not in java_keywords]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in java_keywords]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for report in self.bug_reports.values():\n            report.summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.summary], report.summary]))\n            report.description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.description], report.description]))\n            report.pos_tagged_summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_summary], report.pos_tagged_summary]))\n            report.pos_tagged_description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_description], report.pos_tagged_description]))\n\n    def preprocess(self):\n        self.extract_stack_traces()\n        self.extract_stack_traces_remove()\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_java_keywords()\n        self.stem()\n\nclass SrcPreprocessing:\n    \"\"\"class to preprocess source code\"\"\"\n    __slots__ = ['src_files']\n\n    def __init__(self, src_files):\n        self.src_files = src_files\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from comments\"\"\"\n        for src in self.src_files.values():\n            # tokenize using word_tokenize\n            comments_tok = nltk.word_tokenize(src.comments)\n            comments_pos = nltk.pos_tag(comments_tok)\n            src.pos_tagged_comments = [token for token, pos in comments_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"tokenize source code to tokens\"\"\"\n        for src in self.src_files.values():\n            src.all_content = nltk.wordpunct_tokenize(src.all_content)\n            src.comments = nltk.wordpunct_tokenize(src.comments)\n\n    def _split_camelcase(self, tokens):\n        # copy token\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camelcase defect for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        # Split camelcase indenti\n        for src in self.src_files.values():\n            src.all_content = self._split_camelcase(src.all_content)\n            src.comments = self._split_camelcase(src.comments)\n            src.class_names = self._split_camelcase(src.class_names)\n            src.attributes = self._split_camelcase(src.attributes)\n            src.method_names = self._split_camelcase(src.method_names)\n            src.variables = self._split_camelcase(src.variables)\n            src.pos_tagged_comments = self._split_camelcase(src.pos_tagged_comments)\n\n    def normalize(self):\n        \"remove punctuation, number and lowercase conversion\"\n        # build a translate table for punctuation and number\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n        for src in self.src_files.values():\n            content_punctnum_rem = [token.translate(punctnum_table) for token in src.all_content]\n            comments_punctnum_rem = [token.translate(punctnum_table) for token in src.comments]\n            classnames_punctnum_rem = [token.translate(punctnum_table) for token in src.class_names]\n            attributes_punctnum_rem = [token.translate(punctnum_table) for token in src.attributes]\n            methodnames_punctnum_rem = [token.translate(punctnum_table) for token in src.method_names]\n            variables_punctnum_rem = [token.translate(punctnum_table) for token in src.variables]\n            filename_punctnum_rem = [token.translate(punctnum_table) for token in src.file_name]\n            pos_comments_punctnum_rem = [token.translate(punctnum_table) for token in src.pos_tagged_comments]\n\n            src.all_content = [token.lower() for token in content_punctnum_rem if token]\n            src.comments = [token.lower() for token in comments_punctnum_rem if token]\n            src.class_names = [token.lower() for token in classnames_punctnum_rem if token]\n            src.attributes = [token.lower() for token in attributes_punctnum_rem if token]\n            src.method_names = [token.lower() for token in methodnames_punctnum_rem if token]\n            src.variables = [token.lower() for token in variables_punctnum_rem if token]\n            src.file_name = [token.lower() for token in filename_punctnum_rem if token]\n            src.pos_tagged_comments = [token.lower() for token in pos_comments_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in stop_words]\n            src.comments = [token for token in src.comments if token not in stop_words]\n            src.class_names = [token for token in src.class_names if token not in stop_words]\n            src.attributes = [token for token in src.attributes if token not in stop_words]\n            src.method_names = [token for token in src.method_names if token not in stop_words]\n            src.variables = [token for token in src.variables if token not in stop_words]\n            src.file_name = [token for token in src.file_name if token not in stop_words]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in stop_words]\n\n    def remove_javakeywords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in java_keywords]\n            src.comments = [token for token in src.comments if token not in java_keywords]\n            src.class_names = [token for token in src.class_names if token not in java_keywords]\n            src.attributes = [token for token in src.attributes if token not in java_keywords]\n            src.method_names = [token for token in src.method_names if token not in java_keywords]\n            src.variables = [token for token in src.variables if token not in java_keywords]\n            src.file_name = [token for token in src.file_name if token not in java_keywords]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for src in self.src_files.values():\n            src.all_content = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.all_content], src.all_content]))\n            src.comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.comments], src.comments]))\n            src.class_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.class_names], src.class_names]))\n            src.attributes = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.attributes], src.attributes]))\n            src.method_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.method_names], src.method_names]))\n            src.variables = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.variables], src.variables]))\n            src.file_name = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.file_name], src.file_name]))\n            src.pos_tagged_comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.pos_tagged_comments], src.pos_tagged_comments]))\n\n\n    def preprocess(self):\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_javakeywords()\n        self.stem()","metadata":{"id":"_22yeS4wcWpU","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:05.615236Z","iopub.execute_input":"2025-04-09T13:19:05.615689Z","iopub.status.idle":"2025-04-09T13:19:05.684618Z","shell.execute_reply.started":"2025-04-09T13:19:05.615658Z","shell.execute_reply":"2025-04-09T13:19:05.683252Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install inflection\nimport inflection\n","metadata":{"id":"ACZrz5Byh7Ur","outputId":"68b0fd14-1ac6-484a-cb84-2ab028bd5ed6","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:05.686371Z","iopub.execute_input":"2025-04-09T13:19:05.686691Z","iopub.status.idle":"2025-04-09T13:19:10.519925Z","shell.execute_reply.started":"2025-04-09T13:19:05.686667Z","shell.execute_reply":"2025-04-09T13:19:10.518352Z"}},"outputs":[{"name":"stdout","text":"Collecting inflection\n  Downloading inflection-0.5.1-py2.py3-none-any.whl.metadata (1.7 kB)\nDownloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\nInstalling collected packages: inflection\nSuccessfully installed inflection-0.5.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')\nimport pickle\nfrom google.colab import drive\nimport csv\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport re\nimport string\nfrom nltk.stem.porter import PorterStemmer","metadata":{"id":"P9emxprnStnP","outputId":"51497efe-74ae-4abd-d5a0-485188dab6f7","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:10.521359Z","iopub.execute_input":"2025-04-09T13:19:10.521801Z","iopub.status.idle":"2025-04-09T13:19:10.780022Z","shell.execute_reply.started":"2025-04-09T13:19:10.521765Z","shell.execute_reply":"2025-04-09T13:19:10.778504Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Load d·ªØ li·ªáu","metadata":{"id":"SC6LbKkWy8kR"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# ƒê∆∞·ªùng d·∫´n ƒë·∫øn c√°c file pickle\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_src_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_src_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_src_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_src_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_src_processed.pkl'\n}\n\n# Load t·ª´ng file v√† l∆∞u v√†o c√°c bi·∫øn t∆∞∆°ng ·ª©ng\ndatasets = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        datasets[name] = pickle.load(f)\n\n# Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c load v√†o c√°c bi·∫øn\nfor name, data in datasets.items():\n    print(f\"Data for {name}:\")\n","metadata":{"id":"O3TGVN1KzAXg","outputId":"63f347a7-1b8d-41fe-98e5-105ca43233e2","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:10.781273Z","iopub.execute_input":"2025-04-09T13:19:10.781706Z","iopub.status.idle":"2025-04-09T13:19:27.320383Z","shell.execute_reply.started":"2025-04-09T13:19:10.781663Z","shell.execute_reply":"2025-04-09T13:19:27.319215Z"}},"outputs":[{"name":"stdout","text":"Data for aspectj:\nData for eclipse:\nData for swt:\nData for tomcat:\nData for birt:\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"eclipse_src = datasets['eclipse']\nbirt_src = datasets['birt']\nswt_src = datasets['swt']\ntomcat_src = datasets['tomcat']\naspectj_src = datasets['aspectj']","metadata":{"id":"b91231aHzUu_","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:27.321447Z","iopub.execute_input":"2025-04-09T13:19:27.321801Z","iopub.status.idle":"2025-04-09T13:19:27.326570Z","shell.execute_reply.started":"2025-04-09T13:19:27.321774Z","shell.execute_reply":"2025-04-09T13:19:27.325227Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load d·ªØ li·ªáu t·ª´ c√°c file pickle ƒë√£ l∆∞u\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_reports_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_reports_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_reports_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_reports_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_reports_processed.pkl'\n}\n\n# Load t·ª´ng dataset v√† l∆∞u v√†o c√°c bi·∫øn\nall_processed_reports = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        all_processed_reports[name] = pickle.load(f)\n\n# Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ load v√†o\nfor dataset, reports in all_processed_reports.items():\n    print(f\"Processed reports for {dataset}:\")","metadata":{"id":"kXiYZuNgzv0M","outputId":"053bc2e9-ef09-4aea-9431-acfda03d3300","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:27.327601Z","iopub.execute_input":"2025-04-09T13:19:27.327954Z","iopub.status.idle":"2025-04-09T13:19:30.961049Z","shell.execute_reply.started":"2025-04-09T13:19:27.327929Z","shell.execute_reply":"2025-04-09T13:19:30.959837Z"}},"outputs":[{"name":"stdout","text":"Processed reports for aspectj:\nProcessed reports for eclipse:\nProcessed reports for swt:\nProcessed reports for tomcat:\nProcessed reports for birt:\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"eclipse_reports = all_processed_reports['eclipse']\nbirt_reports = all_processed_reports['birt']\nswt_reports = all_processed_reports['swt']\ntomcat_reports = all_processed_reports['tomcat']\naspectj_reports = all_processed_reports['aspectj']","metadata":{"id":"iAUv0Bnv0FcI","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:30.965414Z","iopub.execute_input":"2025-04-09T13:19:30.965753Z","iopub.status.idle":"2025-04-09T13:19:30.972180Z","shell.execute_reply.started":"2025-04-09T13:19:30.965726Z","shell.execute_reply":"2025-04-09T13:19:30.970583Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 2. X·ª≠ l√≠ data, g√°n nh√£n\n- S·∫Øp x·∫øp bug report theo th·ªùi gian (report_time)\n- Chia th√†nh 10 folds\n- T·∫°o training/test dataset theo ki·ªÉu fold i ‚Üí fold i+1\n- G√°n nh√£n cho t·ª´ng c·∫∑p (bug report, source file)","metadata":{"id":"-hwWTIRJ9PXd"}},{"cell_type":"code","source":"# B1: L·∫•y danh s√°ch (bug_id, bug_report), sau ƒë√≥ s·∫Øp x·∫øp theo report_time\nsorted_bug_reports = sorted(aspectj_reports.items(), key=lambda x: x[1].report_time)\ndata_src = aspectj_src","metadata":{"id":"HrKZiEgO9X16","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:30.974407Z","iopub.execute_input":"2025-04-09T13:19:30.974918Z","iopub.status.idle":"2025-04-09T13:19:31.001797Z","shell.execute_reply.started":"2025-04-09T13:19:30.974879Z","shell.execute_reply":"2025-04-09T13:19:30.999836Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def split_into_folds(sorted_reports, num_folds=10):\n    fold_size = len(sorted_reports) // num_folds\n    folds = [sorted_reports[i*fold_size:(i+1)*fold_size] for i in range(num_folds)]\n\n    # N·∫øu c√≤n d∆∞, r·∫£i ƒë·ªÅu v√†o c√°c fold ƒë·∫ßu\n    remainder = sorted_reports[num_folds*fold_size:]\n    for i, extra in enumerate(remainder):\n        folds[i].append(extra)\n    return folds\n\ndata_folds = split_into_folds(sorted_bug_reports, num_folds=3)\n","metadata":{"id":"fgRpuQKE9aA2","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:31.003052Z","iopub.execute_input":"2025-04-09T13:19:31.004421Z","iopub.status.idle":"2025-04-09T13:19:31.030613Z","shell.execute_reply.started":"2025-04-09T13:19:31.004366Z","shell.execute_reply":"2025-04-09T13:19:31.029435Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"i = 0 # th·ª≠ v·ªõi fold 0 ‚Üí 1\ntrain_fold = data_folds[i]\ntest_fold = data_folds[i+1]","metadata":{"id":"0itPPQ5O9cDT","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:31.031798Z","iopub.execute_input":"2025-04-09T13:19:31.032192Z","iopub.status.idle":"2025-04-09T13:19:31.050936Z","shell.execute_reply.started":"2025-04-09T13:19:31.032159Z","shell.execute_reply":"2025-04-09T13:19:31.049602Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import random\n\ndef generate_balanced_pairs(bug_fold, source_files, num_negatives_per_positive=50):\n    data = []\n    for bug_id, bug in bug_fold:\n        # Danh s√°ch file ch·ª©a bug (poszqitive)\n        positive_paths = set(bug.fixed_files)\n        positive = [\n            (bug_id, bug, src_path, source_files[src_path], 1)\n            for src_path in positive_paths if src_path in source_files\n        ]\n\n        # Danh s√°ch file c√≤n l·∫°i ƒë·ªÉ l·∫•y negative\n        all_paths = list(source_files.keys())\n        negative_paths = list(set(all_paths) - positive_paths)\n        sampled_negatives = random.sample(negative_paths, min(num_negatives_per_positive * len(positive), len(negative_paths)))\n\n        negative = [\n            (bug_id, bug, src_path, source_files[src_path], 0)\n            for src_path in sampled_negatives if src_path in source_files\n        ]\n\n        data.extend(positive + negative)\n    return data\ndef generate_all_negatives_pairs(bug_fold, source_files):\n    data = []\n    for bug_id, bug in bug_fold:\n        positive_paths = set(bug.fixed_files)\n        positive = [\n            (bug_id, bug, src_path, source_files[src_path], 1)\n            for src_path in positive_paths if src_path in source_files\n        ]\n\n        all_paths = list(source_files.keys())\n        negative_paths = list(set(all_paths) - positive_paths)\n\n        negative = [\n            (bug_id, bug, src_path, source_files[src_path], 0)\n            for src_path in negative_paths if src_path in source_files\n        ]\n\n        data.extend(positive + negative)\n    return data\n\n\ntrain_pairs = generate_balanced_pairs(train_fold, data_src, num_negatives_per_positive=50)\n#test_pairs = generate_balanced_pairs(test_fold, data_src, num_negatives_per_positive=50)\ntest_pairs = generate_all_negatives_pairs(test_fold, data_src)\n\n","metadata":{"id":"wjLEL9mR9fPv","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:31.052210Z","iopub.execute_input":"2025-04-09T13:19:31.052532Z","iopub.status.idle":"2025-04-09T13:19:38.676408Z","shell.execute_reply.started":"2025-04-09T13:19:31.052505Z","shell.execute_reply":"2025-04-09T13:19:38.675175Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"X·ª≠ l√≠ m·∫•t c√¢n b·∫±ng","metadata":{"id":"phVI5NPj-GMs"}},{"cell_type":"code","source":"def compute_stats(pairs):\n    total = len(pairs)\n    pos = sum(1 for _, _, _, _, label in pairs if label == 1)\n    neg = total - pos\n    ratio = pos / total if total > 0 else 0\n    return total, pos, neg, ratio\n\n  \ntotal, pos, neg, ratio = compute_stats(train_pairs)\nprint(\"üìä Train Set:\")\nprint(f\"  ‚û§ T·ªïng c·∫∑p: {total}\")\nprint(f\"  ‚úÖ Positive (label=1): {pos}\")\nprint(f\"  ‚ùå Negative (label=0): {neg}\")\nprint(f\"  ‚öñÔ∏è T·ª∑ l·ªá positive: {ratio:.4f}\")\n\ntotal, pos, neg, ratio = compute_stats(test_pairs)\nprint(\"\\nüß™ Test Set:\")\nprint(f\"  ‚û§ T·ªïng c·∫∑p: {total}\")\nprint(f\"  ‚úÖ Positive (label=1): {pos}\")\nprint(f\"  ‚ùå Negative (label=0): {neg}\")\nprint(f\"  ‚öñÔ∏è T·ª∑ l·ªá positive: {ratio:.4f}\")\n","metadata":{"id":"G-m4H5d79uzp","outputId":"1a4a413d-189b-4b5d-ca55-6552e56ce33f","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:38.677457Z","iopub.execute_input":"2025-04-09T13:19:38.677872Z","iopub.status.idle":"2025-04-09T13:19:38.813419Z","shell.execute_reply.started":"2025-04-09T13:19:38.677836Z","shell.execute_reply":"2025-04-09T13:19:38.812098Z"}},"outputs":[{"name":"stdout","text":"üìä Train Set:\n  ‚û§ T·ªïng c·∫∑p: 8619\n  ‚úÖ Positive (label=1): 169\n  ‚ùå Negative (label=0): 8450\n  ‚öñÔ∏è T·ª∑ l·ªá positive: 0.0196\n\nüß™ Test Set:\n  ‚û§ T·ªïng c·∫∑p: 1368180\n  ‚úÖ Positive (label=1): 168\n  ‚ùå Negative (label=0): 1368012\n  ‚öñÔ∏è T·ª∑ l·ªá positive: 0.0001\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### H√†m 1: T·∫°o batches c√≥ bootstrapping (lu√¥n ch·ª©a √≠t nh·∫•t 1 positive sample)","metadata":{}},{"cell_type":"code","source":"import random\ndef create_bootstrapped_batches(pairs, batch_size=128):\n    # T√°ch positive v√† negative\n    positives = [p for p in pairs if p[-1] == 1]\n    negatives = [p for p in pairs if p[-1] == 0]\n\n    batches = []\n    # T√≠nh s·ªë batch c√≥ th·ªÉ t·∫°o\n    total_batches = len(pairs) // batch_size\n\n    for _ in range(total_batches):\n        # Lu√¥n ch·ªçn √≠t nh·∫•t 1 positive\n        pos_sample = random.choice(positives)\n\n        # Ch·ªçn ng·∫´u nhi√™n batch_size - 1 negative samples\n        neg_samples = random.sample(negatives, batch_size - 1)\n\n        # G·ªôp l·∫°i, shuffle ƒë·ªÉ positive kh√¥ng ƒë·ª©ng ƒë·∫ßu\n        batch = [pos_sample] + neg_samples\n        random.shuffle(batch)\n\n        batches.append(batch)\n\n    return batches\n","metadata":{"id":"VVi_cbQD-a1b","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:38.814992Z","iopub.execute_input":"2025-04-09T13:19:38.815459Z","iopub.status.idle":"2025-04-09T13:19:38.822174Z","shell.execute_reply.started":"2025-04-09T13:19:38.815413Z","shell.execute_reply":"2025-04-09T13:19:38.820978Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Focal Loss Function","metadata":{}},{"cell_type":"code","source":"\ndef focal_loss(predictions, targets, alpha=0.999, gamma=2.0, eps=1e-6):\n    \"\"\"\n    predictions: tensor (batch_size,) - output sigmoid from model\n    targets: tensor (batch_size,) - true labels (0 or 1)\n    \"\"\"\n    # Avoid log(0)\n    predictions = predictions.clamp(min=eps, max=1.0 - eps)\n\n    # Compute focal loss\n    loss = -alpha * (1 - predictions)**gamma * targets * predictions.log() \\\n           - (1 - alpha) * predictions**gamma * (1 - targets) * (1 - predictions).log()\n    return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:38.823606Z","iopub.execute_input":"2025-04-09T13:19:38.823957Z","iopub.status.idle":"2025-04-09T13:19:38.842554Z","shell.execute_reply.started":"2025-04-09T13:19:38.823928Z","shell.execute_reply":"2025-04-09T13:19:38.841280Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# 4. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# H√†m x·ª≠ l√Ω text g·ªôp l·∫°i t·ª´ bug report\ndef bug_to_text(bug):\n    summary = bug.summary['unstemmed'] if isinstance(bug.summary, dict) else bug.summary\n    desc = bug.description['unstemmed'] if isinstance(bug.description, dict) else bug.description\n    return \" \".join(summary + desc)\n\n# H√†m x·ª≠ l√Ω text t·ª´ source file\ndef src_to_text(src):\n    content = src.all_content['unstemmed'] if isinstance(src.all_content, dict) else src.all_content\n    comments = src.comments['unstemmed'] if isinstance(src.comments, dict) else src.comments\n    return \" \".join(content + comments)\n","metadata":{"id":"4en89sb6-s54","outputId":"5c14ad2a-da00-495a-ec81-38687265bcfe","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:38.844020Z","iopub.execute_input":"2025-04-09T13:19:38.844408Z","iopub.status.idle":"2025-04-09T13:19:38.865204Z","shell.execute_reply.started":"2025-04-09T13:19:38.844379Z","shell.execute_reply":"2025-04-09T13:19:38.863973Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## ƒê·∫∑c tr∆∞ng 1: T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng t·ª´ v·ª±ng (lexical similarity)\n- Ph∆∞∆°ng ph√°p: s·ª≠ d·ª•ng TF-IDF v√† cosine similarity.\n- Input: C·∫∑p d·ªØ li·ªáu (bug report, source file)\n- Output: m·∫£ng numpy ch·ª©a c√°c gi√° tr·ªã ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa bug report v√† source file cho m·ªói c·∫∑p.","metadata":{}},{"cell_type":"code","source":"def compute_lexical_similarity(pairs):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # G·ªôp c·∫£ bug + src l·∫°i ƒë·ªÉ fit chung vectorizer\n    combined = bug_texts + src_texts\n    tfidf = TfidfVectorizer()\n    tfidf_matrix = tfidf.fit_transform(combined)\n\n    # T√°ch ri√™ng l·∫°i t·ª´ng ph·∫ßn\n    bug_vecs = tfidf_matrix[:len(pairs)]\n    src_vecs = tfidf_matrix[len(pairs):]\n\n    # T√≠nh cosine cho t·ª´ng c·∫∑p (theo h√†ng t∆∞∆°ng ·ª©ng)\n    similarities = cosine_similarity(bug_vecs, src_vecs).diagonal()\n\n    return similarities\n\n# Demo: t√≠nh feature lexical similarity cho train_pairs (gi·ªõi h·∫°n 500 m·∫´u v√¨ t·ªëc ƒë·ªô)\nsampled_pairs = train_pairs[:500]\nlexical_sim = compute_lexical_similarity(sampled_pairs)\n\n# Tr·∫£ v·ªÅ d∆∞·ªõi d·∫°ng numpy array\nlexical_sim[:10]  # Tr√≠ch 10 gi√° tr·ªã ƒë·∫ßu ti√™n ƒë·ªÉ test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:38.866526Z","iopub.execute_input":"2025-04-09T13:19:38.866950Z","iopub.status.idle":"2025-04-09T13:19:39.187297Z","shell.execute_reply.started":"2025-04-09T13:19:38.866918Z","shell.execute_reply":"2025-04-09T13:19:39.185858Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"array([0.00133791, 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.0100989 , 0.00052573, 0.00201953, 0.        ])"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"glove_path = \"/kaggle/input/glove-embedding/glove.6B.100d.txt\"\n# Load GloVe 100d v√†o dictionary\nimport numpy as np\n\ndef load_glove_embeddings(filepath):\n    embeddings = {}\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.strip().split()\n            word = values[0]\n            vector = np.array(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n    \nglove_embeddings = load_glove_embeddings(glove_path)","metadata":{"id":"6FexoFrK_LTE","outputId":"b7e7b200-954e-4541-a27b-dc43607ae8f2","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:39.188462Z","iopub.execute_input":"2025-04-09T13:19:39.188891Z","iopub.status.idle":"2025-04-09T13:19:50.916523Z","shell.execute_reply.started":"2025-04-09T13:19:39.188848Z","shell.execute_reply":"2025-04-09T13:19:50.915228Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## ƒê·∫∑c tr∆∞ng 2: T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a (semantic similarity)\n- Ph∆∞∆°ng ph√°p: TF-IDF weighted average c·ªßa GloVe vectors v√† cosine similarity\n- Input:  (bug report, source file).\n- Output: M·ªôt m·∫£ng numpy ch·ª©a c√°c gi√° tr·ªã ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa bug report v√† source file cho m·ªói c·∫∑p, d·ª±a tr√™n GloVe vectors v√† tr·ªçng s·ªë TF-IDF.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef compute_semantic_similarity(pairs, glove_dict, dim=100):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # D√πng TF-IDF ƒë·ªÉ l·∫•y tr·ªçng s·ªë t·ª´\n    tfidf = TfidfVectorizer()\n    tfidf.fit(bug_texts + src_texts)\n    vocab = tfidf.vocabulary_\n    idf_weights = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n\n    def embed_text(text):\n        tokens = text.split()\n        vecs = []\n        weights = []\n        for token in tokens:\n            if token in glove_dict and token in vocab:\n                vecs.append(glove_dict[token])\n                weights.append(idf_weights[token])\n        if not vecs:\n            return np.zeros(dim)\n        vecs = np.array(vecs)\n        weights = np.array(weights).reshape(-1, 1)\n        weighted_vecs = vecs * weights\n        return weighted_vecs.sum(axis=0) / weights.sum()\n\n    # T√≠nh vector trung b√¨nh cho bug v√† src\n    bug_vecs = [embed_text(text) for text in bug_texts]\n    src_vecs = [embed_text(text) for text in src_texts]\n\n    # T√≠nh cosine similarity gi·ªØa t·ª´ng c·∫∑p\n    similarities = [cosine_similarity([b], [s])[0][0] for b, s in zip(bug_vecs, src_vecs)]\n\n    return np.array(similarities)\nsampled_pairs = train_pairs[:500]\n# Demo tr√™n 500 c·∫∑p nh∆∞ lexical\nsemantic_sim = compute_semantic_similarity(sampled_pairs, glove_embeddings)\nsemantic_sim[:10]\n","metadata":{"id":"BmKfMQ9oAFSQ","outputId":"e0bf60b5-8bc7-401f-c8ee-5e8ac04ebe01","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:50.917558Z","iopub.execute_input":"2025-04-09T13:19:50.918118Z","iopub.status.idle":"2025-04-09T13:19:51.732474Z","shell.execute_reply.started":"2025-04-09T13:19:50.918083Z","shell.execute_reply":"2025-04-09T13:19:51.731417Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"array([0.77527147, 0.69421516, 0.        , 0.50986771, 0.74005518,\n       0.06775918, 0.81621045, 0.77655908, 0.67360652, 0.48345688])"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"### ƒê·∫∑c tr∆∞ng 3: Similar Bug Report Score ","metadata":{}},{"cell_type":"markdown","source":"‚Üí Ki·ªÉm tra xem bug report n√†y c√≥ gi·ªëng¬†**nh·ªØng bug report c≈© t·ª´ng s·ª≠a c√πng file ƒë√≥**¬†kh√¥ng?\n\n- `build_bug_fix_history(pairs)` ‚Üí XD l·ªãch s·ª≠ ch·ªânh s·ª≠a theo t·ª´ng file\n- `compute_similar_bug_score(pairs, history)`\n    - Input: pairs, history\n    - So s√°nh bug hi·ªán t·∫°i v√† bug c≈©:\n    \n    cosine_similarity(TfidfVectorizer().fit_transform([bug_now, bug_old]))[0, 1]\n    \n    - L·∫•y gi√° tr·ªã t∆∞∆°ng ƒë·ªìng cao nh·∫•t v·ª´a t√¨m ƒë∆∞·ª£c","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\n\n# B·ªï tr·ª£: t·∫°o map file_path -> list of (bug_id, report_time, bug_text)\ndef build_bug_fix_history(pairs):\n    history = {}\n    for bug_id, bug, src_path, _, label in pairs:\n        if label == 1:  # ch·ªâ t√≠nh c√°c bug th·∫≠t s·ª± s·ª≠a file\n            if src_path not in history:\n                history[src_path] = []\n            history[src_path].append((bug_id, bug.report_time, bug_to_text(bug)))\n    return history\n\n# ƒê·∫∑c tr∆∞ng 3: Similar Bug Report Score\ndef compute_similar_bug_score(pairs, history):\n    scores = []\n    for bug_id, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        current_text = bug_to_text(bug)\n\n        sim_scores = []\n        if src_path in history:\n            for hist_bug_id, hist_time, hist_text in history[src_path]:\n                if hist_time < current_time:  # ch·ªâ t√≠nh bug trong qu√° kh·ª©\n                    sim = cosine_similarity(\n                        TfidfVectorizer().fit_transform([current_text, hist_text])\n                    )[0, 1]\n                    sim_scores.append(sim)\n        scores.append(max(sim_scores) if sim_scores else 0.0)\n    return np.array(scores)\n","metadata":{"id":"imb4_du_Bgjz","outputId":"8c08c8e9-7047-4e5d-de85-8b70e30cbe28","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:51.733614Z","iopub.execute_input":"2025-04-09T13:19:51.734021Z","iopub.status.idle":"2025-04-09T13:19:51.742602Z","shell.execute_reply.started":"2025-04-09T13:19:51.733983Z","shell.execute_reply":"2025-04-09T13:19:51.741215Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### ƒê·∫∑c trung 4: Time Since Last Fix (ng√†y, normalize)\n- Ki·ªÉm tra v·ªõi m·ªói `(bug report, source file)` xem t·ª´ng ƒë∆∞·ª£c s·ª≠a tr∆∞·ªõc ƒë√≥ kh√¥ng v√† l·∫ßn cu·ªëi khi n√†o\n    - ƒê√£ l√¢u k s·ª≠a ‚Üí √çt l·ªói ‚Üí ƒêi·ªÉm th·∫•p\n    - M·ªõi s·ª≠a ‚Üí c√≥ th·ªÉ li√™n quan t·ªõi l·ªói ‚Üí ƒêi·ªÉm cao\n- C√°ch hƒë:\n    - T√¨m th·ªùi ƒëi·ªÉm bug current_time\n    - T√¨m history c√°c l·∫ßn s·ª≠a file trong qu√° kh·ª©\n    - T√≠nh kho·∫£ng c√°ch time gi·ªØa current v√† history g·∫ßn nh·∫•t\n    - Ch∆∞a s·ª≠a ‚Üí G√°n s·ªë delta_days=9999\n    - Chu·∫©n ho√°","metadata":{}},{"cell_type":"code","source":"# ƒê·∫∑c tr∆∞ng 4: Time Since Last Fix (ng√†y, normalize)\ndef compute_time_since_last_fix(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_times = [hist_time for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            if past_times:\n                delta_days = (current_time - max(past_times)).days\n            else:\n                delta_days = 9999  # C·ª±c l·ªõn n·∫øu ch∆∞a t·ª´ng s·ª≠a\n        else:\n            delta_days = 9999\n        scores.append(delta_days)\n\n    # Normalize v·ªÅ [0,1]\n    max_days = max(scores) if max(scores) != 0 else 1  # Tr√°nh chia cho 0\n\n    return np.array([1 - (s / max_days) for s in scores])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:51.744089Z","iopub.execute_input":"2025-04-09T13:19:51.744566Z","iopub.status.idle":"2025-04-09T13:19:51.777755Z","shell.execute_reply.started":"2025-04-09T13:19:51.744518Z","shell.execute_reply":"2025-04-09T13:19:51.776518Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### ƒê·∫∑c tr∆∞ng 5: Fix Frequency (s·ªë l·∫ßn b·ªã s·ª≠a trong qu√° kh·ª©, normalize)\n\n\n- Ki·ªÉm tra xme m·ªói c·∫∑p ƒë∆∞·ª£c s·ª≠a bao nhi√™u l·∫ßn\n\n‚Üí S·ª≠a nhi·ªÅu ‚Üí File d·ªÖ d√≠nh l·ªói ‚Üí ƒêi·ªÉm cao","metadata":{}},{"cell_type":"code","source":"# ƒê·∫∑c tr∆∞ng 5: Fix Frequency (s·ªë l·∫ßn b·ªã s·ª≠a trong qu√° kh·ª©, normalize)\ndef compute_fix_frequency(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_fixes = [1 for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            freq = len(past_fixes)\n        else:\n            freq = 0\n        scores.append(freq)\n    # Normalize v·ªÅ [0,1] an to√†n\n    max_freq = max(scores)\n    max_freq = max(max_freq, 1)  # tr√°nh chia 0\n    return np.array([s / max_freq for s in scores])\n\n\n# D√πng cho 500 c·∫∑p m·∫´u\nsampled_pairs = train_pairs[:5000]\nbug_history = build_bug_fix_history(train_pairs)\n\nsimilar_bug_score = compute_similar_bug_score(sampled_pairs, bug_history)\ntime_since_last_fix = compute_time_since_last_fix(sampled_pairs, bug_history)\nfix_frequency = compute_fix_frequency(sampled_pairs, bug_history)\n\n# Tr√≠ch 5 gi√° tr·ªã ƒë·∫ßu m·ªói feature\nsimilar_bug_score[:50], time_since_last_fix[:50], fix_frequency[:50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:51.778957Z","iopub.execute_input":"2025-04-09T13:19:51.779336Z","iopub.status.idle":"2025-04-09T13:19:51.990188Z","shell.execute_reply.started":"2025-04-09T13:19:51.779307Z","shell.execute_reply":"2025-04-09T13:19:51.989137Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"# 4. Qu√° tr√¨nh hu·∫•n luy·ªán","metadata":{}},{"cell_type":"markdown","source":"## 4.1 T·∫°o ma tr·∫≠n train, test","metadata":{}},{"cell_type":"code","source":"# G·ªôp 5 ƒë·∫∑c tr∆∞ng l·∫°i th√†nh feature matrix X (n_samples, 5)\ndef build_feature_matrix(pairs, glove_dict, history):\n    lexical = compute_lexical_similarity(pairs)\n    semantic = compute_semantic_similarity(pairs, glove_dict)\n    similar_score = compute_similar_bug_score(pairs, history)\n    recency = compute_time_since_last_fix(pairs, history)\n    freq = compute_fix_frequency(pairs, history)\n\n    # G·ªôp l·∫°i theo chi·ªÅu d·ªçc ‚Üí ma tr·∫≠n (n_samples, 5)\n    X = np.stack([lexical, semantic, similar_score, recency, freq], axis=1)\n\n    return X\n\n# T·∫°o nh√£n y\ndef get_labels(pairs):\n    return np.array([label for *_, label in pairs])\n\n# T·∫°o d·ªØ li·ªáu train t·ª´ sampled_pairs\nX_train = build_feature_matrix(train_pairs, glove_embeddings, bug_history)\ny_train = get_labels(train_pairs)\n\n# In shape ƒë·ªÉ x√°c nh·∫≠n\nX_train.shape, y_train.shape\n\n","metadata":{"id":"PmEZSTiyDEB4","outputId":"6db11041-f3a2-4931-f264-9480bfd7c2e6","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:19:51.991291Z","iopub.execute_input":"2025-04-09T13:19:51.991633Z","iopub.status.idle":"2025-04-09T13:20:12.341693Z","shell.execute_reply.started":"2025-04-09T13:19:51.991605Z","shell.execute_reply":"2025-04-09T13:20:12.340508Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"((8619, 5), (8619,))"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"def build_feature_matrix_batch(pairs_batch, glove_dict, history):\n    lexical = compute_lexical_similarity(pairs_batch)\n    semantic = compute_semantic_similarity(pairs_batch, glove_dict)\n    similar_score = compute_similar_bug_score(pairs_batch, history)\n    recency = compute_time_since_last_fix(pairs_batch, history)\n    freq = compute_fix_frequency(pairs_batch, history)\n\n    # Stack theo chi·ªÅu d·ªçc ‚Üí (batch_size, 5)\n    X = np.stack([lexical, semantic, similar_score, recency, freq], axis=1)\n    return X\n\ndef get_labels(pairs):\n    return np.array([label for *_, label in pairs])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:20:12.342756Z","iopub.execute_input":"2025-04-09T13:20:12.343105Z","iopub.status.idle":"2025-04-09T13:20:12.349625Z","shell.execute_reply.started":"2025-04-09T13:20:12.343041Z","shell.execute_reply":"2025-04-09T13:20:12.348437Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def pair_generator(pairs, batch_size, glove_dict, history):\n    for i in range(0, len(pairs), batch_size):\n        batch = pairs[i:i + batch_size]\n        X = build_feature_matrix_batch(batch, glove_dict, history)\n        y = np.array([label for *_, label in batch])\n        yield torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:20:12.350748Z","iopub.execute_input":"2025-04-09T13:20:12.351141Z","iopub.status.idle":"2025-04-09T13:20:12.379451Z","shell.execute_reply.started":"2025-04-09T13:20:12.351106Z","shell.execute_reply":"2025-04-09T13:20:12.378138Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n# Chu·∫©n b·ªã Dataset & Dataloader t·ª´ numpy\ndef create_dataloader(X, y, batch_size=128):\n    X_tensor = torch.tensor(X, dtype=torch.float32)\n    y_tensor = torch.tensor(y, dtype=torch.float32)\n    dataset = TensorDataset(X_tensor, y_tensor)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:20:12.383772Z","iopub.execute_input":"2025-04-09T13:20:12.384172Z","iopub.status.idle":"2025-04-09T13:20:16.913516Z","shell.execute_reply.started":"2025-04-09T13:20:12.384130Z","shell.execute_reply":"2025-04-09T13:20:16.912202Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## 4.2 X√¢y d·ª±ng m√¥ h√¨nh","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\n# ƒê·ªãnh nghƒ©a m√¥ h√¨nh DNN gi·ªëng b√†i b√°o\nimport torch\nimport torch.nn as nn\n\nclass BugLocalization(nn.Module):\n        def __init__(self, input_dim=5, hidden_dims=[128, 64], output_dim=1):\n            super(BugLocalization, self).__init__()\n\n            # Define a series of fully connected (Dense) layers\n            self.fc1 = nn.Linear(input_dim, hidden_dims[0])  # First hidden layer\n            self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])  # Second hidden layer\n            self.fc3 = nn.Linear(hidden_dims[1], output_dim)  # Output layer\n\n            # Define activation function (ReLU for hidden layers and Sigmoid for output)\n            self.relu = nn.ReLU()\n            self.sigmoid = nn.Sigmoid()\n\n        def forward(self, x):\n        # Forward pass through the DNN\n            x = self.relu(self.fc1(x))  # Pass through the first hidden layer with ReLU activation\n            x = self.relu(self.fc2(x))  # Pass through the second hidden layer with ReLU activation\n            x = self.sigmoid(self.fc3(x))  # Output layer with Sigmoid activation for binary classification\n            return x.squeeze()  # Remove extra dimension from the output (as it's a single value per input)\n\n\n        \n# ƒê·ªãnh nghƒ©a focal loss\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.999, gamma=2.0, eps=1e-6):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, preds, targets):\n        preds = preds.clamp(min=self.eps, max=1. - self.eps)\n        loss = -self.alpha * (1 - preds) ** self.gamma * targets * torch.log(preds) \\\n               - (1 - self.alpha) * preds ** self.gamma * (1 - targets) * torch.log(1 - preds)\n        return loss.mean()\n\n\n# Hu·∫•n luy·ªán m√¥ h√¨nh\ndef train_model_generator(model, train_gen, epochs=10, lr=1e-3):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = FocalLoss()\n\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch_X, batch_y in train_gen:\n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n            # üßπ D·ªçn b·ªô nh·ªõ m·ªói batch\n            del batch_X, batch_y, outputs, loss\n            torch.cuda.empty_cache()\n            import gc; gc.collect()\n\n        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n\n","metadata":{"id":"LiD_wJxjDeQ8","outputId":"9821aee2-a66e-480b-9f69-6e4d207c7ba6","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:20:16.915200Z","iopub.execute_input":"2025-04-09T13:20:16.915663Z","iopub.status.idle":"2025-04-09T13:20:16.928663Z","shell.execute_reply.started":"2025-04-09T13:20:16.915620Z","shell.execute_reply":"2025-04-09T13:20:16.927167Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# In th·ª≠ 1 batch ƒë·∫ßu\nfor X_batch, y_batch in pair_generator(train_pairs, 128, glove_embeddings, bug_history):\n    print(\"üëâ Feature Sample:\", X_batch[2])\n    print(\"üëâ Label Sample:\", y_batch[2])\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:20:16.930214Z","iopub.execute_input":"2025-04-09T13:20:16.930725Z","iopub.status.idle":"2025-04-09T13:20:17.343252Z","shell.execute_reply.started":"2025-04-09T13:20:16.930680Z","shell.execute_reply":"2025-04-09T13:20:17.341856Z"}},"outputs":[{"name":"stdout","text":"üëâ Feature Sample: tensor([0., 0., 0., 0., 0.])\nüëâ Label Sample: tensor(0.)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def test_pair_generator(pairs, batch_size, glove_dict, history):\n    for i in range(0, len(pairs), batch_size):\n        batch = pairs[i:i + batch_size]\n        X = build_feature_matrix_batch(batch, glove_dict, history)\n        yield X\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:20:17.344811Z","iopub.execute_input":"2025-04-09T13:20:17.345380Z","iopub.status.idle":"2025-04-09T13:20:17.352987Z","shell.execute_reply.started":"2025-04-09T13:20:17.345230Z","shell.execute_reply":"2025-04-09T13:20:17.351241Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"\n# ƒê√°nh gi√° c√°c ch·ªâ s·ªë (MAP, MRR, Top-k)\ndef compute_topk_accuracy(y_true, y_scores, k=10):\n    bug_to_scores = {}\n    for (bug_id, _, src_path, _, label), score in zip(test_pairs, y_scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    correct_at_k = 0\n    total = 0\n\n    for bug_id, entries in bug_to_scores.items():\n        sorted_entries = sorted(entries, key=lambda x: x[0], reverse=True)\n        top_k = sorted_entries[:k]\n        if any(label == 1 for _, label in top_k):\n            correct_at_k += 1\n        total += 1\n\n    return correct_at_k / total if total > 0 else 0\n\n# MRR (Mean Reciprocal Rank)\ndef mean_reciprocal_rank(pairs, scores):\n    bug_to_scores = {}\n    for (bug_id, _, _, _, label), score in zip(pairs, scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    rr_sum = 0\n    count = 0\n    for bug_id, ranked in bug_to_scores.items():\n        ranked = sorted(ranked, key=lambda x: x[0], reverse=True)\n        for idx, (_, label) in enumerate(ranked):\n            if label == 1:\n                rr_sum += 1 / (idx + 1)\n                break\n        count += 1\n    return rr_sum / count if count > 0 else 0","metadata":{"id":"hvK-tE_MD4zq","outputId":"5444ce3e-4624-4f81-eb6b-b19a543b0d49","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:20:17.354652Z","iopub.execute_input":"2025-04-09T13:20:17.355144Z","iopub.status.idle":"2025-04-09T13:20:17.382632Z","shell.execute_reply.started":"2025-04-09T13:20:17.355099Z","shell.execute_reply":"2025-04-09T13:20:17.381039Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def run_kfold_training_and_eval(folds, source_files, glove_dict, k=3):\n    results = {\n        \"fold\": [],\n        \"MAP\": [],\n        \"MRR\": [],\n        \"Top1\": [],\n        \"Top2\": [],\n        \"Top3\": [],\n        \"Top4\": [],\n        \"Top5\": [],\n        \"Top10\": [],\n        \"Top15\": []\n    }\n\n    for i in range(k - 1):\n        print(f\"\\nüì¶ Fold {i} ‚û§ {i+1}\")\n        train_fold = folds[i]\n        test_fold = folds[i + 1]\n\n        # ‚úÖ Train: l·∫•y m·ªôt s·ªë l∆∞·ª£ng negative c·ªë ƒë·ªãnh\n        train_pairs = generate_balanced_pairs(train_fold, source_files, num_negatives_per_positive=50)\n\n        # ‚úÖ Test: l·∫•y full negative (kh√¥ng sampling)\n        test_pairs = generate_all_negatives_pairs(test_fold, source_files)\n\n        # B·ªè qua n·∫øu kh√¥ng c√≥ positive n√†o\n        if sum(1 for p in train_pairs if p[-1] == 1) < 1:\n            print(\"‚ö†Ô∏è B·ªè qua do qu√° √≠t positive samples\")\n            continue\n\n        # ‚úÖ X√¢y history t·ª´ train\n        bug_history = build_bug_fix_history(train_pairs)\n\n        # ‚úÖ Kh·ªüi t·∫°o model, optimizer, loss\n        model = BugLocalization(input_dim=5).to(device)  # ‚ö†Ô∏è s·ª≠a l·∫°i input_dim n·∫øu d√πng ƒë·ªß 5 ƒë·∫∑c tr∆∞ng\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\n        criterion = FocalLoss(alpha=0.25)\n\n        # ‚úÖ Train b·∫±ng generator\n        model.train()\n        for epoch in range(10):\n            train_gen = pair_generator(train_pairs, batch_size=128, glove_dict=glove_dict, history=bug_history)\n            total_loss = 0\n            for X_batch, y_batch in train_gen:\n                X_batch = X_batch.to(device)\n                y_batch = y_batch.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(X_batch)\n                loss = criterion(outputs, y_batch)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n                # Clean up\n                del X_batch, y_batch, outputs, loss\n                torch.cuda.empty_cache()\n                import gc; gc.collect()\n\n            print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")\n\n        # ‚úÖ D·ª± ƒëo√°n theo batch\n        bug_history_test = build_bug_fix_history(test_pairs)\n        y_test = get_labels(test_pairs)\n        y_pred_probs = []\n\n        model.eval()\n        with torch.no_grad():\n            for X_batch in test_pair_generator(test_pairs, batch_size=128, glove_dict=glove_dict, history=bug_history_test):\n                X_tensor = torch.tensor(X_batch, dtype=torch.float32).to(device)\n                batch_probs = model(X_tensor).cpu().numpy()\n                y_pred_probs.extend(batch_probs)\n\n        # ‚úÖ T√≠nh c√°c ch·ªâ s·ªë\n        map_score = average_precision_score(y_test, y_pred_probs)\n        mrr_score = mean_reciprocal_rank(test_pairs, y_pred_probs)\n        top1 = compute_topk_accuracy(y_test, y_pred_probs, k=1)\n        top2 = compute_topk_accuracy(y_test, y_pred_probs, k=2)\n        top3 = compute_topk_accuracy(y_test, y_pred_probs, k=3)\n        top4 = compute_topk_accuracy(y_test, y_pred_probs, k=4)\n        top5 = compute_topk_accuracy(y_test, y_pred_probs, k=5)\n        top10 = compute_topk_accuracy(y_test, y_pred_probs, k=10)\n        top15 = compute_topk_accuracy(y_test, y_pred_probs, k=15)\n\n        print(f\"‚úÖ Fold {i + 1} Results:\")\n        print(f\"  ‚û§ MAP:   {map_score:.4f}\")\n        print(f\"  ‚û§ MRR:   {mrr_score:.4f}\")\n        print(f\"  ‚û§ Top@1: {top1:.4f} | Top@2: {top2:.4f} | Top@3: {top3:.4f}\")\n        print(f\"  ‚û§ Top@4: {top4:.4f} | Top@5: {top5:.4f} | Top@10: {top10:.4f} | Top@15: {top15:.4f}\")\n\n        # ‚úÖ Ghi l·∫°i k·∫øt qu·∫£\n        results[\"fold\"].append(i)\n        results[\"MAP\"].append(map_score)\n        results[\"MRR\"].append(mrr_score)\n        results[\"Top1\"].append(top1)\n        results[\"Top2\"].append(top2)\n        results[\"Top3\"].append(top3)\n        results[\"Top4\"].append(top4)\n        results[\"Top5\"].append(top5)\n        results[\"Top10\"].append(top10)\n        results[\"Top15\"].append(top15)\n\n    return results\n","metadata":{"id":"0esoYB7ME-At","outputId":"cbd0b93b-371f-4886-b013-7532bccb8b96","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:20:17.384120Z","iopub.execute_input":"2025-04-09T13:20:17.384516Z","iopub.status.idle":"2025-04-09T13:20:17.404238Z","shell.execute_reply.started":"2025-04-09T13:20:17.384482Z","shell.execute_reply":"2025-04-09T13:20:17.402882Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim  # ‚úÖ ph·∫ßn b·ªã thi·∫øu\n\n# Run K-fold training and evaluation\nfull_results = run_kfold_training_and_eval(data_folds, data_src, glove_embeddings)\n\n# Output full results\nprint(\"\\nFull Results:\")\nfor key, value in full_results.items():\n    print(f\"{key}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:20:17.405418Z","iopub.execute_input":"2025-04-09T13:20:17.405740Z"}},"outputs":[{"name":"stdout","text":"\nüì¶ Fold 0 ‚û§ 1\nEpoch 1: Loss = 2.6087\nEpoch 2: Loss = 0.5380\nEpoch 3: Loss = 0.4695\nEpoch 4: Loss = 0.4240\nEpoch 5: Loss = 0.3950\nEpoch 6: Loss = 0.3853\nEpoch 7: Loss = 0.3828\nEpoch 8: Loss = 0.3817\nEpoch 9: Loss = 0.3812\nEpoch 10: Loss = 0.3807\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n# In k·∫øt qu·∫£ t·ªïng h·ª£p sau khi ch·∫°y t·∫•t c·∫£ folds\nprint(\"\\nüìä K·∫øt qu·∫£ t·ªïng h·ª£p:\")\nfor i in range(len(full_results[\"fold\"])):\n    print(f\"Fold {full_results['fold'][i]}:\")\n    print(f\"  ‚û§ MAP: {full_results['MAP'][i]:.4f}\")\n    print(f\"  ‚û§ MRR: {full_results['MRR'][i]:.4f}\")\n    print(f\"  ‚û§ Top1: {full_results['Top1'][i]:.4f}\")\n    print(f\"  ‚û§ Top2: {full_results['Top2'][i]:.4f}\")\n    print(f\"  ‚û§ Top3: {full_results['Top3'][i]:.4f}\")\n    print(f\"  ‚û§ Top4: {full_results['Top4'][i]:.4f}\")\n    print(f\"  ‚û§ Top5: {full_results['Top5'][i]:.4f}\")\n    print(f\"  ‚û§ Top10: {full_results['Top10'][i]:.4f}\")\n    print(f\"  ‚û§ Top15: {full_results['Top15'][i]:.4f}\")\n\n# T√≠nh trung b√¨nh cho t·∫•t c·∫£ c√°c ch·ªâ s·ªë\nmean_map = np.mean(full_results[\"MAP\"])\nmean_mrr = np.mean(full_results[\"MRR\"])\nmean_top1 = np.mean(full_results[\"Top1\"])\nmean_top2 = np.mean(full_results[\"Top2\"])\nmean_top3 = np.mean(full_results[\"Top3\"])\nmean_top4 = np.mean(full_results[\"Top4\"])\nmean_top5 = np.mean(full_results[\"Top5\"])\nmean_top10 = np.mean(full_results[\"Top10\"])\nmean_top15 = np.mean(full_results[\"Top15\"])\n\n# In k·∫øt qu·∫£ trung b√¨nh\nprint(\"\\nüìä K·∫øt qu·∫£ trung b√¨nh tr√™n to√†n b·ªô k-folds:\")\nprint(f\"  ‚û§ MAP: {mean_map:.4f}\")\nprint(f\"  ‚û§ MRR: {mean_mrr:.4f}\")\nprint(f\"  ‚û§ Top1: {mean_top1:.4f}\")\nprint(f\"  ‚û§ Top2: {mean_top2:.4f}\")\nprint(f\"  ‚û§ Top3: {mean_top3:.4f}\")\nprint(f\"  ‚û§ Top4: {mean_top4:.4f}\")\nprint(f\"  ‚û§ Top5: {mean_top5:.4f}\")\nprint(f\"  ‚û§ Top10: {mean_top10:.4f}\")\nprint(f\"  ‚û§ Top15: {mean_top15:.4f}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# D·ªØ li·ªáu SOTA: ImbalancedBugLoc\ndata = {\n    \"Project\": [\"AspectJ\", \"Tomcat\", \"Eclipse\", \"SWT\", \"Birt\"],\n    \"Top1_SOTA\": [52.5, 53.2, 48.1, 40.2, 28.3],\n    \"Top2_SOTA\": [68.7, 65.5, 62.1, 54.9, 39.3],\n    \"Top3_SOTA\": [77.2, 71.0, 68.8, 64.2, 45.7],\n    \"Top4_SOTA\": [81.0, 75.0, 73.0, 69.3, 51.0],\n    \"Top5_SOTA\": [83.8, 78.3, 76.7, 73.4, 53.6],\n    \"Top10_SOTA\": [89.0, 85.6, 84.7, 84.8, 63.2],\n    \"Top15_SOTA\": [91.5, 88.9, 87.8, 89.1, 69.2],\n    \"MRR_SOTA\": [0.66, 0.64, 0.60, 0.55, 0.40],\n    \"MAP_SOTA\": [0.50, 0.59, 0.54, 0.50, 0.32],\n    \"Top1_New\": [61.90, 56.75, 66.30, 60.63, 45.97],\n    \"Top2_New\": [71.13, 69.58, 78.20, 74.70, 59.04],\n    \"Top3_New\": [76.19, 77.48, 83.35, 81.28, 66.60],\n    \"Top4_New\": [80.06, 82.71, 86.71, 84.77, 71.74],\n    \"Top5_New\": [82.74, 85.79, 89.13, 87.49, 75.62],\n    \"Top10_New\": [88.39, 92.90, 94.38, 93.91, 85.76],\n    \"Top15_New\": [91.07, 94.90, 96.19, 96.07, 90.50],\n    \"MRR_New\": [0.7109, 0.6899, 0.7644, 0.7183, 0.5921],\n    \"MAP_New\": [0.5367, 0.4946, 0.5692, 0.4669, 0.3734]\n}\n\n# T·∫°o DataFrame t·ª´ d·ªØ li·ªáu\ndf = pd.DataFrame(data)\n\n# T√°ch th√†nh hai b·∫£ng: SOTA v√† New Model\ndf_sota = df[[\"Project\", \"Top1_SOTA\", \"Top2_SOTA\", \"Top3_SOTA\", \"Top4_SOTA\", \"Top5_SOTA\", \n              \"Top10_SOTA\", \"Top15_SOTA\", \"MRR_SOTA\", \"MAP_SOTA\"]].copy()\ndf_sota[\"Model\"] = \"SOTA\"\n\ndf_new = df[[\"Project\", \"Top1_New\", \"Top2_New\", \"Top3_New\", \"Top4_New\", \"Top5_New\", \n             \"Top10_New\", \"Top15_New\", \"MRR_New\", \"MAP_New\"]].copy()\ndf_new[\"Model\"] = \"New Model\"\n\n# ƒê·ªïi t√™n c·ªôt gi·ªëng b·∫£ng trong ·∫£nh\nrename_cols = {\n    \"Top1_SOTA\": \"1\", \"Top2_SOTA\": \"2\", \"Top3_SOTA\": \"3\", \"Top4_SOTA\": \"4\",\n    \"Top5_SOTA\": \"5\", \"Top10_SOTA\": \"10\", \"Top15_SOTA\": \"15\", \"MRR_SOTA\": \"MRR\", \"MAP_SOTA\": \"MAP\",\n    \"Top1_New\": \"1\", \"Top2_New\": \"2\", \"Top3_New\": \"3\", \"Top4_New\": \"4\",\n    \"Top5_New\": \"5\", \"Top10_New\": \"10\", \"Top15_New\": \"15\", \"MRR_New\": \"MRR\", \"MAP_New\": \"MAP\"\n}\n\ndf_sota.rename(columns=rename_cols, inplace=True)\ndf_new.rename(columns=rename_cols, inplace=True)\n\n# G·ªôp l·∫°i\ndf_combined = pd.concat([df_sota, df_new], axis=0)\ndf_combined = df_combined.sort_values(by=[\"Project\", \"Model\"]).reset_index(drop=True)\n\n# ƒê∆∞a c·ªôt 'Model' v·ªÅ sau 'Project'\ncols = df_combined.columns.tolist()\ncols.insert(1, cols.pop(cols.index('Model')))\ndf_combined = df_combined[cols]\n\n# Hi·ªÉn th·ªã k·∫øt qu·∫£\ndf_combined\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Xu·∫•t ra file Excel\ndf_combined.to_excel(\"model_comparison.xlsx\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# T·∫°o l·∫°i df_combined ƒë·ªÉ c·∫≠p nh·∫≠t MAP v√† MRR nh√¢n 100\ndf_combined_scaled = df_combined.copy()\ndf_combined_scaled[\"MAP\"] *= 100\ndf_combined_scaled[\"MRR\"] *= 100\n\n# V·∫Ω l·∫°i 5 bi·ªÉu ƒë·ªì c·ªôt v·ªõi MAP v√† MRR nh√¢n 100\nprojects = df_combined_scaled[\"Project\"].unique()\nmetrics = [\"1\", \"2\", \"3\", \"4\", \"5\", \"10\", \"15\", \"MRR\", \"MAP\"]\n\nplt.figure(figsize=(20, 25))\n\nfor i, project in enumerate(projects, 1):\n    plt.subplot(3, 2, i)\n    sota_vals = df_combined_scaled[(df_combined_scaled[\"Project\"] == project) & (df_combined_scaled[\"Model\"] == \"SOTA\")][metrics].values.flatten()\n    new_vals = df_combined_scaled[(df_combined_scaled[\"Project\"] == project) & (df_combined_scaled[\"Model\"] == \"New Model\")][metrics].values.flatten()\n    \n    x = range(len(metrics))\n    bar_width = 0.35\n\n    plt.bar([xi - bar_width/2 for xi in x], sota_vals, width=bar_width, label=\"SOTA\")\n    plt.bar([xi + bar_width/2 for xi in x], new_vals, width=bar_width, label=\"New Model\")\n\n    plt.title(f\"So s√°nh SOTA vs New Model - {project}\")\n    plt.xticks(ticks=x, labels=metrics)\n    plt.ylabel(\"Gi√° tr·ªã (%)\")\n    plt.ylim(0, 110)\n    plt.grid(axis='y')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}