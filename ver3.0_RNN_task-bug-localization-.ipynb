{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":11265211,"sourceType":"datasetVersion","datasetId":7041476},{"sourceId":11280721,"sourceType":"datasetVersion","datasetId":7052679}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Load dữ liệu đã xử lí","metadata":{"id":"-eUizc5QmpX4"}},{"cell_type":"code","source":"!pip install nltk\n","metadata":{"id":"9qfVWdges0io","outputId":"e6051f73-0867-4eb4-b5c0-984722d836d3","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:24.558760Z","iopub.execute_input":"2025-04-07T01:15:24.559136Z","iopub.status.idle":"2025-04-07T01:15:28.911469Z","shell.execute_reply.started":"2025-04-07T01:15:24.559098Z","shell.execute_reply":"2025-04-07T01:15:28.910416Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import nltk\nnltk.download('averaged_perceptron_tagger_eng')","metadata":{"id":"8SUNdusXgQ-3","outputId":"937573b4-53fb-4bb5-9c39-84eb53bc1890","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:28.912930Z","iopub.execute_input":"2025-04-07T01:15:28.913168Z","iopub.status.idle":"2025-04-07T01:15:29.745069Z","shell.execute_reply.started":"2025-04-07T01:15:28.913147Z","shell.execute_reply":"2025-04-07T01:15:29.744418Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# English stop words\nstop_words = set(\n    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n     'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',\n     'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',\n     'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n     'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n     'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n     'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n     'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n     'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n     'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n     'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n     'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n     's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o',\n     're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven',\n     'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won',\n     'wouldn', 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'n', 'p', 'q', 'u', 'v',\n     'w', 'x', 'z', 'us'])\n\n# Java language keywords\njava_keywords = set(\n    ['abstract', 'assert', 'boolean', 'break', 'byte', 'case',\n     'catch', 'char', 'class', 'const', 'continue', 'default', 'do', 'double',\n     'else', 'enum', 'extends', 'false', 'final', 'finally', 'float', 'for', 'goto',\n     'if', 'implements', 'import', 'instanceof', 'int', 'interface', 'long',\n     'native', 'new', 'null', 'package', 'private', 'protected', 'public', 'return',\n     'short', 'static', 'strictfp', 'super', 'switch', 'synchronized', 'this',\n     'throw', 'throws', 'transient', 'true', 'try', 'void', 'volatile', 'while'])\n\nfrom collections import namedtuple\nfrom pathlib import Path\n\n# Dataset root directory (điều chỉnh đường dẫn nếu cần)\n_DATASET_ROOT = Path('/content/drive/MyDrive/Colab Notebooks/NLP/Task bug localization/')\n\nDataset = namedtuple('Dataset', ['name', 'src', 'bug_repo', 'repo_url', 'features'])\n\n# Các dataset được định nghĩa\naspectj = Dataset(\n    'aspectj',\n    _DATASET_ROOT / 'source files/org.aspectj',\n    _DATASET_ROOT / 'bug reports/AspectJ.txt',\n    \"https://github.com/eclipse/org.aspectj/tree/bug433351.git\",\n    _DATASET_ROOT / 'bug reports/AspectJ.xlsx'\n)\n\neclipse = Dataset(\n    'eclipse',\n    _DATASET_ROOT / 'source files/eclipse.platform.ui-johna-402445',\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.txt',\n    \"https://github.com/eclipse/eclipse.platform.ui.git\",\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.xlsx'\n)\n\nswt = Dataset(\n    'swt',\n    _DATASET_ROOT / 'source files/eclipse.platform.swt-xulrunner-31',\n    _DATASET_ROOT / 'bug reports/SWT.txt',\n    \"https://github.com/eclipse/eclipse.platform.swt.git\",\n    _DATASET_ROOT / 'bug reports/SWT.xlsx'\n)\n\ntomcat = Dataset(\n    'tomcat',\n    _DATASET_ROOT / 'source files/tomcat-7.0.51',\n    _DATASET_ROOT / 'bug reports/Tomcat.txt',\n    \"https://github.com/apache/tomcat.git\",\n    _DATASET_ROOT / 'bug reports/Tomcat.xlsx'\n)\n\nbirt = Dataset(\n    'birt',\n    _DATASET_ROOT / 'source files/birt-20140211-1400',\n    _DATASET_ROOT / 'bug reports/Birt.txt',\n    \"https://github.com/apache/birt.git\",\n    _DATASET_ROOT / 'bug reports/Birt.xlsx'\n)\n\n\n### Current dataset in use. (change this name to change the dataset)\nDATASET = tomcat\n\nclass BugReport:\n    \"\"\"Class representing each bug report\"\"\"\n    __slots__ = ['summary', 'description', 'fixed_files', 'report_time', 'pos_tagged_summary', 'pos_tagged_description','stack_traces','stack_traces_remove']\n\n    def __init__(self, summary, description, fixed_files, report_time):\n        self.summary = summary\n        self.description = description\n        self.fixed_files = fixed_files\n        self.report_time = report_time\n        self.pos_tagged_summary = None\n        self.pos_tagged_description = None\n        self.stack_traces = None\n        self.stack_traces_remove = None\n\nclass SourceFile:\n    \"\"\"Class representing each source file\"\"\"\n    __slots__ = ['all_content', 'comments', 'class_names', 'attributes', 'method_names', 'variables', 'file_name',\n                 'pos_tagged_comments', 'exact_file_name', 'package_name']\n\n    def __init__(self, all_content, comments, class_names, attributes, method_names, variables, file_name,\n                 package_name):\n        self.all_content = all_content\n        self.comments = comments\n        self.class_names = class_names\n        self.attributes = attributes\n        self.method_names = method_names\n        self.variables = variables\n        self.file_name = file_name\n        self.exact_file_name = file_name[0]\n        self.package_name = package_name\n        self.pos_tagged_comments = None\n\n\nclass Parser:\n    \"\"\"Class containing different parsers\"\"\"\n    __slots__ = ['name', 'src', 'bug_repo']\n\n    def __init__(self, pro):\n        self.name = pro.name\n        self.src = pro.src\n        self.bug_repo = pro.bug_repo\n\n    def report_parser(self):\n        reader = csv.DictReader(open(self.bug_repo, \"r\"), delimiter=\"\\t\")\n        bug_reports = OrderedDict()\n        # raw_texts = []\n        # fixed_files = []\n        for line in reader:\n            # line[\"raw_text\"] = line[\"summary\"] + ' ' + line[\"description\"]\n            line[\"report_time\"] = datetime.strptime(line[\"report_time\"], \"%Y-%m-%d %H:%M:%S\")\n            temp = line[\"files\"].strip().split(\".java\")\n            length = len(temp)\n            x = []\n            for i, f in enumerate(temp):\n                if i == (length - 1):\n                    x.append(os.path.normpath(f))\n                    continue\n                x.append(os.path.normpath(f + \".java\"))\n            line[\"files\"] = x\n            bug_reports[line[\"bug_id\"]] = BugReport(line[\"summary\"], line[\"description\"], line[\"files\"],\n                                                    line[\"report_time\"])\n        # bug_reports = tsv2dict(self.bug_repo)\n\n        return bug_reports\n\n    def src_parser(self):\n        \"\"\"Parse source code directory of a program and colect its java files\"\"\"\n\n        # Gettting the list of source files recursively from the source directory\n        src_addresses = glob.glob(str(self.src) + '/**/*.java', recursive=True)\n        print(src_addresses)\n        # Creating a java lexer instance for pygments.lex() method\n        java_lexer = JavaLexer()\n        src_files = OrderedDict()\n        # src_files = dict()\n        # Looping to parse each source file\n        for src_file in src_addresses:\n            with open(src_file, encoding='latin-1') as file:\n                src = file.read()\n\n            # Placeholder for different parts of a source file\n            comments = ''\n            class_names = []\n            attributes = []\n            method_names = []\n            variables = []\n\n            # Source parsing\n            parse_tree = None\n            try:\n                parse_tree = javalang.parse.parse(src)\n                for path, node in parse_tree.filter(javalang.tree.VariableDeclarator):\n                    if isinstance(path[-2], javalang.tree.FieldDeclaration):\n                        attributes.append(node.name)\n                    elif isinstance(path[-2], javalang.tree.VariableDeclaration):\n                        variables.append(node.name)\n            except:\n                pass\n\n            # Triming the source file\n            ind = False\n            if parse_tree:\n                if parse_tree.imports:\n                    last_imp_path = parse_tree.imports[-1].path\n                    src = src[src.index(last_imp_path) + len(last_imp_path) + 1:]\n                elif parse_tree.package:\n                    package_name = parse_tree.package.name\n                    src = src[src.index(package_name) + len(package_name) + 1:]\n                else:  # no import and no package declaration\n                    ind = True\n            # javalang can't parse the source file\n            else:\n                ind = True\n\n            # Lexically tokenize the source file\n            lexed_src = pygments.lex(src, java_lexer)\n\n            for i, token in enumerate(lexed_src):\n                if token[0] in Token.Comment:\n                    if ind and i == 0 and token[0] is Token.Comment.Multiline:\n                        src = src[src.index(token[1]) + len(token[1]):]\n                        continue\n                    comments = comments + token[1]\n                elif token[0] is Token.Name.Class:\n                    class_names.append(token[1])\n                elif token[0] is Token.Name.Function:\n                    method_names.append(token[1])\n\n            # get the package declaration if exists\n            if parse_tree and parse_tree.package:\n                package_name = parse_tree.package.name\n            else:\n                package_name = None\n\n            if self.name == 'aspectj' or 'tomcat' or 'eclipse' or 'swt':\n                src_files[os.path.relpath(src_file, start=self.src)] = SourceFile(src, comments, class_names,\n                                                                                  attributes, method_names, variables, [\n                                                                                      os.path.basename(src_file).split(\n                                                                                          '.')[0]], package_name)\n            else:\n                # If source files has package declaration\n                if package_name:\n                    src_id = (package_name + '.' + os.path.basename(src_file))\n                else:\n                    src_id = os.path.basename(src_file)\n                src_files[src_id] = SourceFile(src, comments, class_names, attributes, method_names, variables,\n                                               [os.path.basename(src_file).split('.')[0]], package_name)\n            # print(src_files)\n            # print(\"===========\")\n        return src_files\n\n\nclass ReportPreprocessing:\n    \"\"\"Class preprocess bug reports\"\"\"\n    __slots__ = ['bug_reports']\n\n    def __init__(self, bug_reports):\n        self.bug_reports = bug_reports\n\n    def extract_stack_traces(self):\n        \"\"\"Extract stack traces from bug reports\"\"\"\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            report.stack_traces = st\n\n    def extract_stack_traces_remove(self):\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            at = []\n            for x in st:\n                if (x[1] == 'Unknown Source'):\n                    temp = 'Unknown'\n                    y = x[0]+ '(' + temp\n                else:\n                    y = x[0] + '(' + x[1] + ')'\n                at.append(y)\n            report.stack_traces_remove = at\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from bug reports raw_text\"\"\"\n        for report in self.bug_reports.values():\n            # Tokenizing using word_tokeize for more accurate pos-tagging\n            sum_tok = nltk.word_tokenize(report.summary)\n            desc_tok = nltk.word_tokenize(report.description)\n            sum_pos = nltk.pos_tag(sum_tok)\n            desc_pos = nltk.pos_tag(desc_tok)\n            report.pos_tagged_summary = [token for token, pos in sum_pos if 'NN' in pos or 'VB' in pos]\n            report.pos_tagged_description = [token for token, pos in desc_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"Tokenize bug report intro tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = nltk.wordpunct_tokenize(report.summary)\n            report.description = nltk.wordpunct_tokenize(report.description)\n\n    def _split_camelcase(self, tokens):\n        # copy tokens\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camel case detection for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        \"\"\"Split camelcase indentifiers\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = self._split_camelcase(report.summary)\n            report.description = self._split_camelcase(report.description)\n            report.pos_tagged_summary = self._split_camelcase(report.pos_tagged_summary)\n            report.pos_tagged_description = self._split_camelcase(report.pos_tagged_description)\n\n    def normalize(self):\n        \"\"\"remove punctuation, numbers and lowecase conversion\"\"\"\n        # build a translate table for punctuation and number removal\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n\n        for report in self.bug_reports.values():\n            summary_punctnum_rem = [token.translate(punctnum_table) for token in report.summary]\n            desc_punctnum_rem = [token.translate(punctnum_table) for token in report.description]\n            pos_sum_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_summary]\n            pos_desc_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_description]\n            report.summary = [token.lower() for token in summary_punctnum_rem if token]\n            report.description = [token.lower() for token in desc_punctnum_rem if token]\n            report.pos_tagged_summary = [token.lower() for token in pos_sum_punctnum_rem if token]\n            report.pos_tagged_description = [token.lower() for token in pos_desc_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        \"\"\"removing stop word from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in stop_words]\n            report.description = [token for token in report.description if token not in stop_words]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in stop_words]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in stop_words]\n\n    def remove_java_keywords(self):\n        \"\"\"removing java language keywords from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in java_keywords]\n            report.description = [token for token in report.description if token not in java_keywords]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in java_keywords]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for report in self.bug_reports.values():\n            report.summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.summary], report.summary]))\n            report.description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.description], report.description]))\n            report.pos_tagged_summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_summary], report.pos_tagged_summary]))\n            report.pos_tagged_description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_description], report.pos_tagged_description]))\n\n    def preprocess(self):\n        self.extract_stack_traces()\n        self.extract_stack_traces_remove()\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_java_keywords()\n        self.stem()\n\nclass SrcPreprocessing:\n    \"\"\"class to preprocess source code\"\"\"\n    __slots__ = ['src_files']\n\n    def __init__(self, src_files):\n        self.src_files = src_files\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from comments\"\"\"\n        for src in self.src_files.values():\n            # tokenize using word_tokenize\n            comments_tok = nltk.word_tokenize(src.comments)\n            comments_pos = nltk.pos_tag(comments_tok)\n            src.pos_tagged_comments = [token for token, pos in comments_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"tokenize source code to tokens\"\"\"\n        for src in self.src_files.values():\n            src.all_content = nltk.wordpunct_tokenize(src.all_content)\n            src.comments = nltk.wordpunct_tokenize(src.comments)\n\n    def _split_camelcase(self, tokens):\n        # copy token\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camelcase defect for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        # Split camelcase indenti\n        for src in self.src_files.values():\n            src.all_content = self._split_camelcase(src.all_content)\n            src.comments = self._split_camelcase(src.comments)\n            src.class_names = self._split_camelcase(src.class_names)\n            src.attributes = self._split_camelcase(src.attributes)\n            src.method_names = self._split_camelcase(src.method_names)\n            src.variables = self._split_camelcase(src.variables)\n            src.pos_tagged_comments = self._split_camelcase(src.pos_tagged_comments)\n\n    def normalize(self):\n        \"remove punctuation, number and lowercase conversion\"\n        # build a translate table for punctuation and number\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n        for src in self.src_files.values():\n            content_punctnum_rem = [token.translate(punctnum_table) for token in src.all_content]\n            comments_punctnum_rem = [token.translate(punctnum_table) for token in src.comments]\n            classnames_punctnum_rem = [token.translate(punctnum_table) for token in src.class_names]\n            attributes_punctnum_rem = [token.translate(punctnum_table) for token in src.attributes]\n            methodnames_punctnum_rem = [token.translate(punctnum_table) for token in src.method_names]\n            variables_punctnum_rem = [token.translate(punctnum_table) for token in src.variables]\n            filename_punctnum_rem = [token.translate(punctnum_table) for token in src.file_name]\n            pos_comments_punctnum_rem = [token.translate(punctnum_table) for token in src.pos_tagged_comments]\n\n            src.all_content = [token.lower() for token in content_punctnum_rem if token]\n            src.comments = [token.lower() for token in comments_punctnum_rem if token]\n            src.class_names = [token.lower() for token in classnames_punctnum_rem if token]\n            src.attributes = [token.lower() for token in attributes_punctnum_rem if token]\n            src.method_names = [token.lower() for token in methodnames_punctnum_rem if token]\n            src.variables = [token.lower() for token in variables_punctnum_rem if token]\n            src.file_name = [token.lower() for token in filename_punctnum_rem if token]\n            src.pos_tagged_comments = [token.lower() for token in pos_comments_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in stop_words]\n            src.comments = [token for token in src.comments if token not in stop_words]\n            src.class_names = [token for token in src.class_names if token not in stop_words]\n            src.attributes = [token for token in src.attributes if token not in stop_words]\n            src.method_names = [token for token in src.method_names if token not in stop_words]\n            src.variables = [token for token in src.variables if token not in stop_words]\n            src.file_name = [token for token in src.file_name if token not in stop_words]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in stop_words]\n\n    def remove_javakeywords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in java_keywords]\n            src.comments = [token for token in src.comments if token not in java_keywords]\n            src.class_names = [token for token in src.class_names if token not in java_keywords]\n            src.attributes = [token for token in src.attributes if token not in java_keywords]\n            src.method_names = [token for token in src.method_names if token not in java_keywords]\n            src.variables = [token for token in src.variables if token not in java_keywords]\n            src.file_name = [token for token in src.file_name if token not in java_keywords]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for src in self.src_files.values():\n            src.all_content = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.all_content], src.all_content]))\n            src.comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.comments], src.comments]))\n            src.class_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.class_names], src.class_names]))\n            src.attributes = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.attributes], src.attributes]))\n            src.method_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.method_names], src.method_names]))\n            src.variables = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.variables], src.variables]))\n            src.file_name = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.file_name], src.file_name]))\n            src.pos_tagged_comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.pos_tagged_comments], src.pos_tagged_comments]))\n\n\n    def preprocess(self):\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_javakeywords()\n        self.stem()","metadata":{"id":"_22yeS4wcWpU","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:29.746574Z","iopub.execute_input":"2025-04-07T01:15:29.746790Z","iopub.status.idle":"2025-04-07T01:15:29.797152Z","shell.execute_reply.started":"2025-04-07T01:15:29.746772Z","shell.execute_reply":"2025-04-07T01:15:29.796345Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install inflection\nimport inflection\n","metadata":{"id":"ACZrz5Byh7Ur","outputId":"68b0fd14-1ac6-484a-cb84-2ab028bd5ed6","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:29.798493Z","iopub.execute_input":"2025-04-07T01:15:29.798902Z","iopub.status.idle":"2025-04-07T01:15:33.343444Z","shell.execute_reply.started":"2025-04-07T01:15:29.798881Z","shell.execute_reply":"2025-04-07T01:15:33.342609Z"}},"outputs":[{"name":"stdout","text":"Collecting inflection\n  Downloading inflection-0.5.1-py2.py3-none-any.whl.metadata (1.7 kB)\nDownloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\nInstalling collected packages: inflection\nSuccessfully installed inflection-0.5.1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')\nimport pickle\nfrom google.colab import drive\nimport csv\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport re\nimport string\nfrom nltk.stem.porter import PorterStemmer","metadata":{"id":"P9emxprnStnP","outputId":"51497efe-74ae-4abd-d5a0-485188dab6f7","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:33.344334Z","iopub.execute_input":"2025-04-07T01:15:33.344558Z","iopub.status.idle":"2025-04-07T01:15:33.615085Z","shell.execute_reply.started":"2025-04-07T01:15:33.344538Z","shell.execute_reply":"2025-04-07T01:15:33.614481Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Load dữ liệu","metadata":{"id":"SC6LbKkWy8kR"}},{"cell_type":"code","source":"import pickle\n\n# Đường dẫn đến các file pickle\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_src_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_src_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_src_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_src_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_src_processed.pkl'\n}\n\n# Load từng file và lưu vào các biến tương ứng\ndatasets = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        datasets[name] = pickle.load(f)\n\n# Kiểm tra dữ liệu đã được load vào các biến\nfor name, data in datasets.items():\n    print(f\"Data for {name}:\")\n","metadata":{"id":"O3TGVN1KzAXg","outputId":"63f347a7-1b8d-41fe-98e5-105ca43233e2","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:33.615738Z","iopub.execute_input":"2025-04-07T01:15:33.615938Z","iopub.status.idle":"2025-04-07T01:15:46.799515Z","shell.execute_reply.started":"2025-04-07T01:15:33.615921Z","shell.execute_reply":"2025-04-07T01:15:46.798628Z"}},"outputs":[{"name":"stdout","text":"Data for aspectj:\nData for eclipse:\nData for swt:\nData for tomcat:\nData for birt:\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"eclipse_src = datasets['eclipse']\nbirt_src = datasets['birt']\nswt_src = datasets['swt']\ntomcat_src = datasets['tomcat']\naspectj_src = datasets['aspectj']","metadata":{"id":"b91231aHzUu_","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:46.800451Z","iopub.execute_input":"2025-04-07T01:15:46.800762Z","iopub.status.idle":"2025-04-07T01:15:46.804312Z","shell.execute_reply.started":"2025-04-07T01:15:46.800738Z","shell.execute_reply":"2025-04-07T01:15:46.803582Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Load dữ liệu từ các file pickle đã lưu\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_reports_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_reports_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_reports_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_reports_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_reports_processed.pkl'\n}\n\n# Load từng dataset và lưu vào các biến\nall_processed_reports = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        all_processed_reports[name] = pickle.load(f)\n\n# Kiểm tra dữ liệu đã load vào\nfor dataset, reports in all_processed_reports.items():\n    print(f\"Processed reports for {dataset}:\")","metadata":{"id":"kXiYZuNgzv0M","outputId":"053bc2e9-ef09-4aea-9431-acfda03d3300","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:46.806775Z","iopub.execute_input":"2025-04-07T01:15:46.806988Z","iopub.status.idle":"2025-04-07T01:15:49.793179Z","shell.execute_reply.started":"2025-04-07T01:15:46.806961Z","shell.execute_reply":"2025-04-07T01:15:49.792480Z"}},"outputs":[{"name":"stdout","text":"Processed reports for aspectj:\nProcessed reports for eclipse:\nProcessed reports for swt:\nProcessed reports for tomcat:\nProcessed reports for birt:\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"eclipse_reports = all_processed_reports['eclipse']\nbirt_reports = all_processed_reports['birt']\nswt_reports = all_processed_reports['swt']\ntomcat_reports = all_processed_reports['tomcat']\naspectj_reports = all_processed_reports['aspectj']","metadata":{"id":"iAUv0Bnv0FcI","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:49.794543Z","iopub.execute_input":"2025-04-07T01:15:49.794864Z","iopub.status.idle":"2025-04-07T01:15:49.798493Z","shell.execute_reply.started":"2025-04-07T01:15:49.794842Z","shell.execute_reply":"2025-04-07T01:15:49.797801Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## 2. Xử lí data, gán nhãn\n- Sắp xếp bug report theo thời gian (report_time)\n- Chia thành 10 folds\n- Tạo training/test dataset theo kiểu fold i → fold i+1\n- Gán nhãn cho từng cặp (bug report, source file)","metadata":{"id":"-hwWTIRJ9PXd"}},{"cell_type":"code","source":"# B1: Lấy danh sách (bug_id, bug_report), sau đó sắp xếp theo report_time\nsorted_bug_reports = sorted(birt_reports.items(), key=lambda x: x[1].report_time)\ndata_src = birt_src","metadata":{"id":"HrKZiEgO9X16","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:49.799198Z","iopub.execute_input":"2025-04-07T01:15:49.799434Z","iopub.status.idle":"2025-04-07T01:15:49.815008Z","shell.execute_reply.started":"2025-04-07T01:15:49.799415Z","shell.execute_reply":"2025-04-07T01:15:49.814331Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def split_into_folds(sorted_reports, num_folds=10):\n    fold_size = len(sorted_reports) // num_folds\n    folds = [sorted_reports[i*fold_size:(i+1)*fold_size] for i in range(num_folds)]\n\n    # Nếu còn dư, rải đều vào các fold đầu\n    remainder = sorted_reports[num_folds*fold_size:]\n    for i, extra in enumerate(remainder):\n        folds[i].append(extra)\n    return folds\n\ndata_folds = split_into_folds(sorted_bug_reports, num_folds=10)\n","metadata":{"id":"fgRpuQKE9aA2","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:49.815868Z","iopub.execute_input":"2025-04-07T01:15:49.816126Z","iopub.status.idle":"2025-04-07T01:15:49.826714Z","shell.execute_reply.started":"2025-04-07T01:15:49.816096Z","shell.execute_reply":"2025-04-07T01:15:49.825888Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"i = 0 # thử với fold 0 → 1\ntrain_fold = data_folds[i]\ntest_fold = data_folds[i+1]","metadata":{"id":"0itPPQ5O9cDT","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:49.827571Z","iopub.execute_input":"2025-04-07T01:15:49.827876Z","iopub.status.idle":"2025-04-07T01:15:49.838129Z","shell.execute_reply.started":"2025-04-07T01:15:49.827845Z","shell.execute_reply":"2025-04-07T01:15:49.837258Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import random\n\ndef generate_balanced_pairs(bug_fold, source_files, num_negatives_per_positive=50):\n    data = []\n    for bug_id, bug in bug_fold:\n        # Danh sách file chứa bug (poszqitive)\n        positive_paths = set(bug.fixed_files)\n        positive = [\n            (bug_id, bug, src_path, source_files[src_path], 1)\n            for src_path in positive_paths if src_path in source_files\n        ]\n\n        # Danh sách file còn lại để lấy negative\n        all_paths = list(source_files.keys())\n        negative_paths = list(set(all_paths) - positive_paths)\n        sampled_negatives = random.sample(negative_paths, min(num_negatives_per_positive * len(positive), len(negative_paths)))\n\n        negative = [\n            (bug_id, bug, src_path, source_files[src_path], 0)\n            for src_path in sampled_negatives if src_path in source_files\n        ]\n\n        data.extend(positive + negative)\n    return data\ntrain_pairs = generate_balanced_pairs(train_fold, data_src, num_negatives_per_positive=50)\ntest_pairs = generate_balanced_pairs(test_fold, data_src, num_negatives_per_positive=50)\n","metadata":{"id":"wjLEL9mR9fPv","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:49.839001Z","iopub.execute_input":"2025-04-07T01:15:49.839343Z","iopub.status.idle":"2025-04-07T01:15:51.118393Z","shell.execute_reply.started":"2025-04-07T01:15:49.839315Z","shell.execute_reply":"2025-04-07T01:15:51.117717Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"Xử lí mất cân bằng","metadata":{"id":"phVI5NPj-GMs"}},{"cell_type":"code","source":"def compute_stats(pairs):\n    total = len(pairs)\n    pos = sum(1 for _, _, _, _, label in pairs if label == 1)\n    neg = total - pos\n    ratio = pos / total if total > 0 else 0\n    return total, pos, neg, ratio\n\ntotal, pos, neg, ratio = compute_stats(train_pairs)\nprint(\"📊 Train Set:\")\nprint(f\"  ➤ Tổng cặp: {total}\")\nprint(f\"  ✅ Positive (label=1): {pos}\")\nprint(f\"  ❌ Negative (label=0): {neg}\")\nprint(f\"  ⚖️ Tỷ lệ positive: {ratio:.4f}\")\n\ntotal, pos, neg, ratio = compute_stats(test_pairs)\nprint(\"\\n🧪 Test Set:\")\nprint(f\"  ➤ Tổng cặp: {total}\")\nprint(f\"  ✅ Positive (label=1): {pos}\")\nprint(f\"  ❌ Negative (label=0): {neg}\")\nprint(f\"  ⚖️ Tỷ lệ positive: {ratio:.4f}\")\n","metadata":{"id":"G-m4H5d79uzp","outputId":"1a4a413d-189b-4b5d-ca55-6552e56ce33f","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:51.119130Z","iopub.execute_input":"2025-04-07T01:15:51.119340Z","iopub.status.idle":"2025-04-07T01:15:51.131335Z","shell.execute_reply.started":"2025-04-07T01:15:51.119322Z","shell.execute_reply":"2025-04-07T01:15:51.130681Z"}},"outputs":[{"name":"stdout","text":"📊 Train Set:\n  ➤ Tổng cặp: 21318\n  ✅ Positive (label=1): 418\n  ❌ Negative (label=0): 20900\n  ⚖️ Tỷ lệ positive: 0.0196\n\n🧪 Test Set:\n  ➤ Tổng cặp: 21318\n  ✅ Positive (label=1): 418\n  ❌ Negative (label=0): 20900\n  ⚖️ Tỷ lệ positive: 0.0196\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Hàm 1: Tạo batches có bootstrapping (luôn chứa ít nhất 1 positive sample)","metadata":{}},{"cell_type":"code","source":"import random\ndef create_bootstrapped_batches(pairs, batch_size=128):\n    # Tách positive và negative\n    positives = [p for p in pairs if p[-1] == 1]\n    negatives = [p for p in pairs if p[-1] == 0]\n\n    batches = []\n    # Tính số batch có thể tạo\n    total_batches = len(pairs) // batch_size\n\n    for _ in range(total_batches):\n        # Luôn chọn ít nhất 1 positive\n        pos_sample = random.choice(positives)\n\n        # Chọn ngẫu nhiên batch_size - 1 negative samples\n        neg_samples = random.sample(negatives, batch_size - 1)\n\n        # Gộp lại, shuffle để positive không đứng đầu\n        batch = [pos_sample] + neg_samples\n        random.shuffle(batch)\n\n        batches.append(batch)\n\n    return batches\n","metadata":{"id":"VVi_cbQD-a1b","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:51.132186Z","iopub.execute_input":"2025-04-07T01:15:51.132434Z","iopub.status.idle":"2025-04-07T01:15:51.144054Z","shell.execute_reply.started":"2025-04-07T01:15:51.132416Z","shell.execute_reply":"2025-04-07T01:15:51.143192Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Focal Loss Function","metadata":{}},{"cell_type":"code","source":"\ndef focal_loss(predictions, targets, alpha=0.999, gamma=2.0, eps=1e-6):\n    \"\"\"\n    predictions: tensor (batch_size,) - output sigmoid from model\n    targets: tensor (batch_size,) - true labels (0 or 1)\n    \"\"\"\n    # Avoid log(0)\n    predictions = predictions.clamp(min=eps, max=1.0 - eps)\n\n    # Compute focal loss\n    loss = -alpha * (1 - predictions)**gamma * targets * predictions.log() \\\n           - (1 - alpha) * predictions**gamma * (1 - targets) * (1 - predictions).log()\n    return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:51.145022Z","iopub.execute_input":"2025-04-07T01:15:51.145345Z","iopub.status.idle":"2025-04-07T01:15:51.159307Z","shell.execute_reply.started":"2025-04-07T01:15:51.145311Z","shell.execute_reply":"2025-04-07T01:15:51.158353Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# 4. Trích xuất đặc trưng","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Hàm xử lý text gộp lại từ bug report\ndef bug_to_text(bug):\n    summary = bug.summary['unstemmed'] if isinstance(bug.summary, dict) else bug.summary\n    desc = bug.description['unstemmed'] if isinstance(bug.description, dict) else bug.description\n    return \" \".join(summary + desc)\n\n# Hàm xử lý text từ source file\ndef src_to_text(src):\n    content = src.all_content['unstemmed'] if isinstance(src.all_content, dict) else src.all_content\n    comments = src.comments['unstemmed'] if isinstance(src.comments, dict) else src.comments\n    return \" \".join(content + comments)\n","metadata":{"id":"4en89sb6-s54","outputId":"5c14ad2a-da00-495a-ec81-38687265bcfe","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:51.160387Z","iopub.execute_input":"2025-04-07T01:15:51.160738Z","iopub.status.idle":"2025-04-07T01:15:51.173162Z","shell.execute_reply.started":"2025-04-07T01:15:51.160705Z","shell.execute_reply":"2025-04-07T01:15:51.172524Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## Đặc trưng 1: Tính toán độ tương đồng từ vựng (lexical similarity)\n- Phương pháp: sử dụng TF-IDF và cosine similarity.\n- Input: Cặp dữ liệu (bug report, source file)\n- Output: mảng numpy chứa các giá trị độ tương đồng cosine giữa bug report và source file cho mỗi cặp.","metadata":{}},{"cell_type":"code","source":"def compute_lexical_similarity(pairs):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # Gộp cả bug + src lại để fit chung vectorizer\n    combined = bug_texts + src_texts\n    tfidf = TfidfVectorizer()\n    tfidf_matrix = tfidf.fit_transform(combined)\n\n    # Tách riêng lại từng phần\n    bug_vecs = tfidf_matrix[:len(pairs)]\n    src_vecs = tfidf_matrix[len(pairs):]\n\n    # Tính cosine cho từng cặp (theo hàng tương ứng)\n    similarities = cosine_similarity(bug_vecs, src_vecs).diagonal()\n\n    return similarities\n\n# Demo: tính feature lexical similarity cho train_pairs (giới hạn 500 mẫu vì tốc độ)\nsampled_pairs = train_pairs[:500]\nlexical_sim = compute_lexical_similarity(sampled_pairs)\n\n# Trả về dưới dạng numpy array\nlexical_sim[:10]  # Trích 10 giá trị đầu tiên để test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:51.174133Z","iopub.execute_input":"2025-04-07T01:15:51.174446Z","iopub.status.idle":"2025-04-07T01:15:52.944080Z","shell.execute_reply.started":"2025-04-07T01:15:51.174417Z","shell.execute_reply":"2025-04-07T01:15:52.943351Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"array([0.03418086, 0.05550649, 0.04123273, 0.01937713, 0.00352067,\n       0.0181865 , 0.01544532, 0.01677382, 0.05421859, 0.03415126])"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"glove_path = \"/kaggle/input/glove-embedding/glove.6B.100d.txt\"\n# Load GloVe 100d vào dictionary\nimport numpy as np\n\ndef load_glove_embeddings(filepath):\n    embeddings = {}\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.strip().split()\n            word = values[0]\n            vector = np.array(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n    \nglove_embeddings = load_glove_embeddings(glove_path)","metadata":{"id":"6FexoFrK_LTE","outputId":"b7e7b200-954e-4541-a27b-dc43607ae8f2","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:52.944887Z","iopub.execute_input":"2025-04-07T01:15:52.945194Z","iopub.status.idle":"2025-04-07T01:16:02.275954Z","shell.execute_reply.started":"2025-04-07T01:15:52.945165Z","shell.execute_reply":"2025-04-07T01:16:02.275304Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Đặc trưng 2: Tính toán độ tương đồng ngữ nghĩa (semantic similarity)\n- Phương pháp: TF-IDF weighted average của GloVe vectors và cosine similarity\n- Input:  (bug report, source file).\n- Output: Một mảng numpy chứa các giá trị độ tương đồng cosine giữa bug report và source file cho mỗi cặp, dựa trên GloVe vectors và trọng số TF-IDF.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef compute_semantic_similarity(pairs, glove_dict, dim=100):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # Dùng TF-IDF để lấy trọng số từ\n    tfidf = TfidfVectorizer()\n    tfidf.fit(bug_texts + src_texts)\n    vocab = tfidf.vocabulary_\n    idf_weights = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n\n    def embed_text(text):\n        tokens = text.split()\n        vecs = []\n        weights = []\n        for token in tokens:\n            if token in glove_dict and token in vocab:\n                vecs.append(glove_dict[token])\n                weights.append(idf_weights[token])\n        if not vecs:\n            return np.zeros(dim)\n        vecs = np.array(vecs)\n        weights = np.array(weights).reshape(-1, 1)\n        weighted_vecs = vecs * weights\n        return weighted_vecs.sum(axis=0) / weights.sum()\n\n    # Tính vector trung bình cho bug và src\n    bug_vecs = [embed_text(text) for text in bug_texts]\n    src_vecs = [embed_text(text) for text in src_texts]\n\n    # Tính cosine similarity giữa từng cặp\n    similarities = [cosine_similarity([b], [s])[0][0] for b, s in zip(bug_vecs, src_vecs)]\n\n    return np.array(similarities)\nsampled_pairs = train_pairs[:500]\n# Demo trên 500 cặp như lexical\nsemantic_sim = compute_semantic_similarity(sampled_pairs, glove_embeddings)\nsemantic_sim[:10]\n","metadata":{"id":"BmKfMQ9oAFSQ","outputId":"e0bf60b5-8bc7-401f-c8ee-5e8ac04ebe01","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:16:02.276755Z","iopub.execute_input":"2025-04-07T01:16:02.276968Z","iopub.status.idle":"2025-04-07T01:16:03.366062Z","shell.execute_reply.started":"2025-04-07T01:16:02.276949Z","shell.execute_reply":"2025-04-07T01:16:03.365351Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"array([0.67417744, 0.57356281, 0.54573841, 0.64720149, 0.40659827,\n       0.52761221, 0.5508162 , 0.53280833, 0.65285643, 0.59984962])"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"### Đặc trưng 3: Similar Bug Report Score ","metadata":{}},{"cell_type":"markdown","source":"→ Kiểm tra xem bug report này có giống **những bug report cũ từng sửa cùng file đó** không?\n\n- `build_bug_fix_history(pairs)` → XD lịch sử chỉnh sửa theo từng file\n- `compute_similar_bug_score(pairs, history)`\n    - Input: pairs, history\n    - So sánh bug hiện tại và bug cũ:\n    \n    cosine_similarity(TfidfVectorizer().fit_transform([bug_now, bug_old]))[0, 1]\n    \n    - Lấy giá trị tương đồng cao nhất vừa tìm được","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\n\n# Bổ trợ: tạo map file_path -> list of (bug_id, report_time, bug_text)\ndef build_bug_fix_history(pairs):\n    history = {}\n    for bug_id, bug, src_path, _, label in pairs:\n        if label == 1:  # chỉ tính các bug thật sự sửa file\n            if src_path not in history:\n                history[src_path] = []\n            history[src_path].append((bug_id, bug.report_time, bug_to_text(bug)))\n    return history\n\n# Đặc trưng 3: Similar Bug Report Score\ndef compute_similar_bug_score(pairs, history):\n    scores = []\n    for bug_id, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        current_text = bug_to_text(bug)\n\n        sim_scores = []\n        if src_path in history:\n            for hist_bug_id, hist_time, hist_text in history[src_path]:\n                if hist_time < current_time:  # chỉ tính bug trong quá khứ\n                    sim = cosine_similarity(\n                        TfidfVectorizer().fit_transform([current_text, hist_text])\n                    )[0, 1]\n                    sim_scores.append(sim)\n        scores.append(max(sim_scores) if sim_scores else 0.0)\n    return np.array(scores)\n","metadata":{"id":"imb4_du_Bgjz","outputId":"8c08c8e9-7047-4e5d-de85-8b70e30cbe28","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:16:03.366866Z","iopub.execute_input":"2025-04-07T01:16:03.367110Z","iopub.status.idle":"2025-04-07T01:16:03.373032Z","shell.execute_reply.started":"2025-04-07T01:16:03.367090Z","shell.execute_reply":"2025-04-07T01:16:03.372288Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"### Đặc trung 4: Time Since Last Fix (ngày, normalize)\n- Kiểm tra với mỗi `(bug report, source file)` xem từng được sửa trước đó không và lần cuối khi nào\n    - Đã lâu k sửa → Ít lỗi → Điểm thấp\n    - Mới sửa → có thể liên quan tới lỗi → Điểm cao\n- Cách hđ:\n    - Tìm thời điểm bug current_time\n    - Tìm history các lần sửa file trong quá khứ\n    - Tính khoảng cách time giữa current và history gần nhất\n    - Chưa sửa → Gán số delta_days=9999\n    - Chuẩn hoá","metadata":{}},{"cell_type":"code","source":"# Đặc trưng 4: Time Since Last Fix (ngày, normalize)\ndef compute_time_since_last_fix(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_times = [hist_time for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            if past_times:\n                delta_days = (current_time - max(past_times)).days\n            else:\n                delta_days = 9999  # Cực lớn nếu chưa từng sửa\n        else:\n            delta_days = 9999\n        scores.append(delta_days)\n\n    # Normalize về [0,1]\n    max_days = max(scores) if max(scores) != 0 else 1  # Tránh chia cho 0\n\n    return np.array([1 - (s / max_days) for s in scores])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:16:03.373960Z","iopub.execute_input":"2025-04-07T01:16:03.374223Z","iopub.status.idle":"2025-04-07T01:16:03.388947Z","shell.execute_reply.started":"2025-04-07T01:16:03.374198Z","shell.execute_reply":"2025-04-07T01:16:03.388236Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"### Đặc trưng 5: Fix Frequency (số lần bị sửa trong quá khứ, normalize)\n\n\n- Kiểm tra xme mỗi cặp được sửa bao nhiêu lần\n\n→ Sửa nhiều → File dễ dính lỗi → Điểm cao","metadata":{}},{"cell_type":"code","source":"# Đặc trưng 5: Fix Frequency (số lần bị sửa trong quá khứ, normalize)\ndef compute_fix_frequency(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_fixes = [1 for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            freq = len(past_fixes)\n        else:\n            freq = 0\n        scores.append(freq)\n    # Normalize về [0,1] an toàn\n    max_freq = max(scores)\n    max_freq = max(max_freq, 1)  # tránh chia 0\n    return np.array([s / max_freq for s in scores])\n\n\n# Dùng cho 500 cặp mẫu\nsampled_pairs = train_pairs[:5000]\nbug_history = build_bug_fix_history(train_pairs)\n\nsimilar_bug_score = compute_similar_bug_score(sampled_pairs, bug_history)\ntime_since_last_fix = compute_time_since_last_fix(sampled_pairs, bug_history)\nfix_frequency = compute_fix_frequency(sampled_pairs, bug_history)\n\n# Trích 5 giá trị đầu mỗi feature\nsimilar_bug_score[:50], time_since_last_fix[:50], fix_frequency[:50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:16:03.391793Z","iopub.execute_input":"2025-04-07T01:16:03.391984Z","iopub.status.idle":"2025-04-07T01:16:03.577940Z","shell.execute_reply.started":"2025-04-07T01:16:03.391968Z","shell.execute_reply":"2025-04-07T01:16:03.577267Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"# 4. Quá trình huấn luyện","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Tạo ma trận train, test","metadata":{}},{"cell_type":"code","source":"# Gộp 5 đặc trưng lại thành feature matrix X (n_samples, 5)\ndef build_feature_matrix(pairs, glove_dict, history):\n    lexical = compute_lexical_similarity(pairs)\n    semantic = compute_semantic_similarity(pairs, glove_dict)\n    similar_score = compute_similar_bug_score(pairs, history)\n    recency = compute_time_since_last_fix(pairs, history)\n    freq = compute_fix_frequency(pairs, history)\n\n    # Gộp lại theo chiều dọc → ma trận (n_samples, 5)\n    X = np.stack([lexical, semantic, similar_score, recency, freq], axis=1)\n\n    return X\n\n# Tạo nhãn y\ndef get_labels(pairs):\n    return np.array([label for *_, label in pairs])\n\n# Tạo dữ liệu train từ sampled_pairs\nX_train = build_feature_matrix(train_pairs, glove_embeddings, bug_history)\ny_train = get_labels(train_pairs)\n\n# In shape để xác nhận\nX_train.shape, y_train.shape\n","metadata":{"id":"PmEZSTiyDEB4","outputId":"6db11041-f3a2-4931-f264-9480bfd7c2e6","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:16:03.579023Z","iopub.execute_input":"2025-04-07T01:16:03.579316Z","iopub.status.idle":"2025-04-07T01:17:22.152743Z","shell.execute_reply.started":"2025-04-07T01:16:03.579288Z","shell.execute_reply":"2025-04-07T01:17:22.151977Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"((21318, 5), (21318,))"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n# Chuẩn bị Dataset & Dataloader từ numpy\ndef create_dataloader(X, y, batch_size=128):\n    X_tensor = torch.tensor(X, dtype=torch.float32)\n    y_tensor = torch.tensor(y, dtype=torch.float32)\n    dataset = TensorDataset(X_tensor, y_tensor)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:17:22.153537Z","iopub.execute_input":"2025-04-07T01:17:22.153756Z","iopub.status.idle":"2025-04-07T01:17:25.553975Z","shell.execute_reply.started":"2025-04-07T01:17:22.153736Z","shell.execute_reply":"2025-04-07T01:17:25.553267Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"## 4.2 Xây dựng mô hình","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\n# Định nghĩa mô hình DNN giống bài báo\nimport torch\nimport torch.nn as nn\n\nclass BugLocalization(nn.Module):\n    def __init__(self, input_dim=5, hidden_dim=64, output_dim=1):\n        super(BugLocalization, self).__init__()\n        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Nếu x chỉ có 2 chiều (batch_size, input_dim), hãy thêm một chiều giả định (sequence_length=1)\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)  # Thêm một chiều giả định (batch_size, 1, input_dim)\n\n        # x shape: (batch_size, sequence_length, input_dim)\n        rnn_out, _ = self.rnn(x)\n        # Lấy output của phần cuối cùng trong chuỗi\n        final_rnn_out = rnn_out[:, -1, :]\n        out = torch.sigmoid(self.fc(final_rnn_out)).squeeze()\n        return out\n\n        \n# Định nghĩa focal loss\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.999, gamma=2.0, eps=1e-6):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, preds, targets):\n        preds = preds.clamp(min=self.eps, max=1. - self.eps)\n        loss = -self.alpha * (1 - preds) ** self.gamma * targets * torch.log(preds) \\\n               - (1 - self.alpha) * preds ** self.gamma * (1 - targets) * torch.log(1 - preds)\n        return loss.mean()\n\n\n# Huấn luyện mô hình\ndef train_model(model, dataloader, epochs=10, lr=1e-3):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = FocalLoss()\n\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch_X, batch_y in dataloader:\n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n\n# Tạo dataloader & model, bắt đầu huấn luyện\ntrain_loader = create_dataloader(X_train, y_train, batch_size=128)\nmodel = BugLocalization()\ntrain_model(model, train_loader, epochs=5)\n","metadata":{"id":"LiD_wJxjDeQ8","outputId":"9821aee2-a66e-480b-9f69-6e4d207c7ba6","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:17:25.554751Z","iopub.execute_input":"2025-04-07T01:17:25.555074Z","iopub.status.idle":"2025-04-07T01:17:31.691067Z","shell.execute_reply.started":"2025-04-07T01:17:25.555054Z","shell.execute_reply":"2025-04-07T01:17:31.690295Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5 - Loss: 0.2112\nEpoch 2/5 - Loss: 0.1602\nEpoch 3/5 - Loss: 0.1519\nEpoch 4/5 - Loss: 0.1455\nEpoch 5/5 - Loss: 0.1404\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score, label_ranking_average_precision_score\nimport numpy as np\n\n# Chuẩn bị tập test\nsampled_test_pairs = test_pairs\nbug_history_test = build_bug_fix_history(test_pairs)\n\nX_test = build_feature_matrix(sampled_test_pairs, glove_embeddings, bug_history_test)\ny_test = get_labels(sampled_test_pairs)\n\n#  Dự đoán xác suất từ mô hình\nmodel.eval()\nwith torch.no_grad():\n    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n    y_pred_probs = model(X_test_tensor).numpy()\n\n# Đánh giá các chỉ số (MAP, MRR, Top-k)\ndef compute_topk_accuracy(y_true, y_scores, k=10):\n    bug_to_scores = {}\n    for (bug_id, _, src_path, _, label), score in zip(sampled_test_pairs, y_scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    correct_at_k = 0\n    total = 0\n\n    for bug_id, entries in bug_to_scores.items():\n        sorted_entries = sorted(entries, key=lambda x: x[0], reverse=True)\n        top_k = sorted_entries[:k]\n        if any(label == 1 for _, label in top_k):\n            correct_at_k += 1\n        total += 1\n\n    return correct_at_k / total if total > 0 else 0\n\n# MAP (Mean Average Precision)\nmap_score = average_precision_score(y_test, y_pred_probs)\n\n# MRR (Mean Reciprocal Rank)\ndef mean_reciprocal_rank(pairs, scores):\n    bug_to_scores = {}\n    for (bug_id, _, _, _, label), score in zip(pairs, scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    rr_sum = 0\n    count = 0\n    for bug_id, ranked in bug_to_scores.items():\n        ranked = sorted(ranked, key=lambda x: x[0], reverse=True)\n        for idx, (_, label) in enumerate(ranked):\n            if label == 1:\n                rr_sum += 1 / (idx + 1)\n                break\n        count += 1\n    return rr_sum / count if count > 0 else 0\n\nmrr_score = mean_reciprocal_rank(sampled_test_pairs, y_pred_probs)\n\n# Top-k Accuracy (k = 1, 5, 10, 15)\ntop1 = compute_topk_accuracy(y_test, y_pred_probs, k=1)\ntop5 = compute_topk_accuracy(y_test, y_pred_probs, k=5)\ntop10 = compute_topk_accuracy(y_test, y_pred_probs, k=10)\ntop15 = compute_topk_accuracy(y_test, y_pred_probs, k=15)\n\n# Trả về kết quả\nmap_score, mrr_score, top1, top5, top10, top15","metadata":{"id":"hvK-tE_MD4zq","outputId":"5444ce3e-4624-4f81-eb6b-b19a543b0d49","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:17:31.691968Z","iopub.execute_input":"2025-04-07T01:17:31.692337Z","iopub.status.idle":"2025-04-07T01:18:49.994245Z","shell.execute_reply.started":"2025-04-07T01:17:31.692314Z","shell.execute_reply":"2025-04-07T01:18:49.993202Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"(0.4398018297288224,\n 0.6382144271018614,\n 0.5023923444976076,\n 0.8205741626794258,\n 0.9043062200956937,\n 0.9401913875598086)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"import random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import average_precision_score\n\n# ✅ Kiểm tra và xác định thiết bị (GPU hoặc CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ✅ Chạy đánh giá toàn bộ mô hình theo kiểu k-fold như bài báo\ndef run_kfold_training_and_eval(folds, source_files, glove_dict, k=9):\n    results = {\n        \"fold\": [],\n        \"MAP\": [],\n        \"MRR\": [],\n        \"Top1\": [],\n        \"Top2\": [],\n        \"Top3\": [],\n        \"Top4\": [],\n        \"Top5\": [],\n        \"Top10\": [],\n        \"Top15\": []\n    }\n\n    for i in range(9):\n        print(f\"\\n📦 Fold {i} ➤ {i+1}\")\n        train_fold = folds[i]\n        test_fold = folds[i + 1]\n\n        train_pairs = generate_balanced_pairs(train_fold, source_files)\n        test_pairs = generate_balanced_pairs(test_fold, source_files)\n\n        # Skip if not enough positive samples\n        if sum(1 for p in train_pairs if p[-1] == 1) < 1:\n            print(\"⚠️ Bỏ qua do quá ít positive samples\")\n            continue\n\n        # Build history only from train set\n        bug_history = build_bug_fix_history(train_pairs)\n\n        # Feature extraction\n        X_train = build_feature_matrix(train_pairs, glove_dict, bug_history)\n        y_train = get_labels(train_pairs)\n\n        X_test = build_feature_matrix(test_pairs, glove_dict, bug_history)\n        y_test = get_labels(test_pairs)\n\n        # Create batches with bootstrapping\n        train_batches = create_bootstrapped_batches(train_pairs, batch_size=128)\n\n        # Model initialization\n        model = BugLocalization().to(device)  # Chuyển mô hình sang GPU nếu có\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n        # Training loop\n        model.train()\n        for epoch in range(10):  # Huấn luyện trong 5 epoch\n            total_loss = 0\n            for batch in train_batches:\n                # Lấy đặc trưng (feature) từ batch và chuyển sang GPU\n                # Lấy đặc trưng (feature) từ batch và chuyển sang GPU\n                X_batch = torch.tensor(np.array([build_feature_matrix([x], glove_dict, bug_history) for x in batch]), dtype=torch.float32).to(device)\n                y_batch = torch.tensor([x[-1] for x in batch], dtype=torch.float32).to(device)\n\n                optimizer.zero_grad()\n                y_pred = model(X_batch)\n                loss = focal_loss(y_pred, y_batch)  # Sử dụng Focal Loss\n                loss.backward()\n                optimizer.step()\n\n                total_loss += loss.item()\n\n            print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_batches)}\")\n\n        # Predict\n        model.eval()\n        with torch.no_grad():\n            X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)  # Chuyển dữ liệu vào GPU\n            y_pred_probs = model(X_test_tensor).cpu().numpy()  # Dự đoán xác suất và chuyển về CPU\n\n        # Evaluate\n        sampled_test_pairs = test_pairs\n        map_score = average_precision_score(y_test, y_pred_probs)\n        mrr_score = mean_reciprocal_rank(sampled_test_pairs, y_pred_probs)\n        top1 = compute_topk_accuracy(y_test, y_pred_probs, k=1)\n        top2 = compute_topk_accuracy(y_test, y_pred_probs, k=2)\n        top3 = compute_topk_accuracy(y_test, y_pred_probs, k=3)\n        top4 = compute_topk_accuracy(y_test, y_pred_probs, k=4)\n        top5 = compute_topk_accuracy(y_test, y_pred_probs, k=5)\n        top10 = compute_topk_accuracy(y_test, y_pred_probs, k=10)\n        top15 = compute_topk_accuracy(y_test, y_pred_probs, k=15)\n\n        print(f\"Fold {i + 1} Results:\")\n        print(f\"  ➤ MAP: {map_score:.4f}\")\n        print(f\"  ➤ MRR: {mrr_score:.4f}\")\n        print(f\"  ➤ Top1: {top1:.4f}\")\n        print(f\"  ➤ Top2: {top2:.4f}\")\n        print(f\"  ➤ Top3: {top3:.4f}\")\n        print(f\"  ➤ Top4: {top4:.4f}\")\n        print(f\"  ➤ Top5: {top5:.4f}\")\n        print(f\"  ➤ Top10: {top10:.4f}\")\n        print(f\"  ➤ Top15: {top15:.4f}\")\n\n        # Save results\n        results[\"fold\"].append(i)\n        results[\"MAP\"].append(map_score)\n        results[\"MRR\"].append(mrr_score)\n        results[\"Top1\"].append(top1)\n        results[\"Top2\"].append(top2)\n        results[\"Top3\"].append(top3)\n        results[\"Top4\"].append(top4)\n        results[\"Top5\"].append(top5)\n        results[\"Top10\"].append(top10)\n        results[\"Top15\"].append(top15)\n\n    return results","metadata":{"id":"0esoYB7ME-At","outputId":"cbd0b93b-371f-4886-b013-7532bccb8b96","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:18:49.995541Z","iopub.execute_input":"2025-04-07T01:18:49.995924Z","iopub.status.idle":"2025-04-07T01:18:50.010536Z","shell.execute_reply.started":"2025-04-07T01:18:49.995891Z","shell.execute_reply":"2025-04-07T01:18:50.009726Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"\n# Run K-fold training and evaluation\nfull_results = run_kfold_training_and_eval(data_folds, data_src, glove_embeddings)\n\n# Output full results\nprint(\"\\nFull Results:\")\nfor key, value in full_results.items():\n    print(f\"{key}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:18:50.011475Z","iopub.execute_input":"2025-04-07T01:18:50.011741Z","iopub.status.idle":"2025-04-07T05:13:27.436897Z","shell.execute_reply.started":"2025-04-07T01:18:50.011720Z","shell.execute_reply":"2025-04-07T05:13:27.436107Z"}},"outputs":[{"name":"stdout","text":"\n📦 Fold 0 ➤ 1\nEpoch 1: Loss = 0.0008753524049667709\nEpoch 2: Loss = 0.0007048352168808433\nEpoch 3: Loss = 0.0006611337410131784\nEpoch 4: Loss = 0.0006357434576346416\nEpoch 5: Loss = 0.0006205207871270355\nEpoch 6: Loss = 0.0006115372145074386\nEpoch 7: Loss = 0.0006064673770339711\nEpoch 8: Loss = 0.0006037643398452798\nEpoch 9: Loss = 0.0006023535078197874\nEpoch 10: Loss = 0.0006015806189592344\nFold 1 Results:\n  ➤ MAP: 0.4059\n  ➤ MRR: 0.5927\n  ➤ Top1: 0.4569\n  ➤ Top2: 0.5837\n  ➤ Top3: 0.6627\n  ➤ Top4: 0.7225\n  ➤ Top5: 0.7632\n  ➤ Top10: 0.8876\n  ➤ Top15: 0.9211\n\n📦 Fold 1 ➤ 2\nEpoch 1: Loss = 0.0008152179954382759\nEpoch 2: Loss = 0.0006687378618481895\nEpoch 3: Loss = 0.0006291639136574063\nEpoch 4: Loss = 0.0006119119052856555\nEpoch 5: Loss = 0.0006033652011481821\nEpoch 6: Loss = 0.0005990530852318169\nEpoch 7: Loss = 0.0005969365591292707\nEpoch 8: Loss = 0.0005959208187905241\nEpoch 9: Loss = 0.0005953949309832204\nEpoch 10: Loss = 0.000595058818033571\nFold 2 Results:\n  ➤ MAP: 0.4296\n  ➤ MRR: 0.6436\n  ➤ Top1: 0.5144\n  ➤ Top2: 0.6531\n  ➤ Top3: 0.7249\n  ➤ Top4: 0.7727\n  ➤ Top5: 0.7990\n  ➤ Top10: 0.8876\n  ➤ Top15: 0.9282\n\n📦 Fold 2 ➤ 3\nEpoch 1: Loss = 0.0008617430832096746\nEpoch 2: Loss = 0.0007077728561897123\nEpoch 3: Loss = 0.0006584210860604668\nEpoch 4: Loss = 0.0006235240115671901\nEpoch 5: Loss = 0.0006002189659202157\nEpoch 6: Loss = 0.0005855050705182534\nEpoch 7: Loss = 0.0005768838223541178\nEpoch 8: Loss = 0.0005722163721613294\nEpoch 9: Loss = 0.0005697301884899926\nEpoch 10: Loss = 0.0005682962220751243\nFold 3 Results:\n  ➤ MAP: 0.3458\n  ➤ MRR: 0.5711\n  ➤ Top1: 0.4378\n  ➤ Top2: 0.5646\n  ➤ Top3: 0.6340\n  ➤ Top4: 0.7105\n  ➤ Top5: 0.7344\n  ➤ Top10: 0.8349\n  ➤ Top15: 0.8923\n\n📦 Fold 3 ➤ 4\nEpoch 1: Loss = 0.0008468588327439165\nEpoch 2: Loss = 0.0007079207658885518\nEpoch 3: Loss = 0.0006624309339768153\nEpoch 4: Loss = 0.0006348632284123114\nEpoch 5: Loss = 0.0006195212819919282\nEpoch 6: Loss = 0.0006117607806654962\nEpoch 7: Loss = 0.0006080362134043643\nEpoch 8: Loss = 0.0006061618278400568\nEpoch 9: Loss = 0.0006050350067713466\nEpoch 10: Loss = 0.0006042066141009241\nFold 4 Results:\n  ➤ MAP: 0.3367\n  ➤ MRR: 0.5827\n  ➤ Top1: 0.4522\n  ➤ Top2: 0.5742\n  ➤ Top3: 0.6651\n  ➤ Top4: 0.7033\n  ➤ Top5: 0.7608\n  ➤ Top10: 0.8397\n  ➤ Top15: 0.8828\n\n📦 Fold 4 ➤ 5\nEpoch 1: Loss = 0.000836548163325537\nEpoch 2: Loss = 0.000715898312186452\nEpoch 3: Loss = 0.0006796790676819811\nEpoch 4: Loss = 0.0006609673040963888\nEpoch 5: Loss = 0.0006498643959779853\nEpoch 6: Loss = 0.0006434353181029143\nEpoch 7: Loss = 0.0006400011842775843\nEpoch 8: Loss = 0.0006382745486916974\nEpoch 9: Loss = 0.0006373931668875515\nEpoch 10: Loss = 0.0006369002829993099\nFold 5 Results:\n  ➤ MAP: 0.2909\n  ➤ MRR: 0.5842\n  ➤ Top1: 0.4426\n  ➤ Top2: 0.5813\n  ➤ Top3: 0.6651\n  ➤ Top4: 0.7249\n  ➤ Top5: 0.7727\n  ➤ Top10: 0.8708\n  ➤ Top15: 0.9115\n\n📦 Fold 5 ➤ 6\nEpoch 1: Loss = 0.0008848024639497366\nEpoch 2: Loss = 0.0007461346044199519\nEpoch 3: Loss = 0.0007038661611900406\nEpoch 4: Loss = 0.0006769285461913053\nEpoch 5: Loss = 0.0006611833318363853\nEpoch 6: Loss = 0.0006526906419536997\nEpoch 7: Loss = 0.0006485523742620822\nEpoch 8: Loss = 0.0006466455096996064\nEpoch 9: Loss = 0.0006457042797252603\nEpoch 10: Loss = 0.0006451486330759902\nFold 6 Results:\n  ➤ MAP: 0.3769\n  ➤ MRR: 0.5872\n  ➤ Top1: 0.4737\n  ➤ Top2: 0.5885\n  ➤ Top3: 0.6411\n  ➤ Top4: 0.6770\n  ➤ Top5: 0.7057\n  ➤ Top10: 0.8086\n  ➤ Top15: 0.8780\n\n📦 Fold 6 ➤ 7\nEpoch 1: Loss = 0.0008878822677561183\nEpoch 2: Loss = 0.0007270471927855747\nEpoch 3: Loss = 0.0006954796997625493\nEpoch 4: Loss = 0.0006808229784848707\nEpoch 5: Loss = 0.0006731949253596859\nEpoch 6: Loss = 0.000668823847362031\nEpoch 7: Loss = 0.0006662445268748014\nEpoch 8: Loss = 0.0006647259589559567\nEpoch 9: Loss = 0.0006638137253382838\nEpoch 10: Loss = 0.0006632259927678808\nFold 7 Results:\n  ➤ MAP: 0.3732\n  ➤ MRR: 0.5576\n  ➤ Top1: 0.4211\n  ➤ Top2: 0.5622\n  ➤ Top3: 0.6340\n  ➤ Top4: 0.6746\n  ➤ Top5: 0.7225\n  ➤ Top10: 0.8254\n  ➤ Top15: 0.8780\n\n📦 Fold 7 ➤ 8\nEpoch 1: Loss = 0.0008490718957261149\nEpoch 2: Loss = 0.0007361957455525197\nEpoch 3: Loss = 0.000700446056112288\nEpoch 4: Loss = 0.0006771210785492335\nEpoch 5: Loss = 0.0006634620006381344\nEpoch 6: Loss = 0.0006562013593530394\nEpoch 7: Loss = 0.000652643592280605\nEpoch 8: Loss = 0.0006509565400080306\nEpoch 9: Loss = 0.0006500964594025906\nEpoch 10: Loss = 0.000649581050727494\nFold 8 Results:\n  ➤ MAP: 0.3834\n  ➤ MRR: 0.5864\n  ➤ Top1: 0.4495\n  ➤ Top2: 0.5745\n  ➤ Top3: 0.6562\n  ➤ Top4: 0.7139\n  ➤ Top5: 0.7620\n  ➤ Top10: 0.8822\n  ➤ Top15: 0.9303\n\n📦 Fold 8 ➤ 9\nEpoch 1: Loss = 0.0008200137632560323\nEpoch 2: Loss = 0.0007118132145313376\nEpoch 3: Loss = 0.0006770762210979945\nEpoch 4: Loss = 0.0006571697229942815\nEpoch 5: Loss = 0.0006443904901530142\nEpoch 6: Loss = 0.0006363614460552167\nEpoch 7: Loss = 0.0006316511503731211\nEpoch 8: Loss = 0.0006289575120194279\nEpoch 9: Loss = 0.0006273378208844048\nEpoch 10: Loss = 0.0006262673528963757\nFold 9 Results:\n  ➤ MAP: 0.4184\n  ➤ MRR: 0.6230\n  ➤ Top1: 0.4892\n  ➤ Top2: 0.6313\n  ➤ Top3: 0.7108\n  ➤ Top4: 0.7566\n  ➤ Top5: 0.7855\n  ➤ Top10: 0.8819\n  ➤ Top15: 0.9229\n\nFull Results:\nfold: [0, 1, 2, 3, 4, 5, 6, 7, 8]\nMAP: [0.40594291518421294, 0.42964434428945575, 0.3457719387079057, 0.3366605267318004, 0.29093389926052166, 0.376856841997509, 0.37322238588413226, 0.3833729835838348, 0.41838771044511014]\nMRR: [0.5926757578231548, 0.6435867063768955, 0.5710986287648228, 0.5826895060385546, 0.5842187223254296, 0.5871674619121703, 0.5576263238205738, 0.586425335305539, 0.6229862391905384]\nTop1: [0.4569377990430622, 0.5143540669856459, 0.43779904306220097, 0.45215311004784686, 0.44258373205741625, 0.47368421052631576, 0.42105263157894735, 0.4495192307692308, 0.4891566265060241]\nTop2: [0.583732057416268, 0.65311004784689, 0.5645933014354066, 0.5741626794258373, 0.5813397129186603, 0.5885167464114832, 0.562200956937799, 0.5745192307692307, 0.6313253012048192]\nTop3: [0.6626794258373205, 0.7248803827751196, 0.6339712918660287, 0.6650717703349283, 0.6650717703349283, 0.6411483253588517, 0.6339712918660287, 0.65625, 0.7108433734939759]\nTop4: [0.722488038277512, 0.7727272727272727, 0.7105263157894737, 0.7033492822966507, 0.7248803827751196, 0.6770334928229665, 0.6746411483253588, 0.7139423076923077, 0.7566265060240964]\nTop5: [0.7631578947368421, 0.7990430622009569, 0.7344497607655502, 0.7607655502392344, 0.7727272727272727, 0.7057416267942583, 0.722488038277512, 0.7620192307692307, 0.7855421686746988]\nTop10: [0.8875598086124402, 0.8875598086124402, 0.8349282296650717, 0.8397129186602871, 0.8708133971291866, 0.8086124401913876, 0.8253588516746412, 0.8822115384615384, 0.8819277108433735]\nTop15: [0.9210526315789473, 0.9282296650717703, 0.8923444976076556, 0.8827751196172249, 0.9114832535885168, 0.8779904306220095, 0.8779904306220095, 0.9302884615384616, 0.9228915662650602]\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n# In kết quả tổng hợp sau khi chạy tất cả folds\nprint(\"\\n📊 Kết quả tổng hợp:\")\nfor i in range(len(full_results[\"fold\"])):\n    print(f\"Fold {full_results['fold'][i]}:\")\n    print(f\"  ➤ MAP: {full_results['MAP'][i]:.4f}\")\n    print(f\"  ➤ MRR: {full_results['MRR'][i]:.4f}\")\n    print(f\"  ➤ Top1: {full_results['Top1'][i]:.4f}\")\n    print(f\"  ➤ Top2: {full_results['Top2'][i]:.4f}\")\n    print(f\"  ➤ Top3: {full_results['Top3'][i]:.4f}\")\n    print(f\"  ➤ Top4: {full_results['Top4'][i]:.4f}\")\n    print(f\"  ➤ Top5: {full_results['Top5'][i]:.4f}\")\n    print(f\"  ➤ Top10: {full_results['Top10'][i]:.4f}\")\n    print(f\"  ➤ Top15: {full_results['Top15'][i]:.4f}\")\n\n# Tính trung bình cho tất cả các chỉ số\nmean_map = np.mean(full_results[\"MAP\"])\nmean_mrr = np.mean(full_results[\"MRR\"])\nmean_top1 = np.mean(full_results[\"Top1\"])\nmean_top2 = np.mean(full_results[\"Top2\"])\nmean_top3 = np.mean(full_results[\"Top3\"])\nmean_top4 = np.mean(full_results[\"Top4\"])\nmean_top5 = np.mean(full_results[\"Top5\"])\nmean_top10 = np.mean(full_results[\"Top10\"])\nmean_top15 = np.mean(full_results[\"Top15\"])\n\n# In kết quả trung bình\nprint(\"\\n📊 Kết quả trung bình trên toàn bộ k-folds:\")\nprint(f\"  ➤ MAP: {mean_map:.4f}\")\nprint(f\"  ➤ MRR: {mean_mrr:.4f}\")\nprint(f\"  ➤ Top1: {mean_top1:.4f}\")\nprint(f\"  ➤ Top2: {mean_top2:.4f}\")\nprint(f\"  ➤ Top3: {mean_top3:.4f}\")\nprint(f\"  ➤ Top4: {mean_top4:.4f}\")\nprint(f\"  ➤ Top5: {mean_top5:.4f}\")\nprint(f\"  ➤ Top10: {mean_top10:.4f}\")\nprint(f\"  ➤ Top15: {mean_top15:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:13:27.437892Z","iopub.execute_input":"2025-04-07T05:13:27.438139Z","iopub.status.idle":"2025-04-07T05:13:27.458909Z","shell.execute_reply.started":"2025-04-07T05:13:27.438118Z","shell.execute_reply":"2025-04-07T05:13:27.458169Z"}},"outputs":[{"name":"stdout","text":"\n📊 Kết quả tổng hợp:\nFold 0:\n  ➤ MAP: 0.4059\n  ➤ MRR: 0.5927\n  ➤ Top1: 0.4569\n  ➤ Top2: 0.5837\n  ➤ Top3: 0.6627\n  ➤ Top4: 0.7225\n  ➤ Top5: 0.7632\n  ➤ Top10: 0.8876\n  ➤ Top15: 0.9211\nFold 1:\n  ➤ MAP: 0.4296\n  ➤ MRR: 0.6436\n  ➤ Top1: 0.5144\n  ➤ Top2: 0.6531\n  ➤ Top3: 0.7249\n  ➤ Top4: 0.7727\n  ➤ Top5: 0.7990\n  ➤ Top10: 0.8876\n  ➤ Top15: 0.9282\nFold 2:\n  ➤ MAP: 0.3458\n  ➤ MRR: 0.5711\n  ➤ Top1: 0.4378\n  ➤ Top2: 0.5646\n  ➤ Top3: 0.6340\n  ➤ Top4: 0.7105\n  ➤ Top5: 0.7344\n  ➤ Top10: 0.8349\n  ➤ Top15: 0.8923\nFold 3:\n  ➤ MAP: 0.3367\n  ➤ MRR: 0.5827\n  ➤ Top1: 0.4522\n  ➤ Top2: 0.5742\n  ➤ Top3: 0.6651\n  ➤ Top4: 0.7033\n  ➤ Top5: 0.7608\n  ➤ Top10: 0.8397\n  ➤ Top15: 0.8828\nFold 4:\n  ➤ MAP: 0.2909\n  ➤ MRR: 0.5842\n  ➤ Top1: 0.4426\n  ➤ Top2: 0.5813\n  ➤ Top3: 0.6651\n  ➤ Top4: 0.7249\n  ➤ Top5: 0.7727\n  ➤ Top10: 0.8708\n  ➤ Top15: 0.9115\nFold 5:\n  ➤ MAP: 0.3769\n  ➤ MRR: 0.5872\n  ➤ Top1: 0.4737\n  ➤ Top2: 0.5885\n  ➤ Top3: 0.6411\n  ➤ Top4: 0.6770\n  ➤ Top5: 0.7057\n  ➤ Top10: 0.8086\n  ➤ Top15: 0.8780\nFold 6:\n  ➤ MAP: 0.3732\n  ➤ MRR: 0.5576\n  ➤ Top1: 0.4211\n  ➤ Top2: 0.5622\n  ➤ Top3: 0.6340\n  ➤ Top4: 0.6746\n  ➤ Top5: 0.7225\n  ➤ Top10: 0.8254\n  ➤ Top15: 0.8780\nFold 7:\n  ➤ MAP: 0.3834\n  ➤ MRR: 0.5864\n  ➤ Top1: 0.4495\n  ➤ Top2: 0.5745\n  ➤ Top3: 0.6562\n  ➤ Top4: 0.7139\n  ➤ Top5: 0.7620\n  ➤ Top10: 0.8822\n  ➤ Top15: 0.9303\nFold 8:\n  ➤ MAP: 0.4184\n  ➤ MRR: 0.6230\n  ➤ Top1: 0.4892\n  ➤ Top2: 0.6313\n  ➤ Top3: 0.7108\n  ➤ Top4: 0.7566\n  ➤ Top5: 0.7855\n  ➤ Top10: 0.8819\n  ➤ Top15: 0.9229\n\n📊 Kết quả trung bình trên toàn bộ k-folds:\n  ➤ MAP: 0.3734\n  ➤ MRR: 0.5921\n  ➤ Top1: 0.4597\n  ➤ Top2: 0.5904\n  ➤ Top3: 0.6660\n  ➤ Top4: 0.7174\n  ➤ Top5: 0.7562\n  ➤ Top10: 0.8576\n  ➤ Top15: 0.9050\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import pandas as pd\n\n# Dữ liệu SOTA: ImbalancedBugLoc\ndata = {\n    \"Project\": [\"AspectJ\", \"Tomcat\", \"Eclipse\", \"SWT\", \"Birt\"],\n    \"Top1_SOTA\": [52.5, 53.2, 48.1, 40.2, 28.3],\n    \"Top2_SOTA\": [68.7, 65.5, 62.1, 54.9, 39.3],\n    \"Top3_SOTA\": [77.2, 71.0, 68.8, 64.2, 45.7],\n    \"Top4_SOTA\": [81.0, 75.0, 73.0, 69.3, 51.0],\n    \"Top5_SOTA\": [83.8, 78.3, 76.7, 73.4, 53.6],\n    \"Top10_SOTA\": [89.0, 85.6, 84.7, 84.8, 63.2],\n    \"Top15_SOTA\": [91.5, 88.9, 87.8, 89.1, 69.2],\n    \"MRR_SOTA\": [0.66, 0.64, 0.60, 0.55, 0.40],\n    \"MAP_SOTA\": [0.50, 0.59, 0.54, 0.50, 0.32],\n    \"Top1_New\": [61.90, 56.75, 66.30, 60.63, 45.97],\n    \"Top2_New\": [71.13, 69.58, 78.20, 74.70, 59.04],\n    \"Top3_New\": [76.19, 77.48, 83.35, 81.28, 66.60],\n    \"Top4_New\": [80.06, 82.71, 86.71, 84.77, 71.74],\n    \"Top5_New\": [82.74, 85.79, 89.13, 87.49, 75.62],\n    \"Top10_New\": [88.39, 92.90, 94.38, 93.91, 85.76],\n    \"Top15_New\": [91.07, 94.90, 96.19, 96.07, 90.50],\n    \"MRR_New\": [0.7109, 0.6899, 0.7644, 0.7183, 0.5921],\n    \"MAP_New\": [0.5367, 0.4946, 0.5692, 0.4669, 0.3734]\n}\n\n# Tạo DataFrame từ dữ liệu\ndf = pd.DataFrame(data)\n\n# Tách thành hai bảng: SOTA và New Model\ndf_sota = df[[\"Project\", \"Top1_SOTA\", \"Top2_SOTA\", \"Top3_SOTA\", \"Top4_SOTA\", \"Top5_SOTA\", \n              \"Top10_SOTA\", \"Top15_SOTA\", \"MRR_SOTA\", \"MAP_SOTA\"]].copy()\ndf_sota[\"Model\"] = \"SOTA\"\n\ndf_new = df[[\"Project\", \"Top1_New\", \"Top2_New\", \"Top3_New\", \"Top4_New\", \"Top5_New\", \n             \"Top10_New\", \"Top15_New\", \"MRR_New\", \"MAP_New\"]].copy()\ndf_new[\"Model\"] = \"New Model\"\n\n# Đổi tên cột giống bảng trong ảnh\nrename_cols = {\n    \"Top1_SOTA\": \"1\", \"Top2_SOTA\": \"2\", \"Top3_SOTA\": \"3\", \"Top4_SOTA\": \"4\",\n    \"Top5_SOTA\": \"5\", \"Top10_SOTA\": \"10\", \"Top15_SOTA\": \"15\", \"MRR_SOTA\": \"MRR\", \"MAP_SOTA\": \"MAP\",\n    \"Top1_New\": \"1\", \"Top2_New\": \"2\", \"Top3_New\": \"3\", \"Top4_New\": \"4\",\n    \"Top5_New\": \"5\", \"Top10_New\": \"10\", \"Top15_New\": \"15\", \"MRR_New\": \"MRR\", \"MAP_New\": \"MAP\"\n}\n\ndf_sota.rename(columns=rename_cols, inplace=True)\ndf_new.rename(columns=rename_cols, inplace=True)\n\n# Gộp lại\ndf_combined = pd.concat([df_sota, df_new], axis=0)\ndf_combined = df_combined.sort_values(by=[\"Project\", \"Model\"]).reset_index(drop=True)\n\n# Đưa cột 'Model' về sau 'Project'\ncols = df_combined.columns.tolist()\ncols.insert(1, cols.pop(cols.index('Model')))\ndf_combined = df_combined[cols]\n\n# Hiển thị kết quả\ndf_combined\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:24:11.685527Z","iopub.execute_input":"2025-04-07T07:24:11.685962Z","iopub.status.idle":"2025-04-07T07:24:11.720882Z","shell.execute_reply.started":"2025-04-07T07:24:11.685926Z","shell.execute_reply":"2025-04-07T07:24:11.719901Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Project      Model      1      2      3      4      5     10     15  \\\n0  AspectJ  New Model  61.90  71.13  76.19  80.06  82.74  88.39  91.07   \n1  AspectJ       SOTA  52.50  68.70  77.20  81.00  83.80  89.00  91.50   \n2     Birt  New Model  45.97  59.04  66.60  71.74  75.62  85.76  90.50   \n3     Birt       SOTA  28.30  39.30  45.70  51.00  53.60  63.20  69.20   \n4  Eclipse  New Model  66.30  78.20  83.35  86.71  89.13  94.38  96.19   \n5  Eclipse       SOTA  48.10  62.10  68.80  73.00  76.70  84.70  87.80   \n6      SWT  New Model  60.63  74.70  81.28  84.77  87.49  93.91  96.07   \n7      SWT       SOTA  40.20  54.90  64.20  69.30  73.40  84.80  89.10   \n8   Tomcat  New Model  56.75  69.58  77.48  82.71  85.79  92.90  94.90   \n9   Tomcat       SOTA  53.20  65.50  71.00  75.00  78.30  85.60  88.90   \n\n      MRR     MAP  \n0  0.7109  0.5367  \n1  0.6600  0.5000  \n2  0.5921  0.3734  \n3  0.4000  0.3200  \n4  0.7644  0.5692  \n5  0.6000  0.5400  \n6  0.7183  0.4669  \n7  0.5500  0.5000  \n8  0.6899  0.4946  \n9  0.6400  0.5900  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Project</th>\n      <th>Model</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>10</th>\n      <th>15</th>\n      <th>MRR</th>\n      <th>MAP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AspectJ</td>\n      <td>New Model</td>\n      <td>61.90</td>\n      <td>71.13</td>\n      <td>76.19</td>\n      <td>80.06</td>\n      <td>82.74</td>\n      <td>88.39</td>\n      <td>91.07</td>\n      <td>0.7109</td>\n      <td>0.5367</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AspectJ</td>\n      <td>SOTA</td>\n      <td>52.50</td>\n      <td>68.70</td>\n      <td>77.20</td>\n      <td>81.00</td>\n      <td>83.80</td>\n      <td>89.00</td>\n      <td>91.50</td>\n      <td>0.6600</td>\n      <td>0.5000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Birt</td>\n      <td>New Model</td>\n      <td>45.97</td>\n      <td>59.04</td>\n      <td>66.60</td>\n      <td>71.74</td>\n      <td>75.62</td>\n      <td>85.76</td>\n      <td>90.50</td>\n      <td>0.5921</td>\n      <td>0.3734</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Birt</td>\n      <td>SOTA</td>\n      <td>28.30</td>\n      <td>39.30</td>\n      <td>45.70</td>\n      <td>51.00</td>\n      <td>53.60</td>\n      <td>63.20</td>\n      <td>69.20</td>\n      <td>0.4000</td>\n      <td>0.3200</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Eclipse</td>\n      <td>New Model</td>\n      <td>66.30</td>\n      <td>78.20</td>\n      <td>83.35</td>\n      <td>86.71</td>\n      <td>89.13</td>\n      <td>94.38</td>\n      <td>96.19</td>\n      <td>0.7644</td>\n      <td>0.5692</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Eclipse</td>\n      <td>SOTA</td>\n      <td>48.10</td>\n      <td>62.10</td>\n      <td>68.80</td>\n      <td>73.00</td>\n      <td>76.70</td>\n      <td>84.70</td>\n      <td>87.80</td>\n      <td>0.6000</td>\n      <td>0.5400</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>SWT</td>\n      <td>New Model</td>\n      <td>60.63</td>\n      <td>74.70</td>\n      <td>81.28</td>\n      <td>84.77</td>\n      <td>87.49</td>\n      <td>93.91</td>\n      <td>96.07</td>\n      <td>0.7183</td>\n      <td>0.4669</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>SWT</td>\n      <td>SOTA</td>\n      <td>40.20</td>\n      <td>54.90</td>\n      <td>64.20</td>\n      <td>69.30</td>\n      <td>73.40</td>\n      <td>84.80</td>\n      <td>89.10</td>\n      <td>0.5500</td>\n      <td>0.5000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Tomcat</td>\n      <td>New Model</td>\n      <td>56.75</td>\n      <td>69.58</td>\n      <td>77.48</td>\n      <td>82.71</td>\n      <td>85.79</td>\n      <td>92.90</td>\n      <td>94.90</td>\n      <td>0.6899</td>\n      <td>0.4946</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Tomcat</td>\n      <td>SOTA</td>\n      <td>53.20</td>\n      <td>65.50</td>\n      <td>71.00</td>\n      <td>75.00</td>\n      <td>78.30</td>\n      <td>85.60</td>\n      <td>88.90</td>\n      <td>0.6400</td>\n      <td>0.5900</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Xuất ra file Excel\ndf_combined.to_excel(\"model_comparison.xlsx\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:25:26.687202Z","iopub.execute_input":"2025-04-07T07:25:26.687562Z","iopub.status.idle":"2025-04-07T07:25:27.235033Z","shell.execute_reply.started":"2025-04-07T07:25:26.687537Z","shell.execute_reply":"2025-04-07T07:25:27.233860Z"}},"outputs":[],"execution_count":4}]}