{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":11265211,"sourceType":"datasetVersion","datasetId":7041476},{"sourceId":11280721,"sourceType":"datasetVersion","datasetId":7052679}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Load d·ªØ li·ªáu ƒë√£ x·ª≠ l√≠","metadata":{"id":"-eUizc5QmpX4"}},{"cell_type":"code","source":"!pip install nltk\n","metadata":{"id":"9qfVWdges0io","outputId":"e6051f73-0867-4eb4-b5c0-984722d836d3","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:24.558760Z","iopub.execute_input":"2025-04-07T01:15:24.559136Z","iopub.status.idle":"2025-04-07T01:15:28.911469Z","shell.execute_reply.started":"2025-04-07T01:15:24.559098Z","shell.execute_reply":"2025-04-07T01:15:28.910416Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import nltk\nnltk.download('averaged_perceptron_tagger_eng')","metadata":{"id":"8SUNdusXgQ-3","outputId":"937573b4-53fb-4bb5-9c39-84eb53bc1890","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:28.912930Z","iopub.execute_input":"2025-04-07T01:15:28.913168Z","iopub.status.idle":"2025-04-07T01:15:29.745069Z","shell.execute_reply.started":"2025-04-07T01:15:28.913147Z","shell.execute_reply":"2025-04-07T01:15:29.744418Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# English stop words\nstop_words = set(\n    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n     'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',\n     'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',\n     'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n     'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n     'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n     'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n     'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n     'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n     'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n     'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n     'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n     's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o',\n     're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven',\n     'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won',\n     'wouldn', 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'n', 'p', 'q', 'u', 'v',\n     'w', 'x', 'z', 'us'])\n\n# Java language keywords\njava_keywords = set(\n    ['abstract', 'assert', 'boolean', 'break', 'byte', 'case',\n     'catch', 'char', 'class', 'const', 'continue', 'default', 'do', 'double',\n     'else', 'enum', 'extends', 'false', 'final', 'finally', 'float', 'for', 'goto',\n     'if', 'implements', 'import', 'instanceof', 'int', 'interface', 'long',\n     'native', 'new', 'null', 'package', 'private', 'protected', 'public', 'return',\n     'short', 'static', 'strictfp', 'super', 'switch', 'synchronized', 'this',\n     'throw', 'throws', 'transient', 'true', 'try', 'void', 'volatile', 'while'])\n\nfrom collections import namedtuple\nfrom pathlib import Path\n\n# Dataset root directory (ƒëi·ªÅu ch·ªânh ƒë∆∞·ªùng d·∫´n n·∫øu c·∫ßn)\n_DATASET_ROOT = Path('/content/drive/MyDrive/Colab Notebooks/NLP/Task bug localization/')\n\nDataset = namedtuple('Dataset', ['name', 'src', 'bug_repo', 'repo_url', 'features'])\n\n# C√°c dataset ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\naspectj = Dataset(\n    'aspectj',\n    _DATASET_ROOT / 'source files/org.aspectj',\n    _DATASET_ROOT / 'bug reports/AspectJ.txt',\n    \"https://github.com/eclipse/org.aspectj/tree/bug433351.git\",\n    _DATASET_ROOT / 'bug reports/AspectJ.xlsx'\n)\n\neclipse = Dataset(\n    'eclipse',\n    _DATASET_ROOT / 'source files/eclipse.platform.ui-johna-402445',\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.txt',\n    \"https://github.com/eclipse/eclipse.platform.ui.git\",\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.xlsx'\n)\n\nswt = Dataset(\n    'swt',\n    _DATASET_ROOT / 'source files/eclipse.platform.swt-xulrunner-31',\n    _DATASET_ROOT / 'bug reports/SWT.txt',\n    \"https://github.com/eclipse/eclipse.platform.swt.git\",\n    _DATASET_ROOT / 'bug reports/SWT.xlsx'\n)\n\ntomcat = Dataset(\n    'tomcat',\n    _DATASET_ROOT / 'source files/tomcat-7.0.51',\n    _DATASET_ROOT / 'bug reports/Tomcat.txt',\n    \"https://github.com/apache/tomcat.git\",\n    _DATASET_ROOT / 'bug reports/Tomcat.xlsx'\n)\n\nbirt = Dataset(\n    'birt',\n    _DATASET_ROOT / 'source files/birt-20140211-1400',\n    _DATASET_ROOT / 'bug reports/Birt.txt',\n    \"https://github.com/apache/birt.git\",\n    _DATASET_ROOT / 'bug reports/Birt.xlsx'\n)\n\n\n### Current dataset in use. (change this name to change the dataset)\nDATASET = tomcat\n\nclass BugReport:\n    \"\"\"Class representing each bug report\"\"\"\n    __slots__ = ['summary', 'description', 'fixed_files', 'report_time', 'pos_tagged_summary', 'pos_tagged_description','stack_traces','stack_traces_remove']\n\n    def __init__(self, summary, description, fixed_files, report_time):\n        self.summary = summary\n        self.description = description\n        self.fixed_files = fixed_files\n        self.report_time = report_time\n        self.pos_tagged_summary = None\n        self.pos_tagged_description = None\n        self.stack_traces = None\n        self.stack_traces_remove = None\n\nclass SourceFile:\n    \"\"\"Class representing each source file\"\"\"\n    __slots__ = ['all_content', 'comments', 'class_names', 'attributes', 'method_names', 'variables', 'file_name',\n                 'pos_tagged_comments', 'exact_file_name', 'package_name']\n\n    def __init__(self, all_content, comments, class_names, attributes, method_names, variables, file_name,\n                 package_name):\n        self.all_content = all_content\n        self.comments = comments\n        self.class_names = class_names\n        self.attributes = attributes\n        self.method_names = method_names\n        self.variables = variables\n        self.file_name = file_name\n        self.exact_file_name = file_name[0]\n        self.package_name = package_name\n        self.pos_tagged_comments = None\n\n\nclass Parser:\n    \"\"\"Class containing different parsers\"\"\"\n    __slots__ = ['name', 'src', 'bug_repo']\n\n    def __init__(self, pro):\n        self.name = pro.name\n        self.src = pro.src\n        self.bug_repo = pro.bug_repo\n\n    def report_parser(self):\n        reader = csv.DictReader(open(self.bug_repo, \"r\"), delimiter=\"\\t\")\n        bug_reports = OrderedDict()\n        # raw_texts = []\n        # fixed_files = []\n        for line in reader:\n            # line[\"raw_text\"] = line[\"summary\"] + ' ' + line[\"description\"]\n            line[\"report_time\"] = datetime.strptime(line[\"report_time\"], \"%Y-%m-%d %H:%M:%S\")\n            temp = line[\"files\"].strip().split(\".java\")\n            length = len(temp)\n            x = []\n            for i, f in enumerate(temp):\n                if i == (length - 1):\n                    x.append(os.path.normpath(f))\n                    continue\n                x.append(os.path.normpath(f + \".java\"))\n            line[\"files\"] = x\n            bug_reports[line[\"bug_id\"]] = BugReport(line[\"summary\"], line[\"description\"], line[\"files\"],\n                                                    line[\"report_time\"])\n        # bug_reports = tsv2dict(self.bug_repo)\n\n        return bug_reports\n\n    def src_parser(self):\n        \"\"\"Parse source code directory of a program and colect its java files\"\"\"\n\n        # Gettting the list of source files recursively from the source directory\n        src_addresses = glob.glob(str(self.src) + '/**/*.java', recursive=True)\n        print(src_addresses)\n        # Creating a java lexer instance for pygments.lex() method\n        java_lexer = JavaLexer()\n        src_files = OrderedDict()\n        # src_files = dict()\n        # Looping to parse each source file\n        for src_file in src_addresses:\n            with open(src_file, encoding='latin-1') as file:\n                src = file.read()\n\n            # Placeholder for different parts of a source file\n            comments = ''\n            class_names = []\n            attributes = []\n            method_names = []\n            variables = []\n\n            # Source parsing\n            parse_tree = None\n            try:\n                parse_tree = javalang.parse.parse(src)\n                for path, node in parse_tree.filter(javalang.tree.VariableDeclarator):\n                    if isinstance(path[-2], javalang.tree.FieldDeclaration):\n                        attributes.append(node.name)\n                    elif isinstance(path[-2], javalang.tree.VariableDeclaration):\n                        variables.append(node.name)\n            except:\n                pass\n\n            # Triming the source file\n            ind = False\n            if parse_tree:\n                if parse_tree.imports:\n                    last_imp_path = parse_tree.imports[-1].path\n                    src = src[src.index(last_imp_path) + len(last_imp_path) + 1:]\n                elif parse_tree.package:\n                    package_name = parse_tree.package.name\n                    src = src[src.index(package_name) + len(package_name) + 1:]\n                else:  # no import and no package declaration\n                    ind = True\n            # javalang can't parse the source file\n            else:\n                ind = True\n\n            # Lexically tokenize the source file\n            lexed_src = pygments.lex(src, java_lexer)\n\n            for i, token in enumerate(lexed_src):\n                if token[0] in Token.Comment:\n                    if ind and i == 0 and token[0] is Token.Comment.Multiline:\n                        src = src[src.index(token[1]) + len(token[1]):]\n                        continue\n                    comments = comments + token[1]\n                elif token[0] is Token.Name.Class:\n                    class_names.append(token[1])\n                elif token[0] is Token.Name.Function:\n                    method_names.append(token[1])\n\n            # get the package declaration if exists\n            if parse_tree and parse_tree.package:\n                package_name = parse_tree.package.name\n            else:\n                package_name = None\n\n            if self.name == 'aspectj' or 'tomcat' or 'eclipse' or 'swt':\n                src_files[os.path.relpath(src_file, start=self.src)] = SourceFile(src, comments, class_names,\n                                                                                  attributes, method_names, variables, [\n                                                                                      os.path.basename(src_file).split(\n                                                                                          '.')[0]], package_name)\n            else:\n                # If source files has package declaration\n                if package_name:\n                    src_id = (package_name + '.' + os.path.basename(src_file))\n                else:\n                    src_id = os.path.basename(src_file)\n                src_files[src_id] = SourceFile(src, comments, class_names, attributes, method_names, variables,\n                                               [os.path.basename(src_file).split('.')[0]], package_name)\n            # print(src_files)\n            # print(\"===========\")\n        return src_files\n\n\nclass ReportPreprocessing:\n    \"\"\"Class preprocess bug reports\"\"\"\n    __slots__ = ['bug_reports']\n\n    def __init__(self, bug_reports):\n        self.bug_reports = bug_reports\n\n    def extract_stack_traces(self):\n        \"\"\"Extract stack traces from bug reports\"\"\"\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            report.stack_traces = st\n\n    def extract_stack_traces_remove(self):\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            at = []\n            for x in st:\n                if (x[1] == 'Unknown Source'):\n                    temp = 'Unknown'\n                    y = x[0]+ '(' + temp\n                else:\n                    y = x[0] + '(' + x[1] + ')'\n                at.append(y)\n            report.stack_traces_remove = at\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from bug reports raw_text\"\"\"\n        for report in self.bug_reports.values():\n            # Tokenizing using word_tokeize for more accurate pos-tagging\n            sum_tok = nltk.word_tokenize(report.summary)\n            desc_tok = nltk.word_tokenize(report.description)\n            sum_pos = nltk.pos_tag(sum_tok)\n            desc_pos = nltk.pos_tag(desc_tok)\n            report.pos_tagged_summary = [token for token, pos in sum_pos if 'NN' in pos or 'VB' in pos]\n            report.pos_tagged_description = [token for token, pos in desc_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"Tokenize bug report intro tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = nltk.wordpunct_tokenize(report.summary)\n            report.description = nltk.wordpunct_tokenize(report.description)\n\n    def _split_camelcase(self, tokens):\n        # copy tokens\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camel case detection for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        \"\"\"Split camelcase indentifiers\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = self._split_camelcase(report.summary)\n            report.description = self._split_camelcase(report.description)\n            report.pos_tagged_summary = self._split_camelcase(report.pos_tagged_summary)\n            report.pos_tagged_description = self._split_camelcase(report.pos_tagged_description)\n\n    def normalize(self):\n        \"\"\"remove punctuation, numbers and lowecase conversion\"\"\"\n        # build a translate table for punctuation and number removal\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n\n        for report in self.bug_reports.values():\n            summary_punctnum_rem = [token.translate(punctnum_table) for token in report.summary]\n            desc_punctnum_rem = [token.translate(punctnum_table) for token in report.description]\n            pos_sum_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_summary]\n            pos_desc_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_description]\n            report.summary = [token.lower() for token in summary_punctnum_rem if token]\n            report.description = [token.lower() for token in desc_punctnum_rem if token]\n            report.pos_tagged_summary = [token.lower() for token in pos_sum_punctnum_rem if token]\n            report.pos_tagged_description = [token.lower() for token in pos_desc_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        \"\"\"removing stop word from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in stop_words]\n            report.description = [token for token in report.description if token not in stop_words]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in stop_words]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in stop_words]\n\n    def remove_java_keywords(self):\n        \"\"\"removing java language keywords from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in java_keywords]\n            report.description = [token for token in report.description if token not in java_keywords]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in java_keywords]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for report in self.bug_reports.values():\n            report.summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.summary], report.summary]))\n            report.description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.description], report.description]))\n            report.pos_tagged_summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_summary], report.pos_tagged_summary]))\n            report.pos_tagged_description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_description], report.pos_tagged_description]))\n\n    def preprocess(self):\n        self.extract_stack_traces()\n        self.extract_stack_traces_remove()\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_java_keywords()\n        self.stem()\n\nclass SrcPreprocessing:\n    \"\"\"class to preprocess source code\"\"\"\n    __slots__ = ['src_files']\n\n    def __init__(self, src_files):\n        self.src_files = src_files\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from comments\"\"\"\n        for src in self.src_files.values():\n            # tokenize using word_tokenize\n            comments_tok = nltk.word_tokenize(src.comments)\n            comments_pos = nltk.pos_tag(comments_tok)\n            src.pos_tagged_comments = [token for token, pos in comments_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"tokenize source code to tokens\"\"\"\n        for src in self.src_files.values():\n            src.all_content = nltk.wordpunct_tokenize(src.all_content)\n            src.comments = nltk.wordpunct_tokenize(src.comments)\n\n    def _split_camelcase(self, tokens):\n        # copy token\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camelcase defect for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        # Split camelcase indenti\n        for src in self.src_files.values():\n            src.all_content = self._split_camelcase(src.all_content)\n            src.comments = self._split_camelcase(src.comments)\n            src.class_names = self._split_camelcase(src.class_names)\n            src.attributes = self._split_camelcase(src.attributes)\n            src.method_names = self._split_camelcase(src.method_names)\n            src.variables = self._split_camelcase(src.variables)\n            src.pos_tagged_comments = self._split_camelcase(src.pos_tagged_comments)\n\n    def normalize(self):\n        \"remove punctuation, number and lowercase conversion\"\n        # build a translate table for punctuation and number\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n        for src in self.src_files.values():\n            content_punctnum_rem = [token.translate(punctnum_table) for token in src.all_content]\n            comments_punctnum_rem = [token.translate(punctnum_table) for token in src.comments]\n            classnames_punctnum_rem = [token.translate(punctnum_table) for token in src.class_names]\n            attributes_punctnum_rem = [token.translate(punctnum_table) for token in src.attributes]\n            methodnames_punctnum_rem = [token.translate(punctnum_table) for token in src.method_names]\n            variables_punctnum_rem = [token.translate(punctnum_table) for token in src.variables]\n            filename_punctnum_rem = [token.translate(punctnum_table) for token in src.file_name]\n            pos_comments_punctnum_rem = [token.translate(punctnum_table) for token in src.pos_tagged_comments]\n\n            src.all_content = [token.lower() for token in content_punctnum_rem if token]\n            src.comments = [token.lower() for token in comments_punctnum_rem if token]\n            src.class_names = [token.lower() for token in classnames_punctnum_rem if token]\n            src.attributes = [token.lower() for token in attributes_punctnum_rem if token]\n            src.method_names = [token.lower() for token in methodnames_punctnum_rem if token]\n            src.variables = [token.lower() for token in variables_punctnum_rem if token]\n            src.file_name = [token.lower() for token in filename_punctnum_rem if token]\n            src.pos_tagged_comments = [token.lower() for token in pos_comments_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in stop_words]\n            src.comments = [token for token in src.comments if token not in stop_words]\n            src.class_names = [token for token in src.class_names if token not in stop_words]\n            src.attributes = [token for token in src.attributes if token not in stop_words]\n            src.method_names = [token for token in src.method_names if token not in stop_words]\n            src.variables = [token for token in src.variables if token not in stop_words]\n            src.file_name = [token for token in src.file_name if token not in stop_words]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in stop_words]\n\n    def remove_javakeywords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in java_keywords]\n            src.comments = [token for token in src.comments if token not in java_keywords]\n            src.class_names = [token for token in src.class_names if token not in java_keywords]\n            src.attributes = [token for token in src.attributes if token not in java_keywords]\n            src.method_names = [token for token in src.method_names if token not in java_keywords]\n            src.variables = [token for token in src.variables if token not in java_keywords]\n            src.file_name = [token for token in src.file_name if token not in java_keywords]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for src in self.src_files.values():\n            src.all_content = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.all_content], src.all_content]))\n            src.comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.comments], src.comments]))\n            src.class_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.class_names], src.class_names]))\n            src.attributes = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.attributes], src.attributes]))\n            src.method_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.method_names], src.method_names]))\n            src.variables = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.variables], src.variables]))\n            src.file_name = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.file_name], src.file_name]))\n            src.pos_tagged_comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.pos_tagged_comments], src.pos_tagged_comments]))\n\n\n    def preprocess(self):\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_javakeywords()\n        self.stem()","metadata":{"id":"_22yeS4wcWpU","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:29.746574Z","iopub.execute_input":"2025-04-07T01:15:29.746790Z","iopub.status.idle":"2025-04-07T01:15:29.797152Z","shell.execute_reply.started":"2025-04-07T01:15:29.746772Z","shell.execute_reply":"2025-04-07T01:15:29.796345Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install inflection\nimport inflection\n","metadata":{"id":"ACZrz5Byh7Ur","outputId":"68b0fd14-1ac6-484a-cb84-2ab028bd5ed6","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:29.798493Z","iopub.execute_input":"2025-04-07T01:15:29.798902Z","iopub.status.idle":"2025-04-07T01:15:33.343444Z","shell.execute_reply.started":"2025-04-07T01:15:29.798881Z","shell.execute_reply":"2025-04-07T01:15:33.342609Z"}},"outputs":[{"name":"stdout","text":"Collecting inflection\n  Downloading inflection-0.5.1-py2.py3-none-any.whl.metadata (1.7 kB)\nDownloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\nInstalling collected packages: inflection\nSuccessfully installed inflection-0.5.1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')\nimport pickle\nfrom google.colab import drive\nimport csv\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport re\nimport string\nfrom nltk.stem.porter import PorterStemmer","metadata":{"id":"P9emxprnStnP","outputId":"51497efe-74ae-4abd-d5a0-485188dab6f7","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:33.344334Z","iopub.execute_input":"2025-04-07T01:15:33.344558Z","iopub.status.idle":"2025-04-07T01:15:33.615085Z","shell.execute_reply.started":"2025-04-07T01:15:33.344538Z","shell.execute_reply":"2025-04-07T01:15:33.614481Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Load d·ªØ li·ªáu","metadata":{"id":"SC6LbKkWy8kR"}},{"cell_type":"code","source":"import pickle\n\n# ƒê∆∞·ªùng d·∫´n ƒë·∫øn c√°c file pickle\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_src_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_src_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_src_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_src_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_src_processed.pkl'\n}\n\n# Load t·ª´ng file v√† l∆∞u v√†o c√°c bi·∫øn t∆∞∆°ng ·ª©ng\ndatasets = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        datasets[name] = pickle.load(f)\n\n# Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c load v√†o c√°c bi·∫øn\nfor name, data in datasets.items():\n    print(f\"Data for {name}:\")\n","metadata":{"id":"O3TGVN1KzAXg","outputId":"63f347a7-1b8d-41fe-98e5-105ca43233e2","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:33.615738Z","iopub.execute_input":"2025-04-07T01:15:33.615938Z","iopub.status.idle":"2025-04-07T01:15:46.799515Z","shell.execute_reply.started":"2025-04-07T01:15:33.615921Z","shell.execute_reply":"2025-04-07T01:15:46.798628Z"}},"outputs":[{"name":"stdout","text":"Data for aspectj:\nData for eclipse:\nData for swt:\nData for tomcat:\nData for birt:\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"eclipse_src = datasets['eclipse']\nbirt_src = datasets['birt']\nswt_src = datasets['swt']\ntomcat_src = datasets['tomcat']\naspectj_src = datasets['aspectj']","metadata":{"id":"b91231aHzUu_","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:46.800451Z","iopub.execute_input":"2025-04-07T01:15:46.800762Z","iopub.status.idle":"2025-04-07T01:15:46.804312Z","shell.execute_reply.started":"2025-04-07T01:15:46.800738Z","shell.execute_reply":"2025-04-07T01:15:46.803582Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Load d·ªØ li·ªáu t·ª´ c√°c file pickle ƒë√£ l∆∞u\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_reports_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_reports_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_reports_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_reports_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_reports_processed.pkl'\n}\n\n# Load t·ª´ng dataset v√† l∆∞u v√†o c√°c bi·∫øn\nall_processed_reports = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        all_processed_reports[name] = pickle.load(f)\n\n# Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ load v√†o\nfor dataset, reports in all_processed_reports.items():\n    print(f\"Processed reports for {dataset}:\")","metadata":{"id":"kXiYZuNgzv0M","outputId":"053bc2e9-ef09-4aea-9431-acfda03d3300","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:46.806775Z","iopub.execute_input":"2025-04-07T01:15:46.806988Z","iopub.status.idle":"2025-04-07T01:15:49.793179Z","shell.execute_reply.started":"2025-04-07T01:15:46.806961Z","shell.execute_reply":"2025-04-07T01:15:49.792480Z"}},"outputs":[{"name":"stdout","text":"Processed reports for aspectj:\nProcessed reports for eclipse:\nProcessed reports for swt:\nProcessed reports for tomcat:\nProcessed reports for birt:\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"eclipse_reports = all_processed_reports['eclipse']\nbirt_reports = all_processed_reports['birt']\nswt_reports = all_processed_reports['swt']\ntomcat_reports = all_processed_reports['tomcat']\naspectj_reports = all_processed_reports['aspectj']","metadata":{"id":"iAUv0Bnv0FcI","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:49.794543Z","iopub.execute_input":"2025-04-07T01:15:49.794864Z","iopub.status.idle":"2025-04-07T01:15:49.798493Z","shell.execute_reply.started":"2025-04-07T01:15:49.794842Z","shell.execute_reply":"2025-04-07T01:15:49.797801Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## 2. X·ª≠ l√≠ data, g√°n nh√£n\n- S·∫Øp x·∫øp bug report theo th·ªùi gian (report_time)\n- Chia th√†nh 10 folds\n- T·∫°o training/test dataset theo ki·ªÉu fold i ‚Üí fold i+1\n- G√°n nh√£n cho t·ª´ng c·∫∑p (bug report, source file)","metadata":{"id":"-hwWTIRJ9PXd"}},{"cell_type":"code","source":"# B1: L·∫•y danh s√°ch (bug_id, bug_report), sau ƒë√≥ s·∫Øp x·∫øp theo report_time\nsorted_bug_reports = sorted(birt_reports.items(), key=lambda x: x[1].report_time)\ndata_src = birt_src","metadata":{"id":"HrKZiEgO9X16","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:49.799198Z","iopub.execute_input":"2025-04-07T01:15:49.799434Z","iopub.status.idle":"2025-04-07T01:15:49.815008Z","shell.execute_reply.started":"2025-04-07T01:15:49.799415Z","shell.execute_reply":"2025-04-07T01:15:49.814331Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def split_into_folds(sorted_reports, num_folds=10):\n    fold_size = len(sorted_reports) // num_folds\n    folds = [sorted_reports[i*fold_size:(i+1)*fold_size] for i in range(num_folds)]\n\n    # N·∫øu c√≤n d∆∞, r·∫£i ƒë·ªÅu v√†o c√°c fold ƒë·∫ßu\n    remainder = sorted_reports[num_folds*fold_size:]\n    for i, extra in enumerate(remainder):\n        folds[i].append(extra)\n    return folds\n\ndata_folds = split_into_folds(sorted_bug_reports, num_folds=10)\n","metadata":{"id":"fgRpuQKE9aA2","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:49.815868Z","iopub.execute_input":"2025-04-07T01:15:49.816126Z","iopub.status.idle":"2025-04-07T01:15:49.826714Z","shell.execute_reply.started":"2025-04-07T01:15:49.816096Z","shell.execute_reply":"2025-04-07T01:15:49.825888Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"i = 0 # th·ª≠ v·ªõi fold 0 ‚Üí 1\ntrain_fold = data_folds[i]\ntest_fold = data_folds[i+1]","metadata":{"id":"0itPPQ5O9cDT","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:49.827571Z","iopub.execute_input":"2025-04-07T01:15:49.827876Z","iopub.status.idle":"2025-04-07T01:15:49.838129Z","shell.execute_reply.started":"2025-04-07T01:15:49.827845Z","shell.execute_reply":"2025-04-07T01:15:49.837258Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import random\n\ndef generate_balanced_pairs(bug_fold, source_files, num_negatives_per_positive=50):\n    data = []\n    for bug_id, bug in bug_fold:\n        # Danh s√°ch file ch·ª©a bug (poszqitive)\n        positive_paths = set(bug.fixed_files)\n        positive = [\n            (bug_id, bug, src_path, source_files[src_path], 1)\n            for src_path in positive_paths if src_path in source_files\n        ]\n\n        # Danh s√°ch file c√≤n l·∫°i ƒë·ªÉ l·∫•y negative\n        all_paths = list(source_files.keys())\n        negative_paths = list(set(all_paths) - positive_paths)\n        sampled_negatives = random.sample(negative_paths, min(num_negatives_per_positive * len(positive), len(negative_paths)))\n\n        negative = [\n            (bug_id, bug, src_path, source_files[src_path], 0)\n            for src_path in sampled_negatives if src_path in source_files\n        ]\n\n        data.extend(positive + negative)\n    return data\ntrain_pairs = generate_balanced_pairs(train_fold, data_src, num_negatives_per_positive=50)\ntest_pairs = generate_balanced_pairs(test_fold, data_src, num_negatives_per_positive=50)\n","metadata":{"id":"wjLEL9mR9fPv","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:49.839001Z","iopub.execute_input":"2025-04-07T01:15:49.839343Z","iopub.status.idle":"2025-04-07T01:15:51.118393Z","shell.execute_reply.started":"2025-04-07T01:15:49.839315Z","shell.execute_reply":"2025-04-07T01:15:51.117717Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"X·ª≠ l√≠ m·∫•t c√¢n b·∫±ng","metadata":{"id":"phVI5NPj-GMs"}},{"cell_type":"code","source":"def compute_stats(pairs):\n    total = len(pairs)\n    pos = sum(1 for _, _, _, _, label in pairs if label == 1)\n    neg = total - pos\n    ratio = pos / total if total > 0 else 0\n    return total, pos, neg, ratio\n\ntotal, pos, neg, ratio = compute_stats(train_pairs)\nprint(\"üìä Train Set:\")\nprint(f\"  ‚û§ T·ªïng c·∫∑p: {total}\")\nprint(f\"  ‚úÖ Positive (label=1): {pos}\")\nprint(f\"  ‚ùå Negative (label=0): {neg}\")\nprint(f\"  ‚öñÔ∏è T·ª∑ l·ªá positive: {ratio:.4f}\")\n\ntotal, pos, neg, ratio = compute_stats(test_pairs)\nprint(\"\\nüß™ Test Set:\")\nprint(f\"  ‚û§ T·ªïng c·∫∑p: {total}\")\nprint(f\"  ‚úÖ Positive (label=1): {pos}\")\nprint(f\"  ‚ùå Negative (label=0): {neg}\")\nprint(f\"  ‚öñÔ∏è T·ª∑ l·ªá positive: {ratio:.4f}\")\n","metadata":{"id":"G-m4H5d79uzp","outputId":"1a4a413d-189b-4b5d-ca55-6552e56ce33f","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:51.119130Z","iopub.execute_input":"2025-04-07T01:15:51.119340Z","iopub.status.idle":"2025-04-07T01:15:51.131335Z","shell.execute_reply.started":"2025-04-07T01:15:51.119322Z","shell.execute_reply":"2025-04-07T01:15:51.130681Z"}},"outputs":[{"name":"stdout","text":"üìä Train Set:\n  ‚û§ T·ªïng c·∫∑p: 21318\n  ‚úÖ Positive (label=1): 418\n  ‚ùå Negative (label=0): 20900\n  ‚öñÔ∏è T·ª∑ l·ªá positive: 0.0196\n\nüß™ Test Set:\n  ‚û§ T·ªïng c·∫∑p: 21318\n  ‚úÖ Positive (label=1): 418\n  ‚ùå Negative (label=0): 20900\n  ‚öñÔ∏è T·ª∑ l·ªá positive: 0.0196\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### H√†m 1: T·∫°o batches c√≥ bootstrapping (lu√¥n ch·ª©a √≠t nh·∫•t 1 positive sample)","metadata":{}},{"cell_type":"code","source":"import random\ndef create_bootstrapped_batches(pairs, batch_size=128):\n    # T√°ch positive v√† negative\n    positives = [p for p in pairs if p[-1] == 1]\n    negatives = [p for p in pairs if p[-1] == 0]\n\n    batches = []\n    # T√≠nh s·ªë batch c√≥ th·ªÉ t·∫°o\n    total_batches = len(pairs) // batch_size\n\n    for _ in range(total_batches):\n        # Lu√¥n ch·ªçn √≠t nh·∫•t 1 positive\n        pos_sample = random.choice(positives)\n\n        # Ch·ªçn ng·∫´u nhi√™n batch_size - 1 negative samples\n        neg_samples = random.sample(negatives, batch_size - 1)\n\n        # G·ªôp l·∫°i, shuffle ƒë·ªÉ positive kh√¥ng ƒë·ª©ng ƒë·∫ßu\n        batch = [pos_sample] + neg_samples\n        random.shuffle(batch)\n\n        batches.append(batch)\n\n    return batches\n","metadata":{"id":"VVi_cbQD-a1b","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:51.132186Z","iopub.execute_input":"2025-04-07T01:15:51.132434Z","iopub.status.idle":"2025-04-07T01:15:51.144054Z","shell.execute_reply.started":"2025-04-07T01:15:51.132416Z","shell.execute_reply":"2025-04-07T01:15:51.143192Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Focal Loss Function","metadata":{}},{"cell_type":"code","source":"\ndef focal_loss(predictions, targets, alpha=0.999, gamma=2.0, eps=1e-6):\n    \"\"\"\n    predictions: tensor (batch_size,) - output sigmoid from model\n    targets: tensor (batch_size,) - true labels (0 or 1)\n    \"\"\"\n    # Avoid log(0)\n    predictions = predictions.clamp(min=eps, max=1.0 - eps)\n\n    # Compute focal loss\n    loss = -alpha * (1 - predictions)**gamma * targets * predictions.log() \\\n           - (1 - alpha) * predictions**gamma * (1 - targets) * (1 - predictions).log()\n    return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:51.145022Z","iopub.execute_input":"2025-04-07T01:15:51.145345Z","iopub.status.idle":"2025-04-07T01:15:51.159307Z","shell.execute_reply.started":"2025-04-07T01:15:51.145311Z","shell.execute_reply":"2025-04-07T01:15:51.158353Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# 4. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# H√†m x·ª≠ l√Ω text g·ªôp l·∫°i t·ª´ bug report\ndef bug_to_text(bug):\n    summary = bug.summary['unstemmed'] if isinstance(bug.summary, dict) else bug.summary\n    desc = bug.description['unstemmed'] if isinstance(bug.description, dict) else bug.description\n    return \" \".join(summary + desc)\n\n# H√†m x·ª≠ l√Ω text t·ª´ source file\ndef src_to_text(src):\n    content = src.all_content['unstemmed'] if isinstance(src.all_content, dict) else src.all_content\n    comments = src.comments['unstemmed'] if isinstance(src.comments, dict) else src.comments\n    return \" \".join(content + comments)\n","metadata":{"id":"4en89sb6-s54","outputId":"5c14ad2a-da00-495a-ec81-38687265bcfe","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:51.160387Z","iopub.execute_input":"2025-04-07T01:15:51.160738Z","iopub.status.idle":"2025-04-07T01:15:51.173162Z","shell.execute_reply.started":"2025-04-07T01:15:51.160705Z","shell.execute_reply":"2025-04-07T01:15:51.172524Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## ƒê·∫∑c tr∆∞ng 1: T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng t·ª´ v·ª±ng (lexical similarity)\n- Ph∆∞∆°ng ph√°p: s·ª≠ d·ª•ng TF-IDF v√† cosine similarity.\n- Input: C·∫∑p d·ªØ li·ªáu (bug report, source file)\n- Output: m·∫£ng numpy ch·ª©a c√°c gi√° tr·ªã ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa bug report v√† source file cho m·ªói c·∫∑p.","metadata":{}},{"cell_type":"code","source":"def compute_lexical_similarity(pairs):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # G·ªôp c·∫£ bug + src l·∫°i ƒë·ªÉ fit chung vectorizer\n    combined = bug_texts + src_texts\n    tfidf = TfidfVectorizer()\n    tfidf_matrix = tfidf.fit_transform(combined)\n\n    # T√°ch ri√™ng l·∫°i t·ª´ng ph·∫ßn\n    bug_vecs = tfidf_matrix[:len(pairs)]\n    src_vecs = tfidf_matrix[len(pairs):]\n\n    # T√≠nh cosine cho t·ª´ng c·∫∑p (theo h√†ng t∆∞∆°ng ·ª©ng)\n    similarities = cosine_similarity(bug_vecs, src_vecs).diagonal()\n\n    return similarities\n\n# Demo: t√≠nh feature lexical similarity cho train_pairs (gi·ªõi h·∫°n 500 m·∫´u v√¨ t·ªëc ƒë·ªô)\nsampled_pairs = train_pairs[:500]\nlexical_sim = compute_lexical_similarity(sampled_pairs)\n\n# Tr·∫£ v·ªÅ d∆∞·ªõi d·∫°ng numpy array\nlexical_sim[:10]  # Tr√≠ch 10 gi√° tr·ªã ƒë·∫ßu ti√™n ƒë·ªÉ test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:51.174133Z","iopub.execute_input":"2025-04-07T01:15:51.174446Z","iopub.status.idle":"2025-04-07T01:15:52.944080Z","shell.execute_reply.started":"2025-04-07T01:15:51.174417Z","shell.execute_reply":"2025-04-07T01:15:52.943351Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"array([0.03418086, 0.05550649, 0.04123273, 0.01937713, 0.00352067,\n       0.0181865 , 0.01544532, 0.01677382, 0.05421859, 0.03415126])"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"glove_path = \"/kaggle/input/glove-embedding/glove.6B.100d.txt\"\n# Load GloVe 100d v√†o dictionary\nimport numpy as np\n\ndef load_glove_embeddings(filepath):\n    embeddings = {}\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.strip().split()\n            word = values[0]\n            vector = np.array(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n    \nglove_embeddings = load_glove_embeddings(glove_path)","metadata":{"id":"6FexoFrK_LTE","outputId":"b7e7b200-954e-4541-a27b-dc43607ae8f2","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:15:52.944887Z","iopub.execute_input":"2025-04-07T01:15:52.945194Z","iopub.status.idle":"2025-04-07T01:16:02.275954Z","shell.execute_reply.started":"2025-04-07T01:15:52.945165Z","shell.execute_reply":"2025-04-07T01:16:02.275304Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## ƒê·∫∑c tr∆∞ng 2: T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a (semantic similarity)\n- Ph∆∞∆°ng ph√°p: TF-IDF weighted average c·ªßa GloVe vectors v√† cosine similarity\n- Input:  (bug report, source file).\n- Output: M·ªôt m·∫£ng numpy ch·ª©a c√°c gi√° tr·ªã ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa bug report v√† source file cho m·ªói c·∫∑p, d·ª±a tr√™n GloVe vectors v√† tr·ªçng s·ªë TF-IDF.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef compute_semantic_similarity(pairs, glove_dict, dim=100):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # D√πng TF-IDF ƒë·ªÉ l·∫•y tr·ªçng s·ªë t·ª´\n    tfidf = TfidfVectorizer()\n    tfidf.fit(bug_texts + src_texts)\n    vocab = tfidf.vocabulary_\n    idf_weights = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n\n    def embed_text(text):\n        tokens = text.split()\n        vecs = []\n        weights = []\n        for token in tokens:\n            if token in glove_dict and token in vocab:\n                vecs.append(glove_dict[token])\n                weights.append(idf_weights[token])\n        if not vecs:\n            return np.zeros(dim)\n        vecs = np.array(vecs)\n        weights = np.array(weights).reshape(-1, 1)\n        weighted_vecs = vecs * weights\n        return weighted_vecs.sum(axis=0) / weights.sum()\n\n    # T√≠nh vector trung b√¨nh cho bug v√† src\n    bug_vecs = [embed_text(text) for text in bug_texts]\n    src_vecs = [embed_text(text) for text in src_texts]\n\n    # T√≠nh cosine similarity gi·ªØa t·ª´ng c·∫∑p\n    similarities = [cosine_similarity([b], [s])[0][0] for b, s in zip(bug_vecs, src_vecs)]\n\n    return np.array(similarities)\nsampled_pairs = train_pairs[:500]\n# Demo tr√™n 500 c·∫∑p nh∆∞ lexical\nsemantic_sim = compute_semantic_similarity(sampled_pairs, glove_embeddings)\nsemantic_sim[:10]\n","metadata":{"id":"BmKfMQ9oAFSQ","outputId":"e0bf60b5-8bc7-401f-c8ee-5e8ac04ebe01","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:16:02.276755Z","iopub.execute_input":"2025-04-07T01:16:02.276968Z","iopub.status.idle":"2025-04-07T01:16:03.366062Z","shell.execute_reply.started":"2025-04-07T01:16:02.276949Z","shell.execute_reply":"2025-04-07T01:16:03.365351Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"array([0.67417744, 0.57356281, 0.54573841, 0.64720149, 0.40659827,\n       0.52761221, 0.5508162 , 0.53280833, 0.65285643, 0.59984962])"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"### ƒê·∫∑c tr∆∞ng 3: Similar Bug Report Score ","metadata":{}},{"cell_type":"markdown","source":"‚Üí Ki·ªÉm tra xem bug report n√†y c√≥ gi·ªëng¬†**nh·ªØng bug report c≈© t·ª´ng s·ª≠a c√πng file ƒë√≥**¬†kh√¥ng?\n\n- `build_bug_fix_history(pairs)` ‚Üí XD l·ªãch s·ª≠ ch·ªânh s·ª≠a theo t·ª´ng file\n- `compute_similar_bug_score(pairs, history)`\n    - Input: pairs, history\n    - So s√°nh bug hi·ªán t·∫°i v√† bug c≈©:\n    \n    cosine_similarity(TfidfVectorizer().fit_transform([bug_now, bug_old]))[0, 1]\n    \n    - L·∫•y gi√° tr·ªã t∆∞∆°ng ƒë·ªìng cao nh·∫•t v·ª´a t√¨m ƒë∆∞·ª£c","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\n\n# B·ªï tr·ª£: t·∫°o map file_path -> list of (bug_id, report_time, bug_text)\ndef build_bug_fix_history(pairs):\n    history = {}\n    for bug_id, bug, src_path, _, label in pairs:\n        if label == 1:  # ch·ªâ t√≠nh c√°c bug th·∫≠t s·ª± s·ª≠a file\n            if src_path not in history:\n                history[src_path] = []\n            history[src_path].append((bug_id, bug.report_time, bug_to_text(bug)))\n    return history\n\n# ƒê·∫∑c tr∆∞ng 3: Similar Bug Report Score\ndef compute_similar_bug_score(pairs, history):\n    scores = []\n    for bug_id, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        current_text = bug_to_text(bug)\n\n        sim_scores = []\n        if src_path in history:\n            for hist_bug_id, hist_time, hist_text in history[src_path]:\n                if hist_time < current_time:  # ch·ªâ t√≠nh bug trong qu√° kh·ª©\n                    sim = cosine_similarity(\n                        TfidfVectorizer().fit_transform([current_text, hist_text])\n                    )[0, 1]\n                    sim_scores.append(sim)\n        scores.append(max(sim_scores) if sim_scores else 0.0)\n    return np.array(scores)\n","metadata":{"id":"imb4_du_Bgjz","outputId":"8c08c8e9-7047-4e5d-de85-8b70e30cbe28","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:16:03.366866Z","iopub.execute_input":"2025-04-07T01:16:03.367110Z","iopub.status.idle":"2025-04-07T01:16:03.373032Z","shell.execute_reply.started":"2025-04-07T01:16:03.367090Z","shell.execute_reply":"2025-04-07T01:16:03.372288Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"### ƒê·∫∑c trung 4: Time Since Last Fix (ng√†y, normalize)\n- Ki·ªÉm tra v·ªõi m·ªói `(bug report, source file)` xem t·ª´ng ƒë∆∞·ª£c s·ª≠a tr∆∞·ªõc ƒë√≥ kh√¥ng v√† l·∫ßn cu·ªëi khi n√†o\n    - ƒê√£ l√¢u k s·ª≠a ‚Üí √çt l·ªói ‚Üí ƒêi·ªÉm th·∫•p\n    - M·ªõi s·ª≠a ‚Üí c√≥ th·ªÉ li√™n quan t·ªõi l·ªói ‚Üí ƒêi·ªÉm cao\n- C√°ch hƒë:\n    - T√¨m th·ªùi ƒëi·ªÉm bug current_time\n    - T√¨m history c√°c l·∫ßn s·ª≠a file trong qu√° kh·ª©\n    - T√≠nh kho·∫£ng c√°ch time gi·ªØa current v√† history g·∫ßn nh·∫•t\n    - Ch∆∞a s·ª≠a ‚Üí G√°n s·ªë delta_days=9999\n    - Chu·∫©n ho√°","metadata":{}},{"cell_type":"code","source":"# ƒê·∫∑c tr∆∞ng 4: Time Since Last Fix (ng√†y, normalize)\ndef compute_time_since_last_fix(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_times = [hist_time for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            if past_times:\n                delta_days = (current_time - max(past_times)).days\n            else:\n                delta_days = 9999  # C·ª±c l·ªõn n·∫øu ch∆∞a t·ª´ng s·ª≠a\n        else:\n            delta_days = 9999\n        scores.append(delta_days)\n\n    # Normalize v·ªÅ [0,1]\n    max_days = max(scores) if max(scores) != 0 else 1  # Tr√°nh chia cho 0\n\n    return np.array([1 - (s / max_days) for s in scores])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:16:03.373960Z","iopub.execute_input":"2025-04-07T01:16:03.374223Z","iopub.status.idle":"2025-04-07T01:16:03.388947Z","shell.execute_reply.started":"2025-04-07T01:16:03.374198Z","shell.execute_reply":"2025-04-07T01:16:03.388236Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"### ƒê·∫∑c tr∆∞ng 5: Fix Frequency (s·ªë l·∫ßn b·ªã s·ª≠a trong qu√° kh·ª©, normalize)\n\n\n- Ki·ªÉm tra xme m·ªói c·∫∑p ƒë∆∞·ª£c s·ª≠a bao nhi√™u l·∫ßn\n\n‚Üí S·ª≠a nhi·ªÅu ‚Üí File d·ªÖ d√≠nh l·ªói ‚Üí ƒêi·ªÉm cao","metadata":{}},{"cell_type":"code","source":"# ƒê·∫∑c tr∆∞ng 5: Fix Frequency (s·ªë l·∫ßn b·ªã s·ª≠a trong qu√° kh·ª©, normalize)\ndef compute_fix_frequency(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_fixes = [1 for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            freq = len(past_fixes)\n        else:\n            freq = 0\n        scores.append(freq)\n    # Normalize v·ªÅ [0,1] an to√†n\n    max_freq = max(scores)\n    max_freq = max(max_freq, 1)  # tr√°nh chia 0\n    return np.array([s / max_freq for s in scores])\n\n\n# D√πng cho 500 c·∫∑p m·∫´u\nsampled_pairs = train_pairs[:5000]\nbug_history = build_bug_fix_history(train_pairs)\n\nsimilar_bug_score = compute_similar_bug_score(sampled_pairs, bug_history)\ntime_since_last_fix = compute_time_since_last_fix(sampled_pairs, bug_history)\nfix_frequency = compute_fix_frequency(sampled_pairs, bug_history)\n\n# Tr√≠ch 5 gi√° tr·ªã ƒë·∫ßu m·ªói feature\nsimilar_bug_score[:50], time_since_last_fix[:50], fix_frequency[:50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:16:03.391793Z","iopub.execute_input":"2025-04-07T01:16:03.391984Z","iopub.status.idle":"2025-04-07T01:16:03.577940Z","shell.execute_reply.started":"2025-04-07T01:16:03.391968Z","shell.execute_reply":"2025-04-07T01:16:03.577267Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"# 4. Qu√° tr√¨nh hu·∫•n luy·ªán","metadata":{}},{"cell_type":"markdown","source":"## 4.1 T·∫°o ma tr·∫≠n train, test","metadata":{}},{"cell_type":"code","source":"# G·ªôp 5 ƒë·∫∑c tr∆∞ng l·∫°i th√†nh feature matrix X (n_samples, 5)\ndef build_feature_matrix(pairs, glove_dict, history):\n    lexical = compute_lexical_similarity(pairs)\n    semantic = compute_semantic_similarity(pairs, glove_dict)\n    similar_score = compute_similar_bug_score(pairs, history)\n    recency = compute_time_since_last_fix(pairs, history)\n    freq = compute_fix_frequency(pairs, history)\n\n    # G·ªôp l·∫°i theo chi·ªÅu d·ªçc ‚Üí ma tr·∫≠n (n_samples, 5)\n    X = np.stack([lexical, semantic, similar_score, recency, freq], axis=1)\n\n    return X\n\n# T·∫°o nh√£n y\ndef get_labels(pairs):\n    return np.array([label for *_, label in pairs])\n\n# T·∫°o d·ªØ li·ªáu train t·ª´ sampled_pairs\nX_train = build_feature_matrix(train_pairs, glove_embeddings, bug_history)\ny_train = get_labels(train_pairs)\n\n# In shape ƒë·ªÉ x√°c nh·∫≠n\nX_train.shape, y_train.shape\n","metadata":{"id":"PmEZSTiyDEB4","outputId":"6db11041-f3a2-4931-f264-9480bfd7c2e6","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:16:03.579023Z","iopub.execute_input":"2025-04-07T01:16:03.579316Z","iopub.status.idle":"2025-04-07T01:17:22.152743Z","shell.execute_reply.started":"2025-04-07T01:16:03.579288Z","shell.execute_reply":"2025-04-07T01:17:22.151977Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"((21318, 5), (21318,))"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n# Chu·∫©n b·ªã Dataset & Dataloader t·ª´ numpy\ndef create_dataloader(X, y, batch_size=128):\n    X_tensor = torch.tensor(X, dtype=torch.float32)\n    y_tensor = torch.tensor(y, dtype=torch.float32)\n    dataset = TensorDataset(X_tensor, y_tensor)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:17:22.153537Z","iopub.execute_input":"2025-04-07T01:17:22.153756Z","iopub.status.idle":"2025-04-07T01:17:25.553975Z","shell.execute_reply.started":"2025-04-07T01:17:22.153736Z","shell.execute_reply":"2025-04-07T01:17:25.553267Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"## 4.2 X√¢y d·ª±ng m√¥ h√¨nh","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\n# ƒê·ªãnh nghƒ©a m√¥ h√¨nh DNN gi·ªëng b√†i b√°o\nimport torch\nimport torch.nn as nn\n\nclass BugLocalization(nn.Module):\n    def __init__(self, input_dim=5, hidden_dim=64, output_dim=1):\n        super(BugLocalization, self).__init__()\n        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # N·∫øu x ch·ªâ c√≥ 2 chi·ªÅu (batch_size, input_dim), h√£y th√™m m·ªôt chi·ªÅu gi·∫£ ƒë·ªãnh (sequence_length=1)\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)  # Th√™m m·ªôt chi·ªÅu gi·∫£ ƒë·ªãnh (batch_size, 1, input_dim)\n\n        # x shape: (batch_size, sequence_length, input_dim)\n        rnn_out, _ = self.rnn(x)\n        # L·∫•y output c·ªßa ph·∫ßn cu·ªëi c√πng trong chu·ªói\n        final_rnn_out = rnn_out[:, -1, :]\n        out = torch.sigmoid(self.fc(final_rnn_out)).squeeze()\n        return out\n\n        \n# ƒê·ªãnh nghƒ©a focal loss\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.999, gamma=2.0, eps=1e-6):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, preds, targets):\n        preds = preds.clamp(min=self.eps, max=1. - self.eps)\n        loss = -self.alpha * (1 - preds) ** self.gamma * targets * torch.log(preds) \\\n               - (1 - self.alpha) * preds ** self.gamma * (1 - targets) * torch.log(1 - preds)\n        return loss.mean()\n\n\n# Hu·∫•n luy·ªán m√¥ h√¨nh\ndef train_model(model, dataloader, epochs=10, lr=1e-3):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = FocalLoss()\n\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch_X, batch_y in dataloader:\n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n\n# T·∫°o dataloader & model, b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán\ntrain_loader = create_dataloader(X_train, y_train, batch_size=128)\nmodel = BugLocalization()\ntrain_model(model, train_loader, epochs=5)\n","metadata":{"id":"LiD_wJxjDeQ8","outputId":"9821aee2-a66e-480b-9f69-6e4d207c7ba6","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:17:25.554751Z","iopub.execute_input":"2025-04-07T01:17:25.555074Z","iopub.status.idle":"2025-04-07T01:17:31.691067Z","shell.execute_reply.started":"2025-04-07T01:17:25.555054Z","shell.execute_reply":"2025-04-07T01:17:31.690295Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5 - Loss: 0.2112\nEpoch 2/5 - Loss: 0.1602\nEpoch 3/5 - Loss: 0.1519\nEpoch 4/5 - Loss: 0.1455\nEpoch 5/5 - Loss: 0.1404\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score, label_ranking_average_precision_score\nimport numpy as np\n\n# Chu·∫©n b·ªã t·∫≠p test\nsampled_test_pairs = test_pairs\nbug_history_test = build_bug_fix_history(test_pairs)\n\nX_test = build_feature_matrix(sampled_test_pairs, glove_embeddings, bug_history_test)\ny_test = get_labels(sampled_test_pairs)\n\n#  D·ª± ƒëo√°n x√°c su·∫•t t·ª´ m√¥ h√¨nh\nmodel.eval()\nwith torch.no_grad():\n    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n    y_pred_probs = model(X_test_tensor).numpy()\n\n# ƒê√°nh gi√° c√°c ch·ªâ s·ªë (MAP, MRR, Top-k)\ndef compute_topk_accuracy(y_true, y_scores, k=10):\n    bug_to_scores = {}\n    for (bug_id, _, src_path, _, label), score in zip(sampled_test_pairs, y_scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    correct_at_k = 0\n    total = 0\n\n    for bug_id, entries in bug_to_scores.items():\n        sorted_entries = sorted(entries, key=lambda x: x[0], reverse=True)\n        top_k = sorted_entries[:k]\n        if any(label == 1 for _, label in top_k):\n            correct_at_k += 1\n        total += 1\n\n    return correct_at_k / total if total > 0 else 0\n\n# MAP (Mean Average Precision)\nmap_score = average_precision_score(y_test, y_pred_probs)\n\n# MRR (Mean Reciprocal Rank)\ndef mean_reciprocal_rank(pairs, scores):\n    bug_to_scores = {}\n    for (bug_id, _, _, _, label), score in zip(pairs, scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    rr_sum = 0\n    count = 0\n    for bug_id, ranked in bug_to_scores.items():\n        ranked = sorted(ranked, key=lambda x: x[0], reverse=True)\n        for idx, (_, label) in enumerate(ranked):\n            if label == 1:\n                rr_sum += 1 / (idx + 1)\n                break\n        count += 1\n    return rr_sum / count if count > 0 else 0\n\nmrr_score = mean_reciprocal_rank(sampled_test_pairs, y_pred_probs)\n\n# Top-k Accuracy (k = 1, 5, 10, 15)\ntop1 = compute_topk_accuracy(y_test, y_pred_probs, k=1)\ntop5 = compute_topk_accuracy(y_test, y_pred_probs, k=5)\ntop10 = compute_topk_accuracy(y_test, y_pred_probs, k=10)\ntop15 = compute_topk_accuracy(y_test, y_pred_probs, k=15)\n\n# Tr·∫£ v·ªÅ k·∫øt qu·∫£\nmap_score, mrr_score, top1, top5, top10, top15","metadata":{"id":"hvK-tE_MD4zq","outputId":"5444ce3e-4624-4f81-eb6b-b19a543b0d49","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:17:31.691968Z","iopub.execute_input":"2025-04-07T01:17:31.692337Z","iopub.status.idle":"2025-04-07T01:18:49.994245Z","shell.execute_reply.started":"2025-04-07T01:17:31.692314Z","shell.execute_reply":"2025-04-07T01:18:49.993202Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"(0.4398018297288224,\n 0.6382144271018614,\n 0.5023923444976076,\n 0.8205741626794258,\n 0.9043062200956937,\n 0.9401913875598086)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"import random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import average_precision_score\n\n# ‚úÖ Ki·ªÉm tra v√† x√°c ƒë·ªãnh thi·∫øt b·ªã (GPU ho·∫∑c CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n# ‚úÖ Ch·∫°y ƒë√°nh gi√° to√†n b·ªô m√¥ h√¨nh theo ki·ªÉu k-fold nh∆∞ b√†i b√°o\ndef run_kfold_training_and_eval(folds, source_files, glove_dict, k=9):\n    results = {\n        \"fold\": [],\n        \"MAP\": [],\n        \"MRR\": [],\n        \"Top1\": [],\n        \"Top2\": [],\n        \"Top3\": [],\n        \"Top4\": [],\n        \"Top5\": [],\n        \"Top10\": [],\n        \"Top15\": []\n    }\n\n    for i in range(9):\n        print(f\"\\nüì¶ Fold {i} ‚û§ {i+1}\")\n        train_fold = folds[i]\n        test_fold = folds[i + 1]\n\n        train_pairs = generate_balanced_pairs(train_fold, source_files)\n        test_pairs = generate_balanced_pairs(test_fold, source_files)\n\n        # Skip if not enough positive samples\n        if sum(1 for p in train_pairs if p[-1] == 1) < 1:\n            print(\"‚ö†Ô∏è B·ªè qua do qu√° √≠t positive samples\")\n            continue\n\n        # Build history only from train set\n        bug_history = build_bug_fix_history(train_pairs)\n\n        # Feature extraction\n        X_train = build_feature_matrix(train_pairs, glove_dict, bug_history)\n        y_train = get_labels(train_pairs)\n\n        X_test = build_feature_matrix(test_pairs, glove_dict, bug_history)\n        y_test = get_labels(test_pairs)\n\n        # Create batches with bootstrapping\n        train_batches = create_bootstrapped_batches(train_pairs, batch_size=128)\n\n        # Model initialization\n        model = BugLocalization().to(device)  # Chuy·ªÉn m√¥ h√¨nh sang GPU n·∫øu c√≥\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n        # Training loop\n        model.train()\n        for epoch in range(10):  # Hu·∫•n luy·ªán trong 5 epoch\n            total_loss = 0\n            for batch in train_batches:\n                # L·∫•y ƒë·∫∑c tr∆∞ng (feature) t·ª´ batch v√† chuy·ªÉn sang GPU\n                # L·∫•y ƒë·∫∑c tr∆∞ng (feature) t·ª´ batch v√† chuy·ªÉn sang GPU\n                X_batch = torch.tensor(np.array([build_feature_matrix([x], glove_dict, bug_history) for x in batch]), dtype=torch.float32).to(device)\n                y_batch = torch.tensor([x[-1] for x in batch], dtype=torch.float32).to(device)\n\n                optimizer.zero_grad()\n                y_pred = model(X_batch)\n                loss = focal_loss(y_pred, y_batch)  # S·ª≠ d·ª•ng Focal Loss\n                loss.backward()\n                optimizer.step()\n\n                total_loss += loss.item()\n\n            print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_batches)}\")\n\n        # Predict\n        model.eval()\n        with torch.no_grad():\n            X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)  # Chuy·ªÉn d·ªØ li·ªáu v√†o GPU\n            y_pred_probs = model(X_test_tensor).cpu().numpy()  # D·ª± ƒëo√°n x√°c su·∫•t v√† chuy·ªÉn v·ªÅ CPU\n\n        # Evaluate\n        sampled_test_pairs = test_pairs\n        map_score = average_precision_score(y_test, y_pred_probs)\n        mrr_score = mean_reciprocal_rank(sampled_test_pairs, y_pred_probs)\n        top1 = compute_topk_accuracy(y_test, y_pred_probs, k=1)\n        top2 = compute_topk_accuracy(y_test, y_pred_probs, k=2)\n        top3 = compute_topk_accuracy(y_test, y_pred_probs, k=3)\n        top4 = compute_topk_accuracy(y_test, y_pred_probs, k=4)\n        top5 = compute_topk_accuracy(y_test, y_pred_probs, k=5)\n        top10 = compute_topk_accuracy(y_test, y_pred_probs, k=10)\n        top15 = compute_topk_accuracy(y_test, y_pred_probs, k=15)\n\n        print(f\"Fold {i + 1} Results:\")\n        print(f\"  ‚û§ MAP: {map_score:.4f}\")\n        print(f\"  ‚û§ MRR: {mrr_score:.4f}\")\n        print(f\"  ‚û§ Top1: {top1:.4f}\")\n        print(f\"  ‚û§ Top2: {top2:.4f}\")\n        print(f\"  ‚û§ Top3: {top3:.4f}\")\n        print(f\"  ‚û§ Top4: {top4:.4f}\")\n        print(f\"  ‚û§ Top5: {top5:.4f}\")\n        print(f\"  ‚û§ Top10: {top10:.4f}\")\n        print(f\"  ‚û§ Top15: {top15:.4f}\")\n\n        # Save results\n        results[\"fold\"].append(i)\n        results[\"MAP\"].append(map_score)\n        results[\"MRR\"].append(mrr_score)\n        results[\"Top1\"].append(top1)\n        results[\"Top2\"].append(top2)\n        results[\"Top3\"].append(top3)\n        results[\"Top4\"].append(top4)\n        results[\"Top5\"].append(top5)\n        results[\"Top10\"].append(top10)\n        results[\"Top15\"].append(top15)\n\n    return results","metadata":{"id":"0esoYB7ME-At","outputId":"cbd0b93b-371f-4886-b013-7532bccb8b96","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:18:49.995541Z","iopub.execute_input":"2025-04-07T01:18:49.995924Z","iopub.status.idle":"2025-04-07T01:18:50.010536Z","shell.execute_reply.started":"2025-04-07T01:18:49.995891Z","shell.execute_reply":"2025-04-07T01:18:50.009726Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"\n# Run K-fold training and evaluation\nfull_results = run_kfold_training_and_eval(data_folds, data_src, glove_embeddings)\n\n# Output full results\nprint(\"\\nFull Results:\")\nfor key, value in full_results.items():\n    print(f\"{key}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T01:18:50.011475Z","iopub.execute_input":"2025-04-07T01:18:50.011741Z","iopub.status.idle":"2025-04-07T05:13:27.436897Z","shell.execute_reply.started":"2025-04-07T01:18:50.011720Z","shell.execute_reply":"2025-04-07T05:13:27.436107Z"}},"outputs":[{"name":"stdout","text":"\nüì¶ Fold 0 ‚û§ 1\nEpoch 1: Loss = 0.0008753524049667709\nEpoch 2: Loss = 0.0007048352168808433\nEpoch 3: Loss = 0.0006611337410131784\nEpoch 4: Loss = 0.0006357434576346416\nEpoch 5: Loss = 0.0006205207871270355\nEpoch 6: Loss = 0.0006115372145074386\nEpoch 7: Loss = 0.0006064673770339711\nEpoch 8: Loss = 0.0006037643398452798\nEpoch 9: Loss = 0.0006023535078197874\nEpoch 10: Loss = 0.0006015806189592344\nFold 1 Results:\n  ‚û§ MAP: 0.4059\n  ‚û§ MRR: 0.5927\n  ‚û§ Top1: 0.4569\n  ‚û§ Top2: 0.5837\n  ‚û§ Top3: 0.6627\n  ‚û§ Top4: 0.7225\n  ‚û§ Top5: 0.7632\n  ‚û§ Top10: 0.8876\n  ‚û§ Top15: 0.9211\n\nüì¶ Fold 1 ‚û§ 2\nEpoch 1: Loss = 0.0008152179954382759\nEpoch 2: Loss = 0.0006687378618481895\nEpoch 3: Loss = 0.0006291639136574063\nEpoch 4: Loss = 0.0006119119052856555\nEpoch 5: Loss = 0.0006033652011481821\nEpoch 6: Loss = 0.0005990530852318169\nEpoch 7: Loss = 0.0005969365591292707\nEpoch 8: Loss = 0.0005959208187905241\nEpoch 9: Loss = 0.0005953949309832204\nEpoch 10: Loss = 0.000595058818033571\nFold 2 Results:\n  ‚û§ MAP: 0.4296\n  ‚û§ MRR: 0.6436\n  ‚û§ Top1: 0.5144\n  ‚û§ Top2: 0.6531\n  ‚û§ Top3: 0.7249\n  ‚û§ Top4: 0.7727\n  ‚û§ Top5: 0.7990\n  ‚û§ Top10: 0.8876\n  ‚û§ Top15: 0.9282\n\nüì¶ Fold 2 ‚û§ 3\nEpoch 1: Loss = 0.0008617430832096746\nEpoch 2: Loss = 0.0007077728561897123\nEpoch 3: Loss = 0.0006584210860604668\nEpoch 4: Loss = 0.0006235240115671901\nEpoch 5: Loss = 0.0006002189659202157\nEpoch 6: Loss = 0.0005855050705182534\nEpoch 7: Loss = 0.0005768838223541178\nEpoch 8: Loss = 0.0005722163721613294\nEpoch 9: Loss = 0.0005697301884899926\nEpoch 10: Loss = 0.0005682962220751243\nFold 3 Results:\n  ‚û§ MAP: 0.3458\n  ‚û§ MRR: 0.5711\n  ‚û§ Top1: 0.4378\n  ‚û§ Top2: 0.5646\n  ‚û§ Top3: 0.6340\n  ‚û§ Top4: 0.7105\n  ‚û§ Top5: 0.7344\n  ‚û§ Top10: 0.8349\n  ‚û§ Top15: 0.8923\n\nüì¶ Fold 3 ‚û§ 4\nEpoch 1: Loss = 0.0008468588327439165\nEpoch 2: Loss = 0.0007079207658885518\nEpoch 3: Loss = 0.0006624309339768153\nEpoch 4: Loss = 0.0006348632284123114\nEpoch 5: Loss = 0.0006195212819919282\nEpoch 6: Loss = 0.0006117607806654962\nEpoch 7: Loss = 0.0006080362134043643\nEpoch 8: Loss = 0.0006061618278400568\nEpoch 9: Loss = 0.0006050350067713466\nEpoch 10: Loss = 0.0006042066141009241\nFold 4 Results:\n  ‚û§ MAP: 0.3367\n  ‚û§ MRR: 0.5827\n  ‚û§ Top1: 0.4522\n  ‚û§ Top2: 0.5742\n  ‚û§ Top3: 0.6651\n  ‚û§ Top4: 0.7033\n  ‚û§ Top5: 0.7608\n  ‚û§ Top10: 0.8397\n  ‚û§ Top15: 0.8828\n\nüì¶ Fold 4 ‚û§ 5\nEpoch 1: Loss = 0.000836548163325537\nEpoch 2: Loss = 0.000715898312186452\nEpoch 3: Loss = 0.0006796790676819811\nEpoch 4: Loss = 0.0006609673040963888\nEpoch 5: Loss = 0.0006498643959779853\nEpoch 6: Loss = 0.0006434353181029143\nEpoch 7: Loss = 0.0006400011842775843\nEpoch 8: Loss = 0.0006382745486916974\nEpoch 9: Loss = 0.0006373931668875515\nEpoch 10: Loss = 0.0006369002829993099\nFold 5 Results:\n  ‚û§ MAP: 0.2909\n  ‚û§ MRR: 0.5842\n  ‚û§ Top1: 0.4426\n  ‚û§ Top2: 0.5813\n  ‚û§ Top3: 0.6651\n  ‚û§ Top4: 0.7249\n  ‚û§ Top5: 0.7727\n  ‚û§ Top10: 0.8708\n  ‚û§ Top15: 0.9115\n\nüì¶ Fold 5 ‚û§ 6\nEpoch 1: Loss = 0.0008848024639497366\nEpoch 2: Loss = 0.0007461346044199519\nEpoch 3: Loss = 0.0007038661611900406\nEpoch 4: Loss = 0.0006769285461913053\nEpoch 5: Loss = 0.0006611833318363853\nEpoch 6: Loss = 0.0006526906419536997\nEpoch 7: Loss = 0.0006485523742620822\nEpoch 8: Loss = 0.0006466455096996064\nEpoch 9: Loss = 0.0006457042797252603\nEpoch 10: Loss = 0.0006451486330759902\nFold 6 Results:\n  ‚û§ MAP: 0.3769\n  ‚û§ MRR: 0.5872\n  ‚û§ Top1: 0.4737\n  ‚û§ Top2: 0.5885\n  ‚û§ Top3: 0.6411\n  ‚û§ Top4: 0.6770\n  ‚û§ Top5: 0.7057\n  ‚û§ Top10: 0.8086\n  ‚û§ Top15: 0.8780\n\nüì¶ Fold 6 ‚û§ 7\nEpoch 1: Loss = 0.0008878822677561183\nEpoch 2: Loss = 0.0007270471927855747\nEpoch 3: Loss = 0.0006954796997625493\nEpoch 4: Loss = 0.0006808229784848707\nEpoch 5: Loss = 0.0006731949253596859\nEpoch 6: Loss = 0.000668823847362031\nEpoch 7: Loss = 0.0006662445268748014\nEpoch 8: Loss = 0.0006647259589559567\nEpoch 9: Loss = 0.0006638137253382838\nEpoch 10: Loss = 0.0006632259927678808\nFold 7 Results:\n  ‚û§ MAP: 0.3732\n  ‚û§ MRR: 0.5576\n  ‚û§ Top1: 0.4211\n  ‚û§ Top2: 0.5622\n  ‚û§ Top3: 0.6340\n  ‚û§ Top4: 0.6746\n  ‚û§ Top5: 0.7225\n  ‚û§ Top10: 0.8254\n  ‚û§ Top15: 0.8780\n\nüì¶ Fold 7 ‚û§ 8\nEpoch 1: Loss = 0.0008490718957261149\nEpoch 2: Loss = 0.0007361957455525197\nEpoch 3: Loss = 0.000700446056112288\nEpoch 4: Loss = 0.0006771210785492335\nEpoch 5: Loss = 0.0006634620006381344\nEpoch 6: Loss = 0.0006562013593530394\nEpoch 7: Loss = 0.000652643592280605\nEpoch 8: Loss = 0.0006509565400080306\nEpoch 9: Loss = 0.0006500964594025906\nEpoch 10: Loss = 0.000649581050727494\nFold 8 Results:\n  ‚û§ MAP: 0.3834\n  ‚û§ MRR: 0.5864\n  ‚û§ Top1: 0.4495\n  ‚û§ Top2: 0.5745\n  ‚û§ Top3: 0.6562\n  ‚û§ Top4: 0.7139\n  ‚û§ Top5: 0.7620\n  ‚û§ Top10: 0.8822\n  ‚û§ Top15: 0.9303\n\nüì¶ Fold 8 ‚û§ 9\nEpoch 1: Loss = 0.0008200137632560323\nEpoch 2: Loss = 0.0007118132145313376\nEpoch 3: Loss = 0.0006770762210979945\nEpoch 4: Loss = 0.0006571697229942815\nEpoch 5: Loss = 0.0006443904901530142\nEpoch 6: Loss = 0.0006363614460552167\nEpoch 7: Loss = 0.0006316511503731211\nEpoch 8: Loss = 0.0006289575120194279\nEpoch 9: Loss = 0.0006273378208844048\nEpoch 10: Loss = 0.0006262673528963757\nFold 9 Results:\n  ‚û§ MAP: 0.4184\n  ‚û§ MRR: 0.6230\n  ‚û§ Top1: 0.4892\n  ‚û§ Top2: 0.6313\n  ‚û§ Top3: 0.7108\n  ‚û§ Top4: 0.7566\n  ‚û§ Top5: 0.7855\n  ‚û§ Top10: 0.8819\n  ‚û§ Top15: 0.9229\n\nFull Results:\nfold: [0, 1, 2, 3, 4, 5, 6, 7, 8]\nMAP: [0.40594291518421294, 0.42964434428945575, 0.3457719387079057, 0.3366605267318004, 0.29093389926052166, 0.376856841997509, 0.37322238588413226, 0.3833729835838348, 0.41838771044511014]\nMRR: [0.5926757578231548, 0.6435867063768955, 0.5710986287648228, 0.5826895060385546, 0.5842187223254296, 0.5871674619121703, 0.5576263238205738, 0.586425335305539, 0.6229862391905384]\nTop1: [0.4569377990430622, 0.5143540669856459, 0.43779904306220097, 0.45215311004784686, 0.44258373205741625, 0.47368421052631576, 0.42105263157894735, 0.4495192307692308, 0.4891566265060241]\nTop2: [0.583732057416268, 0.65311004784689, 0.5645933014354066, 0.5741626794258373, 0.5813397129186603, 0.5885167464114832, 0.562200956937799, 0.5745192307692307, 0.6313253012048192]\nTop3: [0.6626794258373205, 0.7248803827751196, 0.6339712918660287, 0.6650717703349283, 0.6650717703349283, 0.6411483253588517, 0.6339712918660287, 0.65625, 0.7108433734939759]\nTop4: [0.722488038277512, 0.7727272727272727, 0.7105263157894737, 0.7033492822966507, 0.7248803827751196, 0.6770334928229665, 0.6746411483253588, 0.7139423076923077, 0.7566265060240964]\nTop5: [0.7631578947368421, 0.7990430622009569, 0.7344497607655502, 0.7607655502392344, 0.7727272727272727, 0.7057416267942583, 0.722488038277512, 0.7620192307692307, 0.7855421686746988]\nTop10: [0.8875598086124402, 0.8875598086124402, 0.8349282296650717, 0.8397129186602871, 0.8708133971291866, 0.8086124401913876, 0.8253588516746412, 0.8822115384615384, 0.8819277108433735]\nTop15: [0.9210526315789473, 0.9282296650717703, 0.8923444976076556, 0.8827751196172249, 0.9114832535885168, 0.8779904306220095, 0.8779904306220095, 0.9302884615384616, 0.9228915662650602]\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n# In k·∫øt qu·∫£ t·ªïng h·ª£p sau khi ch·∫°y t·∫•t c·∫£ folds\nprint(\"\\nüìä K·∫øt qu·∫£ t·ªïng h·ª£p:\")\nfor i in range(len(full_results[\"fold\"])):\n    print(f\"Fold {full_results['fold'][i]}:\")\n    print(f\"  ‚û§ MAP: {full_results['MAP'][i]:.4f}\")\n    print(f\"  ‚û§ MRR: {full_results['MRR'][i]:.4f}\")\n    print(f\"  ‚û§ Top1: {full_results['Top1'][i]:.4f}\")\n    print(f\"  ‚û§ Top2: {full_results['Top2'][i]:.4f}\")\n    print(f\"  ‚û§ Top3: {full_results['Top3'][i]:.4f}\")\n    print(f\"  ‚û§ Top4: {full_results['Top4'][i]:.4f}\")\n    print(f\"  ‚û§ Top5: {full_results['Top5'][i]:.4f}\")\n    print(f\"  ‚û§ Top10: {full_results['Top10'][i]:.4f}\")\n    print(f\"  ‚û§ Top15: {full_results['Top15'][i]:.4f}\")\n\n# T√≠nh trung b√¨nh cho t·∫•t c·∫£ c√°c ch·ªâ s·ªë\nmean_map = np.mean(full_results[\"MAP\"])\nmean_mrr = np.mean(full_results[\"MRR\"])\nmean_top1 = np.mean(full_results[\"Top1\"])\nmean_top2 = np.mean(full_results[\"Top2\"])\nmean_top3 = np.mean(full_results[\"Top3\"])\nmean_top4 = np.mean(full_results[\"Top4\"])\nmean_top5 = np.mean(full_results[\"Top5\"])\nmean_top10 = np.mean(full_results[\"Top10\"])\nmean_top15 = np.mean(full_results[\"Top15\"])\n\n# In k·∫øt qu·∫£ trung b√¨nh\nprint(\"\\nüìä K·∫øt qu·∫£ trung b√¨nh tr√™n to√†n b·ªô k-folds:\")\nprint(f\"  ‚û§ MAP: {mean_map:.4f}\")\nprint(f\"  ‚û§ MRR: {mean_mrr:.4f}\")\nprint(f\"  ‚û§ Top1: {mean_top1:.4f}\")\nprint(f\"  ‚û§ Top2: {mean_top2:.4f}\")\nprint(f\"  ‚û§ Top3: {mean_top3:.4f}\")\nprint(f\"  ‚û§ Top4: {mean_top4:.4f}\")\nprint(f\"  ‚û§ Top5: {mean_top5:.4f}\")\nprint(f\"  ‚û§ Top10: {mean_top10:.4f}\")\nprint(f\"  ‚û§ Top15: {mean_top15:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:13:27.437892Z","iopub.execute_input":"2025-04-07T05:13:27.438139Z","iopub.status.idle":"2025-04-07T05:13:27.458909Z","shell.execute_reply.started":"2025-04-07T05:13:27.438118Z","shell.execute_reply":"2025-04-07T05:13:27.458169Z"}},"outputs":[{"name":"stdout","text":"\nüìä K·∫øt qu·∫£ t·ªïng h·ª£p:\nFold 0:\n  ‚û§ MAP: 0.4059\n  ‚û§ MRR: 0.5927\n  ‚û§ Top1: 0.4569\n  ‚û§ Top2: 0.5837\n  ‚û§ Top3: 0.6627\n  ‚û§ Top4: 0.7225\n  ‚û§ Top5: 0.7632\n  ‚û§ Top10: 0.8876\n  ‚û§ Top15: 0.9211\nFold 1:\n  ‚û§ MAP: 0.4296\n  ‚û§ MRR: 0.6436\n  ‚û§ Top1: 0.5144\n  ‚û§ Top2: 0.6531\n  ‚û§ Top3: 0.7249\n  ‚û§ Top4: 0.7727\n  ‚û§ Top5: 0.7990\n  ‚û§ Top10: 0.8876\n  ‚û§ Top15: 0.9282\nFold 2:\n  ‚û§ MAP: 0.3458\n  ‚û§ MRR: 0.5711\n  ‚û§ Top1: 0.4378\n  ‚û§ Top2: 0.5646\n  ‚û§ Top3: 0.6340\n  ‚û§ Top4: 0.7105\n  ‚û§ Top5: 0.7344\n  ‚û§ Top10: 0.8349\n  ‚û§ Top15: 0.8923\nFold 3:\n  ‚û§ MAP: 0.3367\n  ‚û§ MRR: 0.5827\n  ‚û§ Top1: 0.4522\n  ‚û§ Top2: 0.5742\n  ‚û§ Top3: 0.6651\n  ‚û§ Top4: 0.7033\n  ‚û§ Top5: 0.7608\n  ‚û§ Top10: 0.8397\n  ‚û§ Top15: 0.8828\nFold 4:\n  ‚û§ MAP: 0.2909\n  ‚û§ MRR: 0.5842\n  ‚û§ Top1: 0.4426\n  ‚û§ Top2: 0.5813\n  ‚û§ Top3: 0.6651\n  ‚û§ Top4: 0.7249\n  ‚û§ Top5: 0.7727\n  ‚û§ Top10: 0.8708\n  ‚û§ Top15: 0.9115\nFold 5:\n  ‚û§ MAP: 0.3769\n  ‚û§ MRR: 0.5872\n  ‚û§ Top1: 0.4737\n  ‚û§ Top2: 0.5885\n  ‚û§ Top3: 0.6411\n  ‚û§ Top4: 0.6770\n  ‚û§ Top5: 0.7057\n  ‚û§ Top10: 0.8086\n  ‚û§ Top15: 0.8780\nFold 6:\n  ‚û§ MAP: 0.3732\n  ‚û§ MRR: 0.5576\n  ‚û§ Top1: 0.4211\n  ‚û§ Top2: 0.5622\n  ‚û§ Top3: 0.6340\n  ‚û§ Top4: 0.6746\n  ‚û§ Top5: 0.7225\n  ‚û§ Top10: 0.8254\n  ‚û§ Top15: 0.8780\nFold 7:\n  ‚û§ MAP: 0.3834\n  ‚û§ MRR: 0.5864\n  ‚û§ Top1: 0.4495\n  ‚û§ Top2: 0.5745\n  ‚û§ Top3: 0.6562\n  ‚û§ Top4: 0.7139\n  ‚û§ Top5: 0.7620\n  ‚û§ Top10: 0.8822\n  ‚û§ Top15: 0.9303\nFold 8:\n  ‚û§ MAP: 0.4184\n  ‚û§ MRR: 0.6230\n  ‚û§ Top1: 0.4892\n  ‚û§ Top2: 0.6313\n  ‚û§ Top3: 0.7108\n  ‚û§ Top4: 0.7566\n  ‚û§ Top5: 0.7855\n  ‚û§ Top10: 0.8819\n  ‚û§ Top15: 0.9229\n\nüìä K·∫øt qu·∫£ trung b√¨nh tr√™n to√†n b·ªô k-folds:\n  ‚û§ MAP: 0.3734\n  ‚û§ MRR: 0.5921\n  ‚û§ Top1: 0.4597\n  ‚û§ Top2: 0.5904\n  ‚û§ Top3: 0.6660\n  ‚û§ Top4: 0.7174\n  ‚û§ Top5: 0.7562\n  ‚û§ Top10: 0.8576\n  ‚û§ Top15: 0.9050\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import pandas as pd\n\n# D·ªØ li·ªáu SOTA: ImbalancedBugLoc\ndata = {\n    \"Project\": [\"AspectJ\", \"Tomcat\", \"Eclipse\", \"SWT\", \"Birt\"],\n    \"Top1_SOTA\": [52.5, 53.2, 48.1, 40.2, 28.3],\n    \"Top2_SOTA\": [68.7, 65.5, 62.1, 54.9, 39.3],\n    \"Top3_SOTA\": [77.2, 71.0, 68.8, 64.2, 45.7],\n    \"Top4_SOTA\": [81.0, 75.0, 73.0, 69.3, 51.0],\n    \"Top5_SOTA\": [83.8, 78.3, 76.7, 73.4, 53.6],\n    \"Top10_SOTA\": [89.0, 85.6, 84.7, 84.8, 63.2],\n    \"Top15_SOTA\": [91.5, 88.9, 87.8, 89.1, 69.2],\n    \"MRR_SOTA\": [0.66, 0.64, 0.60, 0.55, 0.40],\n    \"MAP_SOTA\": [0.50, 0.59, 0.54, 0.50, 0.32],\n    \"Top1_New\": [61.90, 56.75, 66.30, 60.63, 45.97],\n    \"Top2_New\": [71.13, 69.58, 78.20, 74.70, 59.04],\n    \"Top3_New\": [76.19, 77.48, 83.35, 81.28, 66.60],\n    \"Top4_New\": [80.06, 82.71, 86.71, 84.77, 71.74],\n    \"Top5_New\": [82.74, 85.79, 89.13, 87.49, 75.62],\n    \"Top10_New\": [88.39, 92.90, 94.38, 93.91, 85.76],\n    \"Top15_New\": [91.07, 94.90, 96.19, 96.07, 90.50],\n    \"MRR_New\": [0.7109, 0.6899, 0.7644, 0.7183, 0.5921],\n    \"MAP_New\": [0.5367, 0.4946, 0.5692, 0.4669, 0.3734]\n}\n\n# T·∫°o DataFrame t·ª´ d·ªØ li·ªáu\ndf = pd.DataFrame(data)\n\n# T√°ch th√†nh hai b·∫£ng: SOTA v√† New Model\ndf_sota = df[[\"Project\", \"Top1_SOTA\", \"Top2_SOTA\", \"Top3_SOTA\", \"Top4_SOTA\", \"Top5_SOTA\", \n              \"Top10_SOTA\", \"Top15_SOTA\", \"MRR_SOTA\", \"MAP_SOTA\"]].copy()\ndf_sota[\"Model\"] = \"SOTA\"\n\ndf_new = df[[\"Project\", \"Top1_New\", \"Top2_New\", \"Top3_New\", \"Top4_New\", \"Top5_New\", \n             \"Top10_New\", \"Top15_New\", \"MRR_New\", \"MAP_New\"]].copy()\ndf_new[\"Model\"] = \"New Model\"\n\n# ƒê·ªïi t√™n c·ªôt gi·ªëng b·∫£ng trong ·∫£nh\nrename_cols = {\n    \"Top1_SOTA\": \"1\", \"Top2_SOTA\": \"2\", \"Top3_SOTA\": \"3\", \"Top4_SOTA\": \"4\",\n    \"Top5_SOTA\": \"5\", \"Top10_SOTA\": \"10\", \"Top15_SOTA\": \"15\", \"MRR_SOTA\": \"MRR\", \"MAP_SOTA\": \"MAP\",\n    \"Top1_New\": \"1\", \"Top2_New\": \"2\", \"Top3_New\": \"3\", \"Top4_New\": \"4\",\n    \"Top5_New\": \"5\", \"Top10_New\": \"10\", \"Top15_New\": \"15\", \"MRR_New\": \"MRR\", \"MAP_New\": \"MAP\"\n}\n\ndf_sota.rename(columns=rename_cols, inplace=True)\ndf_new.rename(columns=rename_cols, inplace=True)\n\n# G·ªôp l·∫°i\ndf_combined = pd.concat([df_sota, df_new], axis=0)\ndf_combined = df_combined.sort_values(by=[\"Project\", \"Model\"]).reset_index(drop=True)\n\n# ƒê∆∞a c·ªôt 'Model' v·ªÅ sau 'Project'\ncols = df_combined.columns.tolist()\ncols.insert(1, cols.pop(cols.index('Model')))\ndf_combined = df_combined[cols]\n\n# Hi·ªÉn th·ªã k·∫øt qu·∫£\ndf_combined\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:24:11.685527Z","iopub.execute_input":"2025-04-07T07:24:11.685962Z","iopub.status.idle":"2025-04-07T07:24:11.720882Z","shell.execute_reply.started":"2025-04-07T07:24:11.685926Z","shell.execute_reply":"2025-04-07T07:24:11.719901Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Project      Model      1      2      3      4      5     10     15  \\\n0  AspectJ  New Model  61.90  71.13  76.19  80.06  82.74  88.39  91.07   \n1  AspectJ       SOTA  52.50  68.70  77.20  81.00  83.80  89.00  91.50   \n2     Birt  New Model  45.97  59.04  66.60  71.74  75.62  85.76  90.50   \n3     Birt       SOTA  28.30  39.30  45.70  51.00  53.60  63.20  69.20   \n4  Eclipse  New Model  66.30  78.20  83.35  86.71  89.13  94.38  96.19   \n5  Eclipse       SOTA  48.10  62.10  68.80  73.00  76.70  84.70  87.80   \n6      SWT  New Model  60.63  74.70  81.28  84.77  87.49  93.91  96.07   \n7      SWT       SOTA  40.20  54.90  64.20  69.30  73.40  84.80  89.10   \n8   Tomcat  New Model  56.75  69.58  77.48  82.71  85.79  92.90  94.90   \n9   Tomcat       SOTA  53.20  65.50  71.00  75.00  78.30  85.60  88.90   \n\n      MRR     MAP  \n0  0.7109  0.5367  \n1  0.6600  0.5000  \n2  0.5921  0.3734  \n3  0.4000  0.3200  \n4  0.7644  0.5692  \n5  0.6000  0.5400  \n6  0.7183  0.4669  \n7  0.5500  0.5000  \n8  0.6899  0.4946  \n9  0.6400  0.5900  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Project</th>\n      <th>Model</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>10</th>\n      <th>15</th>\n      <th>MRR</th>\n      <th>MAP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AspectJ</td>\n      <td>New Model</td>\n      <td>61.90</td>\n      <td>71.13</td>\n      <td>76.19</td>\n      <td>80.06</td>\n      <td>82.74</td>\n      <td>88.39</td>\n      <td>91.07</td>\n      <td>0.7109</td>\n      <td>0.5367</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AspectJ</td>\n      <td>SOTA</td>\n      <td>52.50</td>\n      <td>68.70</td>\n      <td>77.20</td>\n      <td>81.00</td>\n      <td>83.80</td>\n      <td>89.00</td>\n      <td>91.50</td>\n      <td>0.6600</td>\n      <td>0.5000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Birt</td>\n      <td>New Model</td>\n      <td>45.97</td>\n      <td>59.04</td>\n      <td>66.60</td>\n      <td>71.74</td>\n      <td>75.62</td>\n      <td>85.76</td>\n      <td>90.50</td>\n      <td>0.5921</td>\n      <td>0.3734</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Birt</td>\n      <td>SOTA</td>\n      <td>28.30</td>\n      <td>39.30</td>\n      <td>45.70</td>\n      <td>51.00</td>\n      <td>53.60</td>\n      <td>63.20</td>\n      <td>69.20</td>\n      <td>0.4000</td>\n      <td>0.3200</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Eclipse</td>\n      <td>New Model</td>\n      <td>66.30</td>\n      <td>78.20</td>\n      <td>83.35</td>\n      <td>86.71</td>\n      <td>89.13</td>\n      <td>94.38</td>\n      <td>96.19</td>\n      <td>0.7644</td>\n      <td>0.5692</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Eclipse</td>\n      <td>SOTA</td>\n      <td>48.10</td>\n      <td>62.10</td>\n      <td>68.80</td>\n      <td>73.00</td>\n      <td>76.70</td>\n      <td>84.70</td>\n      <td>87.80</td>\n      <td>0.6000</td>\n      <td>0.5400</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>SWT</td>\n      <td>New Model</td>\n      <td>60.63</td>\n      <td>74.70</td>\n      <td>81.28</td>\n      <td>84.77</td>\n      <td>87.49</td>\n      <td>93.91</td>\n      <td>96.07</td>\n      <td>0.7183</td>\n      <td>0.4669</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>SWT</td>\n      <td>SOTA</td>\n      <td>40.20</td>\n      <td>54.90</td>\n      <td>64.20</td>\n      <td>69.30</td>\n      <td>73.40</td>\n      <td>84.80</td>\n      <td>89.10</td>\n      <td>0.5500</td>\n      <td>0.5000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Tomcat</td>\n      <td>New Model</td>\n      <td>56.75</td>\n      <td>69.58</td>\n      <td>77.48</td>\n      <td>82.71</td>\n      <td>85.79</td>\n      <td>92.90</td>\n      <td>94.90</td>\n      <td>0.6899</td>\n      <td>0.4946</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Tomcat</td>\n      <td>SOTA</td>\n      <td>53.20</td>\n      <td>65.50</td>\n      <td>71.00</td>\n      <td>75.00</td>\n      <td>78.30</td>\n      <td>85.60</td>\n      <td>88.90</td>\n      <td>0.6400</td>\n      <td>0.5900</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Xu·∫•t ra file Excel\ndf_combined.to_excel(\"model_comparison.xlsx\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T07:25:26.687202Z","iopub.execute_input":"2025-04-07T07:25:26.687562Z","iopub.status.idle":"2025-04-07T07:25:27.235033Z","shell.execute_reply.started":"2025-04-07T07:25:26.687537Z","shell.execute_reply":"2025-04-07T07:25:27.233860Z"}},"outputs":[],"execution_count":4}]}