{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":11265211,"sourceType":"datasetVersion","datasetId":7041476},{"sourceId":11280721,"sourceType":"datasetVersion","datasetId":7052679}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Load dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ­","metadata":{"id":"-eUizc5QmpX4"}},{"cell_type":"code","source":"!pip install nltk\n","metadata":{"id":"9qfVWdges0io","outputId":"e6051f73-0867-4eb4-b5c0-984722d836d3","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:54:27.058293Z","iopub.execute_input":"2025-04-13T13:54:27.058575Z","iopub.status.idle":"2025-04-13T13:54:32.776778Z","shell.execute_reply.started":"2025-04-13T13:54:27.058549Z","shell.execute_reply":"2025-04-13T13:54:32.775597Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import nltk\nnltk.download('averaged_perceptron_tagger_eng')","metadata":{"id":"8SUNdusXgQ-3","outputId":"937573b4-53fb-4bb5-9c39-84eb53bc1890","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:54:32.778258Z","iopub.execute_input":"2025-04-13T13:54:32.778699Z","iopub.status.idle":"2025-04-13T13:54:34.399049Z","shell.execute_reply.started":"2025-04-13T13:54:32.778667Z","shell.execute_reply":"2025-04-13T13:54:34.397973Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# English stop words\nstop_words = set(\n    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n     'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',\n     'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',\n     'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n     'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n     'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n     'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n     'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n     'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n     'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n     'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n     'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n     's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o',\n     're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven',\n     'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won',\n     'wouldn', 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'n', 'p', 'q', 'u', 'v',\n     'w', 'x', 'z', 'us'])\n\n# Java language keywords\njava_keywords = set(\n    ['abstract', 'assert', 'boolean', 'break', 'byte', 'case',\n     'catch', 'char', 'class', 'const', 'continue', 'default', 'do', 'double',\n     'else', 'enum', 'extends', 'false', 'final', 'finally', 'float', 'for', 'goto',\n     'if', 'implements', 'import', 'instanceof', 'int', 'interface', 'long',\n     'native', 'new', 'null', 'package', 'private', 'protected', 'public', 'return',\n     'short', 'static', 'strictfp', 'super', 'switch', 'synchronized', 'this',\n     'throw', 'throws', 'transient', 'true', 'try', 'void', 'volatile', 'while'])\n\nfrom collections import namedtuple\nfrom pathlib import Path\n\n# Dataset root directory (Ä‘iá»u chá»‰nh Ä‘Æ°á»ng dáº«n náº¿u cáº§n)\n_DATASET_ROOT = Path('/content/drive/MyDrive/Colab Notebooks/NLP/Task bug localization/')\n\nDataset = namedtuple('Dataset', ['name', 'src', 'bug_repo', 'repo_url', 'features'])\n\n# CÃ¡c dataset Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a\naspectj = Dataset(\n    'aspectj',\n    _DATASET_ROOT / 'source files/org.aspectj',\n    _DATASET_ROOT / 'bug reports/AspectJ.txt',\n    \"https://github.com/eclipse/org.aspectj/tree/bug433351.git\",\n    _DATASET_ROOT / 'bug reports/AspectJ.xlsx'\n)\n\neclipse = Dataset(\n    'eclipse',\n    _DATASET_ROOT / 'source files/eclipse.platform.ui-johna-402445',\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.txt',\n    \"https://github.com/eclipse/eclipse.platform.ui.git\",\n    _DATASET_ROOT / 'bug reports/Eclipse_Platform_UI.xlsx'\n)\n\nswt = Dataset(\n    'swt',\n    _DATASET_ROOT / 'source files/eclipse.platform.swt-xulrunner-31',\n    _DATASET_ROOT / 'bug reports/SWT.txt',\n    \"https://github.com/eclipse/eclipse.platform.swt.git\",\n    _DATASET_ROOT / 'bug reports/SWT.xlsx'\n)\n\ntomcat = Dataset(\n    'tomcat',\n    _DATASET_ROOT / 'source files/tomcat-7.0.51',\n    _DATASET_ROOT / 'bug reports/Tomcat.txt',\n    \"https://github.com/apache/tomcat.git\",\n    _DATASET_ROOT / 'bug reports/Tomcat.xlsx'\n)\n\nbirt = Dataset(\n    'birt',\n    _DATASET_ROOT / 'source files/birt-20140211-1400',\n    _DATASET_ROOT / 'bug reports/Birt.txt',\n    \"https://github.com/apache/birt.git\",\n    _DATASET_ROOT / 'bug reports/Birt.xlsx'\n)\n\n\n### Current dataset in use. (change this name to change the dataset)\nDATASET = tomcat\n\nclass BugReport:\n    \"\"\"Class representing each bug report\"\"\"\n    __slots__ = ['summary', 'description', 'fixed_files', 'report_time', 'pos_tagged_summary', 'pos_tagged_description','stack_traces','stack_traces_remove']\n\n    def __init__(self, summary, description, fixed_files, report_time):\n        self.summary = summary\n        self.description = description\n        self.fixed_files = fixed_files\n        self.report_time = report_time\n        self.pos_tagged_summary = None\n        self.pos_tagged_description = None\n        self.stack_traces = None\n        self.stack_traces_remove = None\n\nclass SourceFile:\n    \"\"\"Class representing each source file\"\"\"\n    __slots__ = ['all_content', 'comments', 'class_names', 'attributes', 'method_names', 'variables', 'file_name',\n                 'pos_tagged_comments', 'exact_file_name', 'package_name']\n\n    def __init__(self, all_content, comments, class_names, attributes, method_names, variables, file_name,\n                 package_name):\n        self.all_content = all_content\n        self.comments = comments\n        self.class_names = class_names\n        self.attributes = attributes\n        self.method_names = method_names\n        self.variables = variables\n        self.file_name = file_name\n        self.exact_file_name = file_name[0]\n        self.package_name = package_name\n        self.pos_tagged_comments = None\n\n\nclass Parser:\n    \"\"\"Class containing different parsers\"\"\"\n    __slots__ = ['name', 'src', 'bug_repo']\n\n    def __init__(self, pro):\n        self.name = pro.name\n        self.src = pro.src\n        self.bug_repo = pro.bug_repo\n\n    def report_parser(self):\n        reader = csv.DictReader(open(self.bug_repo, \"r\"), delimiter=\"\\t\")\n        bug_reports = OrderedDict()\n        # raw_texts = []\n        # fixed_files = []\n        for line in reader:\n            # line[\"raw_text\"] = line[\"summary\"] + ' ' + line[\"description\"]\n            line[\"report_time\"] = datetime.strptime(line[\"report_time\"], \"%Y-%m-%d %H:%M:%S\")\n            temp = line[\"files\"].strip().split(\".java\")\n            length = len(temp)\n            x = []\n            for i, f in enumerate(temp):\n                if i == (length - 1):\n                    x.append(os.path.normpath(f))\n                    continue\n                x.append(os.path.normpath(f + \".java\"))\n            line[\"files\"] = x\n            bug_reports[line[\"bug_id\"]] = BugReport(line[\"summary\"], line[\"description\"], line[\"files\"],\n                                                    line[\"report_time\"])\n        # bug_reports = tsv2dict(self.bug_repo)\n\n        return bug_reports\n\n    def src_parser(self):\n        \"\"\"Parse source code directory of a program and colect its java files\"\"\"\n\n        # Gettting the list of source files recursively from the source directory\n        src_addresses = glob.glob(str(self.src) + '/**/*.java', recursive=True)\n        print(src_addresses)\n        # Creating a java lexer instance for pygments.lex() method\n        java_lexer = JavaLexer()\n        src_files = OrderedDict()\n        # src_files = dict()\n        # Looping to parse each source file\n        for src_file in src_addresses:\n            with open(src_file, encoding='latin-1') as file:\n                src = file.read()\n\n            # Placeholder for different parts of a source file\n            comments = ''\n            class_names = []\n            attributes = []\n            method_names = []\n            variables = []\n\n            # Source parsing\n            parse_tree = None\n            try:\n                parse_tree = javalang.parse.parse(src)\n                for path, node in parse_tree.filter(javalang.tree.VariableDeclarator):\n                    if isinstance(path[-2], javalang.tree.FieldDeclaration):\n                        attributes.append(node.name)\n                    elif isinstance(path[-2], javalang.tree.VariableDeclaration):\n                        variables.append(node.name)\n            except:\n                pass\n\n            # Triming the source file\n            ind = False\n            if parse_tree:\n                if parse_tree.imports:\n                    last_imp_path = parse_tree.imports[-1].path\n                    src = src[src.index(last_imp_path) + len(last_imp_path) + 1:]\n                elif parse_tree.package:\n                    package_name = parse_tree.package.name\n                    src = src[src.index(package_name) + len(package_name) + 1:]\n                else:  # no import and no package declaration\n                    ind = True\n            # javalang can't parse the source file\n            else:\n                ind = True\n\n            # Lexically tokenize the source file\n            lexed_src = pygments.lex(src, java_lexer)\n\n            for i, token in enumerate(lexed_src):\n                if token[0] in Token.Comment:\n                    if ind and i == 0 and token[0] is Token.Comment.Multiline:\n                        src = src[src.index(token[1]) + len(token[1]):]\n                        continue\n                    comments = comments + token[1]\n                elif token[0] is Token.Name.Class:\n                    class_names.append(token[1])\n                elif token[0] is Token.Name.Function:\n                    method_names.append(token[1])\n\n            # get the package declaration if exists\n            if parse_tree and parse_tree.package:\n                package_name = parse_tree.package.name\n            else:\n                package_name = None\n\n            if self.name == 'aspectj' or 'tomcat' or 'eclipse' or 'swt':\n                src_files[os.path.relpath(src_file, start=self.src)] = SourceFile(src, comments, class_names,\n                                                                                  attributes, method_names, variables, [\n                                                                                      os.path.basename(src_file).split(\n                                                                                          '.')[0]], package_name)\n            else:\n                # If source files has package declaration\n                if package_name:\n                    src_id = (package_name + '.' + os.path.basename(src_file))\n                else:\n                    src_id = os.path.basename(src_file)\n                src_files[src_id] = SourceFile(src, comments, class_names, attributes, method_names, variables,\n                                               [os.path.basename(src_file).split('.')[0]], package_name)\n            # print(src_files)\n            # print(\"===========\")\n        return src_files\n\n\nclass ReportPreprocessing:\n    \"\"\"Class preprocess bug reports\"\"\"\n    __slots__ = ['bug_reports']\n\n    def __init__(self, bug_reports):\n        self.bug_reports = bug_reports\n\n    def extract_stack_traces(self):\n        \"\"\"Extract stack traces from bug reports\"\"\"\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            report.stack_traces = st\n\n    def extract_stack_traces_remove(self):\n        pattern = re.compile(r' at (.*?)\\((.*?)\\)')\n        signs = ['.java', 'Unknown Source', 'Native Method']\n        for report in self.bug_reports.values():\n            st_canid = re.findall(pattern, report.description)\n            st = [x for x in st_canid if any(s in x[1] for s in signs)]\n            at = []\n            for x in st:\n                if (x[1] == 'Unknown Source'):\n                    temp = 'Unknown'\n                    y = x[0]+ '(' + temp\n                else:\n                    y = x[0] + '(' + x[1] + ')'\n                at.append(y)\n            report.stack_traces_remove = at\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from bug reports raw_text\"\"\"\n        for report in self.bug_reports.values():\n            # Tokenizing using word_tokeize for more accurate pos-tagging\n            sum_tok = nltk.word_tokenize(report.summary)\n            desc_tok = nltk.word_tokenize(report.description)\n            sum_pos = nltk.pos_tag(sum_tok)\n            desc_pos = nltk.pos_tag(desc_tok)\n            report.pos_tagged_summary = [token for token, pos in sum_pos if 'NN' in pos or 'VB' in pos]\n            report.pos_tagged_description = [token for token, pos in desc_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"Tokenize bug report intro tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = nltk.wordpunct_tokenize(report.summary)\n            report.description = nltk.wordpunct_tokenize(report.description)\n\n    def _split_camelcase(self, tokens):\n        # copy tokens\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camel case detection for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        \"\"\"Split camelcase indentifiers\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = self._split_camelcase(report.summary)\n            report.description = self._split_camelcase(report.description)\n            report.pos_tagged_summary = self._split_camelcase(report.pos_tagged_summary)\n            report.pos_tagged_description = self._split_camelcase(report.pos_tagged_description)\n\n    def normalize(self):\n        \"\"\"remove punctuation, numbers and lowecase conversion\"\"\"\n        # build a translate table for punctuation and number removal\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n\n        for report in self.bug_reports.values():\n            summary_punctnum_rem = [token.translate(punctnum_table) for token in report.summary]\n            desc_punctnum_rem = [token.translate(punctnum_table) for token in report.description]\n            pos_sum_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_summary]\n            pos_desc_punctnum_rem = [token.translate(punctnum_table) for token in report.pos_tagged_description]\n            report.summary = [token.lower() for token in summary_punctnum_rem if token]\n            report.description = [token.lower() for token in desc_punctnum_rem if token]\n            report.pos_tagged_summary = [token.lower() for token in pos_sum_punctnum_rem if token]\n            report.pos_tagged_description = [token.lower() for token in pos_desc_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        \"\"\"removing stop word from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in stop_words]\n            report.description = [token for token in report.description if token not in stop_words]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in stop_words]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in stop_words]\n\n    def remove_java_keywords(self):\n        \"\"\"removing java language keywords from tokens\"\"\"\n        for report in self.bug_reports.values():\n            report.summary = [token for token in report.summary if token not in java_keywords]\n            report.description = [token for token in report.description if token not in java_keywords]\n            report.pos_tagged_summary = [token for token in report.pos_tagged_summary if token not in java_keywords]\n            report.pos_tagged_description = [token for token in report.pos_tagged_description if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for report in self.bug_reports.values():\n            report.summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.summary], report.summary]))\n            report.description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.description], report.description]))\n            report.pos_tagged_summary = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_summary], report.pos_tagged_summary]))\n            report.pos_tagged_description = dict(\n                zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in report.pos_tagged_description], report.pos_tagged_description]))\n\n    def preprocess(self):\n        self.extract_stack_traces()\n        self.extract_stack_traces_remove()\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_java_keywords()\n        self.stem()\n\nclass SrcPreprocessing:\n    \"\"\"class to preprocess source code\"\"\"\n    __slots__ = ['src_files']\n\n    def __init__(self, src_files):\n        self.src_files = src_files\n\n    def pos_tagging(self):\n        \"\"\"Extracing specific pos tags from comments\"\"\"\n        for src in self.src_files.values():\n            # tokenize using word_tokenize\n            comments_tok = nltk.word_tokenize(src.comments)\n            comments_pos = nltk.pos_tag(comments_tok)\n            src.pos_tagged_comments = [token for token, pos in comments_pos if 'NN' in pos or 'VB' in pos]\n\n    def tokenize(self):\n        \"\"\"tokenize source code to tokens\"\"\"\n        for src in self.src_files.values():\n            src.all_content = nltk.wordpunct_tokenize(src.all_content)\n            src.comments = nltk.wordpunct_tokenize(src.comments)\n\n    def _split_camelcase(self, tokens):\n        # copy token\n        returning_tokens = tokens[:]\n        for token in tokens:\n            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n            # if token is split into some other tokens\n            if len(split_tokens) > 1:\n                returning_tokens.remove(token)\n                # camelcase defect for new tokens\n                for st in split_tokens:\n                    camel_split = inflection.underscore(st).split('_')\n                    if len(camel_split) > 1:\n                        returning_tokens.append(st)\n                        returning_tokens = returning_tokens + camel_split\n                    else:\n                        returning_tokens.append(st)\n            else:\n                camel_split = inflection.underscore(token).split('_')\n                if len(camel_split) > 1:\n                    returning_tokens = returning_tokens + camel_split\n        return returning_tokens\n\n    def split_camelcase(self):\n        # Split camelcase indenti\n        for src in self.src_files.values():\n            src.all_content = self._split_camelcase(src.all_content)\n            src.comments = self._split_camelcase(src.comments)\n            src.class_names = self._split_camelcase(src.class_names)\n            src.attributes = self._split_camelcase(src.attributes)\n            src.method_names = self._split_camelcase(src.method_names)\n            src.variables = self._split_camelcase(src.variables)\n            src.pos_tagged_comments = self._split_camelcase(src.pos_tagged_comments)\n\n    def normalize(self):\n        \"remove punctuation, number and lowercase conversion\"\n        # build a translate table for punctuation and number\n        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n        for src in self.src_files.values():\n            content_punctnum_rem = [token.translate(punctnum_table) for token in src.all_content]\n            comments_punctnum_rem = [token.translate(punctnum_table) for token in src.comments]\n            classnames_punctnum_rem = [token.translate(punctnum_table) for token in src.class_names]\n            attributes_punctnum_rem = [token.translate(punctnum_table) for token in src.attributes]\n            methodnames_punctnum_rem = [token.translate(punctnum_table) for token in src.method_names]\n            variables_punctnum_rem = [token.translate(punctnum_table) for token in src.variables]\n            filename_punctnum_rem = [token.translate(punctnum_table) for token in src.file_name]\n            pos_comments_punctnum_rem = [token.translate(punctnum_table) for token in src.pos_tagged_comments]\n\n            src.all_content = [token.lower() for token in content_punctnum_rem if token]\n            src.comments = [token.lower() for token in comments_punctnum_rem if token]\n            src.class_names = [token.lower() for token in classnames_punctnum_rem if token]\n            src.attributes = [token.lower() for token in attributes_punctnum_rem if token]\n            src.method_names = [token.lower() for token in methodnames_punctnum_rem if token]\n            src.variables = [token.lower() for token in variables_punctnum_rem if token]\n            src.file_name = [token.lower() for token in filename_punctnum_rem if token]\n            src.pos_tagged_comments = [token.lower() for token in pos_comments_punctnum_rem if token]\n\n    def remove_stopwords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in stop_words]\n            src.comments = [token for token in src.comments if token not in stop_words]\n            src.class_names = [token for token in src.class_names if token not in stop_words]\n            src.attributes = [token for token in src.attributes if token not in stop_words]\n            src.method_names = [token for token in src.method_names if token not in stop_words]\n            src.variables = [token for token in src.variables if token not in stop_words]\n            src.file_name = [token for token in src.file_name if token not in stop_words]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in stop_words]\n\n    def remove_javakeywords(self):\n        for src in self.src_files.values():\n            src.all_content = [token for token in src.all_content if token not in java_keywords]\n            src.comments = [token for token in src.comments if token not in java_keywords]\n            src.class_names = [token for token in src.class_names if token not in java_keywords]\n            src.attributes = [token for token in src.attributes if token not in java_keywords]\n            src.method_names = [token for token in src.method_names if token not in java_keywords]\n            src.variables = [token for token in src.variables if token not in java_keywords]\n            src.file_name = [token for token in src.file_name if token not in java_keywords]\n            src.pos_tagged_comments = [token for token in src.pos_tagged_comments if token not in java_keywords]\n\n    def stem(self):\n        # stemming tokens\n        stemmer = PorterStemmer()\n        for src in self.src_files.values():\n            src.all_content = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.all_content], src.all_content]))\n            src.comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.comments], src.comments]))\n            src.class_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.class_names], src.class_names]))\n            src.attributes = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.attributes], src.attributes]))\n            src.method_names = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.method_names], src.method_names]))\n            src.variables = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.variables], src.variables]))\n            src.file_name = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.file_name], src.file_name]))\n            src.pos_tagged_comments = dict(zip(['stemmed', 'unstemmed'], [[stemmer.stem(token) for token in src.pos_tagged_comments], src.pos_tagged_comments]))\n\n\n    def preprocess(self):\n        self.pos_tagging()\n        self.tokenize()\n        self.split_camelcase()\n        self.normalize()\n        self.remove_stopwords()\n        self.remove_javakeywords()\n        self.stem()","metadata":{"id":"_22yeS4wcWpU","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:54:34.401300Z","iopub.execute_input":"2025-04-13T13:54:34.401793Z","iopub.status.idle":"2025-04-13T13:54:34.465984Z","shell.execute_reply.started":"2025-04-13T13:54:34.401763Z","shell.execute_reply":"2025-04-13T13:54:34.464952Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install inflection\nimport inflection\n","metadata":{"id":"ACZrz5Byh7Ur","outputId":"68b0fd14-1ac6-484a-cb84-2ab028bd5ed6","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:54:34.467905Z","iopub.execute_input":"2025-04-13T13:54:34.468305Z","iopub.status.idle":"2025-04-13T13:54:39.001607Z","shell.execute_reply.started":"2025-04-13T13:54:34.468268Z","shell.execute_reply":"2025-04-13T13:54:39.000068Z"}},"outputs":[{"name":"stdout","text":"Collecting inflection\n  Downloading inflection-0.5.1-py2.py3-none-any.whl.metadata (1.7 kB)\nDownloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\nInstalling collected packages: inflection\nSuccessfully installed inflection-0.5.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')\nimport pickle\nfrom google.colab import drive\nimport csv\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport re\nimport string\nfrom nltk.stem.porter import PorterStemmer","metadata":{"id":"P9emxprnStnP","outputId":"51497efe-74ae-4abd-d5a0-485188dab6f7","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:54:39.003003Z","iopub.execute_input":"2025-04-13T13:54:39.003343Z","iopub.status.idle":"2025-04-13T13:54:39.274334Z","shell.execute_reply.started":"2025-04-13T13:54:39.003310Z","shell.execute_reply":"2025-04-13T13:54:39.273195Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Load dá»¯ liá»‡u","metadata":{"id":"SC6LbKkWy8kR"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# ÄÆ°á»ng dáº«n Ä‘áº¿n cÃ¡c file pickle\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_src_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_src_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_src_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_src_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_src_processed.pkl'\n}\n\n# Load tá»«ng file vÃ  lÆ°u vÃ o cÃ¡c biáº¿n tÆ°Æ¡ng á»©ng\ndatasets = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        datasets[name] = pickle.load(f)\n\n# Kiá»ƒm tra dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c load vÃ o cÃ¡c biáº¿n\nfor name, data in datasets.items():\n    print(f\"Data for {name}:\")\n","metadata":{"id":"O3TGVN1KzAXg","outputId":"63f347a7-1b8d-41fe-98e5-105ca43233e2","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:54:39.275667Z","iopub.execute_input":"2025-04-13T13:54:39.276038Z","iopub.status.idle":"2025-04-13T13:54:54.877423Z","shell.execute_reply.started":"2025-04-13T13:54:39.276008Z","shell.execute_reply":"2025-04-13T13:54:54.876518Z"}},"outputs":[{"name":"stdout","text":"Data for aspectj:\nData for eclipse:\nData for swt:\nData for tomcat:\nData for birt:\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"eclipse_src = datasets['eclipse']\nbirt_src = datasets['birt']\nswt_src = datasets['swt']\ntomcat_src = datasets['tomcat']\naspectj_src = datasets['aspectj']","metadata":{"id":"b91231aHzUu_","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:54:54.878246Z","iopub.execute_input":"2025-04-13T13:54:54.878539Z","iopub.status.idle":"2025-04-13T13:54:54.883520Z","shell.execute_reply.started":"2025-04-13T13:54:54.878515Z","shell.execute_reply":"2025-04-13T13:54:54.882136Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load dá»¯ liá»‡u tá»« cÃ¡c file pickle Ä‘Ã£ lÆ°u\nfile_paths = {\n    'aspectj': '/kaggle/input/bug-localization-data/aspectj_reports_processed.pkl',\n    'eclipse': '/kaggle/input/bug-localization-data/eclipse_reports_processed.pkl',\n    'swt': '/kaggle/input/bug-localization-data/swt_reports_processed.pkl',\n    'tomcat': '/kaggle/input/bug-localization-data/tomcat_reports_processed.pkl',\n    'birt': '/kaggle/input/bug-localization-data/birt_reports_processed.pkl'\n}\n\n# Load tá»«ng dataset vÃ  lÆ°u vÃ o cÃ¡c biáº¿n\nall_processed_reports = {}\n\nfor name, path in file_paths.items():\n    with open(path, 'rb') as f:\n        all_processed_reports[name] = pickle.load(f)\n\n# Kiá»ƒm tra dá»¯ liá»‡u Ä‘Ã£ load vÃ o\nfor dataset, reports in all_processed_reports.items():\n    print(f\"Processed reports for {dataset}:\")","metadata":{"id":"kXiYZuNgzv0M","outputId":"053bc2e9-ef09-4aea-9431-acfda03d3300","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:54:54.886952Z","iopub.execute_input":"2025-04-13T13:54:54.887222Z","iopub.status.idle":"2025-04-13T13:54:58.252136Z","shell.execute_reply.started":"2025-04-13T13:54:54.887200Z","shell.execute_reply":"2025-04-13T13:54:58.251120Z"}},"outputs":[{"name":"stdout","text":"Processed reports for aspectj:\nProcessed reports for eclipse:\nProcessed reports for swt:\nProcessed reports for tomcat:\nProcessed reports for birt:\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"eclipse_reports = all_processed_reports['eclipse']\nbirt_reports = all_processed_reports['birt']\nswt_reports = all_processed_reports['swt']\ntomcat_reports = all_processed_reports['tomcat']\naspectj_reports = all_processed_reports['aspectj']","metadata":{"id":"iAUv0Bnv0FcI","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:54:58.254078Z","iopub.execute_input":"2025-04-13T13:54:58.254361Z","iopub.status.idle":"2025-04-13T13:54:58.259249Z","shell.execute_reply.started":"2025-04-13T13:54:58.254336Z","shell.execute_reply":"2025-04-13T13:54:58.258065Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 2. Xá»­ lÃ­ data, gÃ¡n nhÃ£n\n- Sáº¯p xáº¿p bug report theo thá»i gian (report_time)\n- Chia thÃ nh 10 folds\n- Táº¡o training/test dataset theo kiá»ƒu fold i â†’ fold i+1\n- GÃ¡n nhÃ£n cho tá»«ng cáº·p (bug report, source file)","metadata":{"id":"-hwWTIRJ9PXd"}},{"cell_type":"code","source":"# B1: Láº¥y danh sÃ¡ch (bug_id, bug_report), sau Ä‘Ã³ sáº¯p xáº¿p theo report_time\nsorted_bug_reports = sorted(aspectj_reports.items(), key=lambda x: x[1].report_time)\ndata_src = aspectj_src","metadata":{"id":"HrKZiEgO9X16","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:54:58.260484Z","iopub.execute_input":"2025-04-13T13:54:58.260862Z","iopub.status.idle":"2025-04-13T13:54:58.279368Z","shell.execute_reply.started":"2025-04-13T13:54:58.260821Z","shell.execute_reply":"2025-04-13T13:54:58.278245Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def split_into_folds(sorted_reports, num_folds=10):\n    fold_size = len(sorted_reports) // num_folds\n    folds = [sorted_reports[i*fold_size:(i+1)*fold_size] for i in range(num_folds)]\n\n    # Náº¿u cÃ²n dÆ°, ráº£i Ä‘á»u vÃ o cÃ¡c fold Ä‘áº§u\n    remainder = sorted_reports[num_folds*fold_size:]\n    for i, extra in enumerate(remainder):\n        folds[i].append(extra)\n    return folds\n\ndata_folds = split_into_folds(sorted_bug_reports, num_folds=3)\n","metadata":{"id":"fgRpuQKE9aA2","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:54:58.280699Z","iopub.execute_input":"2025-04-13T13:54:58.281128Z","iopub.status.idle":"2025-04-13T13:54:58.300500Z","shell.execute_reply.started":"2025-04-13T13:54:58.281089Z","shell.execute_reply":"2025-04-13T13:54:58.299485Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"i = 0 # thá»­ vá»›i fold 0 â†’ 1\ntrain_fold = data_folds[i]\ntest_fold = data_folds[i+1]","metadata":{"id":"0itPPQ5O9cDT","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:54:58.301511Z","iopub.execute_input":"2025-04-13T13:54:58.301903Z","iopub.status.idle":"2025-04-13T13:54:58.320419Z","shell.execute_reply.started":"2025-04-13T13:54:58.301866Z","shell.execute_reply":"2025-04-13T13:54:58.319342Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import random\n\ndef generate_balanced_pairs(bug_fold, source_files, num_negatives_per_positive=50):\n    data = []\n    for bug_id, bug in bug_fold:\n        # Danh sÃ¡ch file chá»©a bug (poszqitive)\n        positive_paths = set(bug.fixed_files)\n        positive = [\n            (bug_id, bug, src_path, source_files[src_path], 1)\n            for src_path in positive_paths if src_path in source_files\n        ]\n\n        # Danh sÃ¡ch file cÃ²n láº¡i Ä‘á»ƒ láº¥y negative\n        all_paths = list(source_files.keys())\n        negative_paths = list(set(all_paths) - positive_paths)\n        sampled_negatives = random.sample(negative_paths, min(num_negatives_per_positive * len(positive), len(negative_paths)))\n\n        negative = [\n            (bug_id, bug, src_path, source_files[src_path], 0)\n            for src_path in sampled_negatives if src_path in source_files\n        ]\n\n        data.extend(positive + negative)\n    return data\ndef generate_all_negatives_pairs(bug_fold, source_files):\n    data = []\n    for bug_id, bug in bug_fold:\n        positive_paths = set(bug.fixed_files)\n        positive = [\n            (bug_id, bug, src_path, source_files[src_path], 1)\n            for src_path in positive_paths if src_path in source_files\n        ]\n\n        all_paths = list(source_files.keys())\n        negative_paths = list(set(all_paths) - positive_paths)\n\n        negative = [\n            (bug_id, bug, src_path, source_files[src_path], 0)\n            for src_path in negative_paths if src_path in source_files\n        ]\n\n        data.extend(positive + negative)\n    return data\n\n\ntrain_pairs = generate_balanced_pairs(train_fold, data_src, num_negatives_per_positive=50)\n#test_pairs = generate_balanced_pairs(test_fold, data_src, num_negatives_per_positive=50)\ntest_pairs = generate_all_negatives_pairs(test_fold, data_src)\n","metadata":{"id":"wjLEL9mR9fPv","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:54:58.321649Z","iopub.execute_input":"2025-04-13T13:54:58.321994Z","iopub.status.idle":"2025-04-13T13:55:05.640698Z","shell.execute_reply.started":"2025-04-13T13:54:58.321967Z","shell.execute_reply":"2025-04-13T13:55:05.639496Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Xá»­ lÃ­ máº¥t cÃ¢n báº±ng","metadata":{"id":"phVI5NPj-GMs"}},{"cell_type":"code","source":"def compute_stats(pairs):\n    total = len(pairs)\n    pos = sum(1 for _, _, _, _, label in pairs if label == 1)\n    neg = total - pos\n    ratio = pos / total if total > 0 else 0\n    return total, pos, neg, ratio\n\n  \ntotal, pos, neg, ratio = compute_stats(train_pairs)\nprint(\"ğŸ“Š Train Set:\")\nprint(f\"  â¤ Tá»•ng cáº·p: {total}\")\nprint(f\"  âœ… Positive (label=1): {pos}\")\nprint(f\"  âŒ Negative (label=0): {neg}\")\nprint(f\"  âš–ï¸ Tá»· lá»‡ positive: {ratio:.4f}\")\n\ntotal, pos, neg, ratio = compute_stats(test_pairs)\nprint(\"\\nğŸ§ª Test Set:\")\nprint(f\"  â¤ Tá»•ng cáº·p: {total}\")\nprint(f\"  âœ… Positive (label=1): {pos}\")\nprint(f\"  âŒ Negative (label=0): {neg}\")\nprint(f\"  âš–ï¸ Tá»· lá»‡ positive: {ratio:.4f}\")\n","metadata":{"id":"G-m4H5d79uzp","outputId":"1a4a413d-189b-4b5d-ca55-6552e56ce33f","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:05.641850Z","iopub.execute_input":"2025-04-13T13:55:05.642142Z","iopub.status.idle":"2025-04-13T13:55:05.769229Z","shell.execute_reply.started":"2025-04-13T13:55:05.642117Z","shell.execute_reply":"2025-04-13T13:55:05.767884Z"}},"outputs":[{"name":"stdout","text":"ğŸ“Š Train Set:\n  â¤ Tá»•ng cáº·p: 8619\n  âœ… Positive (label=1): 169\n  âŒ Negative (label=0): 8450\n  âš–ï¸ Tá»· lá»‡ positive: 0.0196\n\nğŸ§ª Test Set:\n  â¤ Tá»•ng cáº·p: 1368180\n  âœ… Positive (label=1): 168\n  âŒ Negative (label=0): 1368012\n  âš–ï¸ Tá»· lá»‡ positive: 0.0001\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### HÃ m 1: Táº¡o batches cÃ³ bootstrapping (luÃ´n chá»©a Ã­t nháº¥t 1 positive sample)","metadata":{}},{"cell_type":"code","source":"def create_bootstrapped_batches(pairs, batch_size=128, pos_ratio=0.1):\n    positives = [p for p in pairs if p[-1] == 1]\n    negatives = [p for p in pairs if p[-1] == 0]\n\n    pos_per_batch = max(1, int(batch_size * pos_ratio))\n    neg_per_batch = batch_size - pos_per_batch\n\n    random.shuffle(positives)\n    random.shuffle(negatives)\n\n    batches = []\n    pos_idx, neg_idx = 0, 0\n\n    while neg_idx + neg_per_batch <= len(negatives):\n        pos_batch = []\n        for _ in range(pos_per_batch):\n            pos_batch.append(positives[pos_idx % len(positives)])\n            pos_idx += 1\n\n        neg_batch = negatives[neg_idx:neg_idx + neg_per_batch]\n        neg_idx += neg_per_batch\n\n        batch = pos_batch + neg_batch\n        random.shuffle(batch)\n        batches.append(batch)\n\n    return batches\n","metadata":{"id":"VVi_cbQD-a1b","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:05.770523Z","iopub.execute_input":"2025-04-13T13:55:05.770904Z","iopub.status.idle":"2025-04-13T13:55:05.777995Z","shell.execute_reply.started":"2025-04-13T13:55:05.770866Z","shell.execute_reply":"2025-04-13T13:55:05.776917Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Focal Loss Function","metadata":{}},{"cell_type":"code","source":"\ndef focal_loss(predictions, targets, alpha=0.999, gamma=2.0, eps=1e-6):\n    \"\"\"\n    predictions: tensor (batch_size,) - output sigmoid from model\n    targets: tensor (batch_size,) - true labels (0 or 1)\n    \"\"\"\n    # Avoid log(0)\n    predictions = predictions.clamp(min=eps, max=1.0 - eps)\n\n    # Compute focal loss\n    loss = -alpha * (1 - predictions)**gamma * targets * predictions.log() \\\n           - (1 - alpha) * predictions**gamma * (1 - targets) * (1 - predictions).log()\n    return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:05.778972Z","iopub.execute_input":"2025-04-13T13:55:05.779243Z","iopub.status.idle":"2025-04-13T13:55:05.798131Z","shell.execute_reply.started":"2025-04-13T13:55:05.779220Z","shell.execute_reply":"2025-04-13T13:55:05.797021Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# 4. TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# HÃ m xá»­ lÃ½ text gá»™p láº¡i tá»« bug report\ndef bug_to_text(bug):\n    summary = bug.summary['unstemmed'] if isinstance(bug.summary, dict) else bug.summary\n    desc = bug.description['unstemmed'] if isinstance(bug.description, dict) else bug.description\n    return \" \".join(summary + desc)\n\n# HÃ m xá»­ lÃ½ text tá»« source file\ndef src_to_text(src):\n    content = src.all_content['unstemmed'] if isinstance(src.all_content, dict) else src.all_content\n    comments = src.comments['unstemmed'] if isinstance(src.comments, dict) else src.comments\n    return \" \".join(content + comments)\n","metadata":{"id":"4en89sb6-s54","outputId":"5c14ad2a-da00-495a-ec81-38687265bcfe","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:05.799375Z","iopub.execute_input":"2025-04-13T13:55:05.799881Z","iopub.status.idle":"2025-04-13T13:55:05.816733Z","shell.execute_reply.started":"2025-04-13T13:55:05.799834Z","shell.execute_reply":"2025-04-13T13:55:05.815502Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Äáº·c trÆ°ng 1: TÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng tá»« vá»±ng (lexical similarity)\n- PhÆ°Æ¡ng phÃ¡p: sá»­ dá»¥ng TF-IDF vÃ  cosine similarity.\n- Input: Cáº·p dá»¯ liá»‡u (bug report, source file)\n- Output: máº£ng numpy chá»©a cÃ¡c giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine giá»¯a bug report vÃ  source file cho má»—i cáº·p.","metadata":{}},{"cell_type":"code","source":"def compute_lexical_similarity(pairs):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # Gá»™p cáº£ bug + src láº¡i Ä‘á»ƒ fit chung vectorizer\n    combined = bug_texts + src_texts\n    tfidf = TfidfVectorizer()\n    tfidf_matrix = tfidf.fit_transform(combined)\n\n    # TÃ¡ch riÃªng láº¡i tá»«ng pháº§n\n    bug_vecs = tfidf_matrix[:len(pairs)]\n    src_vecs = tfidf_matrix[len(pairs):]\n\n    # TÃ­nh cosine cho tá»«ng cáº·p (theo hÃ ng tÆ°Æ¡ng á»©ng)\n    similarities = cosine_similarity(bug_vecs, src_vecs).diagonal()\n\n    return similarities\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:05.817708Z","iopub.execute_input":"2025-04-13T13:55:05.818036Z","iopub.status.idle":"2025-04-13T13:55:05.833337Z","shell.execute_reply.started":"2025-04-13T13:55:05.818011Z","shell.execute_reply":"2025-04-13T13:55:05.832255Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"glove_path = \"/kaggle/input/glove-embedding/glove.6B.100d.txt\"\n# Load GloVe 100d vÃ o dictionary\nimport numpy as np\n\ndef load_glove_embeddings(filepath):\n    embeddings = {}\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.strip().split()\n            word = values[0]\n            vector = np.array(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n    \nglove_embeddings = load_glove_embeddings(glove_path)","metadata":{"id":"6FexoFrK_LTE","outputId":"b7e7b200-954e-4541-a27b-dc43607ae8f2","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:05.834364Z","iopub.execute_input":"2025-04-13T13:55:05.834676Z","iopub.status.idle":"2025-04-13T13:55:16.757302Z","shell.execute_reply.started":"2025-04-13T13:55:05.834652Z","shell.execute_reply":"2025-04-13T13:55:16.756373Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Äáº·c trÆ°ng 2: TÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a (semantic similarity)\n- PhÆ°Æ¡ng phÃ¡p: TF-IDF weighted average cá»§a GloVe vectors vÃ  cosine similarity\n- Input:  (bug report, source file).\n- Output: Má»™t máº£ng numpy chá»©a cÃ¡c giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine giá»¯a bug report vÃ  source file cho má»—i cáº·p, dá»±a trÃªn GloVe vectors vÃ  trá»ng sá»‘ TF-IDF.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\ndef compute_semantic_similarity(pairs, glove_dict, dim=100, device=\"cuda\"):\n    bug_texts = [bug_to_text(bug) for _, bug, _, _, _ in pairs]\n    src_texts = [src_to_text(src) for _, _, _, src, _ in pairs]\n\n    # Fit TF-IDF Ä‘á»ƒ láº¥y idf weight\n    tfidf = TfidfVectorizer()\n    tfidf.fit(bug_texts + src_texts)\n    vocab = tfidf.vocabulary_\n    idf_weights = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n\n    # Embed text thÃ nh torch tensor vector\n    def embed_text_tensor(text):\n        tokens = text.split()\n        vecs = []\n        weights = []\n        for token in tokens:\n            if token in glove_dict and token in vocab:\n                vecs.append(torch.tensor(glove_dict[token], dtype=torch.float32))\n                weights.append(idf_weights[token])\n        if not vecs:\n            return torch.zeros(dim)\n        vecs = torch.stack(vecs)  # (n_tokens, dim)\n        weights = torch.tensor(weights, dtype=torch.float32).unsqueeze(1)  # (n_tokens, 1)\n        weighted_vecs = vecs * weights\n        return weighted_vecs.sum(dim=0) / weights.sum()\n\n    # Convert all to tensor and move to GPU\n    bug_vecs = [embed_text_tensor(text).to(device) for text in bug_texts]\n    src_vecs = [embed_text_tensor(text).to(device) for text in src_texts]\n\n    # Normalize before cosine\n    bug_stack = F.normalize(torch.stack(bug_vecs), p=2, dim=1)\n    src_stack = F.normalize(torch.stack(src_vecs), p=2, dim=1)\n\n    # Cosine similarity (dot product of normalized vectors)\n    similarities = (bug_stack * src_stack).sum(dim=1)  # (batch_size,)\n\n    return similarities.cpu().numpy()\n","metadata":{"id":"BmKfMQ9oAFSQ","outputId":"e0bf60b5-8bc7-401f-c8ee-5e8ac04ebe01","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:16.758424Z","iopub.execute_input":"2025-04-13T13:55:16.758816Z","iopub.status.idle":"2025-04-13T13:55:20.876655Z","shell.execute_reply.started":"2025-04-13T13:55:16.758775Z","shell.execute_reply":"2025-04-13T13:55:20.875424Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### Äáº·c trÆ°ng 3: Similar Bug Report Score ","metadata":{}},{"cell_type":"markdown","source":"â†’ Kiá»ƒm tra xem bug report nÃ y cÃ³ giá»‘ngÂ **nhá»¯ng bug report cÅ© tá»«ng sá»­a cÃ¹ng file Ä‘Ã³**Â khÃ´ng?\n\n- `build_bug_fix_history(pairs)` â†’ XD lá»‹ch sá»­ chá»‰nh sá»­a theo tá»«ng file\n- `compute_similar_bug_score(pairs, history)`\n    - Input: pairs, history\n    - So sÃ¡nh bug hiá»‡n táº¡i vÃ  bug cÅ©:\n    \n    cosine_similarity(TfidfVectorizer().fit_transform([bug_now, bug_old]))[0, 1]\n    \n    - Láº¥y giÃ¡ trá»‹ tÆ°Æ¡ng Ä‘á»“ng cao nháº¥t vá»«a tÃ¬m Ä‘Æ°á»£c","metadata":{}},{"cell_type":"code","source":"def build_bug_fix_history(pairs):\n    history = {}\n    for bug_id, bug, src_path, _, label in pairs:\n        if label == 1:  # chá»‰ tÃ­nh cÃ¡c bug tháº­t sá»± sá»­a file\n            if src_path not in history:\n                history[src_path] = []\n            history[src_path].append((bug_id, bug.report_time, bug_to_text(bug)))\n    return history\n\n# Äáº·c trÆ°ng 3: Similar Bug Report Score\ndef compute_similar_bug_score(pairs, history):\n    scores = []\n    for bug_id, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        current_text = bug_to_text(bug)\n\n        sim_scores = []\n        if src_path in history:\n            for hist_bug_id, hist_time, hist_text in history[src_path]:\n                if hist_time < current_time:  # chá»‰ tÃ­nh bug trong quÃ¡ khá»©\n                    sim = cosine_similarity(\n                        TfidfVectorizer().fit_transform([current_text, hist_text])\n                    )[0, 1]\n                    sim_scores.append(sim)\n        scores.append(max(sim_scores) if sim_scores else 0.0)\n    return np.array(scores)","metadata":{"id":"imb4_du_Bgjz","outputId":"8c08c8e9-7047-4e5d-de85-8b70e30cbe28","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:20.877792Z","iopub.execute_input":"2025-04-13T13:55:20.878336Z","iopub.status.idle":"2025-04-13T13:55:20.885693Z","shell.execute_reply.started":"2025-04-13T13:55:20.878307Z","shell.execute_reply":"2025-04-13T13:55:20.884348Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### Äáº·c trung 4: Time Since Last Fix (ngÃ y, normalize)\n- Kiá»ƒm tra vá»›i má»—i `(bug report, source file)` xem tá»«ng Ä‘Æ°á»£c sá»­a trÆ°á»›c Ä‘Ã³ khÃ´ng vÃ  láº§n cuá»‘i khi nÃ o\n    - ÄÃ£ lÃ¢u k sá»­a â†’ Ãt lá»—i â†’ Äiá»ƒm tháº¥p\n    - Má»›i sá»­a â†’ cÃ³ thá»ƒ liÃªn quan tá»›i lá»—i â†’ Äiá»ƒm cao\n- CÃ¡ch hÄ‘:\n    - TÃ¬m thá»i Ä‘iá»ƒm bug current_time\n    - TÃ¬m history cÃ¡c láº§n sá»­a file trong quÃ¡ khá»©\n    - TÃ­nh khoáº£ng cÃ¡ch time giá»¯a current vÃ  history gáº§n nháº¥t\n    - ChÆ°a sá»­a â†’ GÃ¡n sá»‘ delta_days=9999\n    - Chuáº©n hoÃ¡","metadata":{}},{"cell_type":"code","source":"# Äáº·c trÆ°ng 4: Time Since Last Fix (ngÃ y, normalize)\ndef compute_time_since_last_fix(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_times = [hist_time for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            if past_times:\n                delta_days = (current_time - max(past_times)).days\n            else:\n                delta_days = 9999  # Cá»±c lá»›n náº¿u chÆ°a tá»«ng sá»­a\n        else:\n            delta_days = 9999\n        scores.append(delta_days)\n\n    # Normalize vá» [0,1]\n    max_days = max(scores) if max(scores) != 0 else 1  # TrÃ¡nh chia cho 0\n\n    return np.array([1 - (s / max_days) for s in scores])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:20.886885Z","iopub.execute_input":"2025-04-13T13:55:20.887240Z","iopub.status.idle":"2025-04-13T13:55:20.908956Z","shell.execute_reply.started":"2025-04-13T13:55:20.887198Z","shell.execute_reply":"2025-04-13T13:55:20.907714Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Äáº·c trÆ°ng 5: Fix Frequency (sá»‘ láº§n bá»‹ sá»­a trong quÃ¡ khá»©, normalize)\n\n\n- Kiá»ƒm tra xme má»—i cáº·p Ä‘Æ°á»£c sá»­a bao nhiÃªu láº§n\n\nâ†’ Sá»­a nhiá»u â†’ File dá»… dÃ­nh lá»—i â†’ Äiá»ƒm cao","metadata":{}},{"cell_type":"code","source":"# Äáº·c trÆ°ng 5: Fix Frequency (sá»‘ láº§n bá»‹ sá»­a trong quÃ¡ khá»©, normalize)\ndef compute_fix_frequency(pairs, history):\n    scores = []\n    for _, bug, src_path, _, _ in pairs:\n        current_time = bug.report_time\n        if src_path in history:\n            past_fixes = [1 for _, hist_time, _ in history[src_path] if hist_time < current_time]\n            freq = len(past_fixes)\n        else:\n            freq = 0\n        scores.append(freq)\n    # Normalize vá» [0,1] an toÃ n\n    max_freq = max(scores)\n    max_freq = max(max_freq, 1)  # trÃ¡nh chia 0\n    return np.array([s / max_freq for s in scores])\n\n\n# DÃ¹ng cho 500 cáº·p máº«u\nsampled_pairs = train_pairs[:5000]\nbug_history = build_bug_fix_history(train_pairs)\n\nsimilar_bug_score = compute_similar_bug_score(sampled_pairs, bug_history)\ntime_since_last_fix = compute_time_since_last_fix(sampled_pairs, bug_history)\nfix_frequency = compute_fix_frequency(sampled_pairs, bug_history)\n\n# TrÃ­ch 5 giÃ¡ trá»‹ Ä‘áº§u má»—i feature\nsimilar_bug_score[:50], time_since_last_fix[:50], fix_frequency[:50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:20.913139Z","iopub.execute_input":"2025-04-13T13:55:20.913594Z","iopub.status.idle":"2025-04-13T13:55:21.106456Z","shell.execute_reply.started":"2025-04-13T13:55:20.913546Z","shell.execute_reply":"2025-04-13T13:55:21.105369Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"# 4. QuÃ¡ trÃ¬nh huáº¥n luyá»‡n","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Táº¡o ma tráº­n train, test","metadata":{}},{"cell_type":"code","source":"","metadata":{"id":"PmEZSTiyDEB4","outputId":"6db11041-f3a2-4931-f264-9480bfd7c2e6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_feature_matrix_batch(pairs_batch, glove_dict, history):\n    lexical = compute_lexical_similarity(pairs_batch)\n    semantic = compute_semantic_similarity(pairs_batch, glove_dict)\n    similar_score = compute_similar_bug_score(pairs_batch, history)\n    recency = compute_time_since_last_fix(pairs_batch, history)\n    freq = compute_fix_frequency(pairs_batch, history)\n\n    # Stack theo chiá»u dá»c â†’ (batch_size, 5)\n    X = np.stack([lexical, semantic, similar_score, recency, freq], axis=1)\n    return X\n\ndef get_labels(pairs):\n    return np.array([label for *_, label in pairs])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:21.107963Z","iopub.execute_input":"2025-04-13T13:55:21.108265Z","iopub.status.idle":"2025-04-13T13:55:21.113723Z","shell.execute_reply.started":"2025-04-13T13:55:21.108238Z","shell.execute_reply":"2025-04-13T13:55:21.112635Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def pair_generator(pairs, batch_size, glove_dict, history, device=\"cuda\"):\n    for i in range(0, len(pairs), batch_size):\n        batch = pairs[i:i + batch_size]\n        X = build_feature_matrix_batch(batch, glove_dict, history)\n        y = np.array([label for *_, label in batch])\n\n        # Tráº£ vá» tensor Ä‘Ãºng device\n        yield (\n            torch.tensor(X, dtype=torch.float32, device=device),\n            torch.tensor(y, dtype=torch.float32, device=device)\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:21.114841Z","iopub.execute_input":"2025-04-13T13:55:21.115255Z","iopub.status.idle":"2025-04-13T13:55:21.134484Z","shell.execute_reply.started":"2025-04-13T13:55:21.115219Z","shell.execute_reply":"2025-04-13T13:55:21.133147Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.2 XÃ¢y dá»±ng mÃ´ hÃ¬nh","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\n# Äá»‹nh nghÄ©a mÃ´ hÃ¬nh DNN giá»‘ng bÃ i bÃ¡o\nimport torch\nimport torch.nn as nn\n\nclass BugLocalization(nn.Module):\n        def __init__(self, input_dim=5, hidden_dims=[128, 64], output_dim=1):\n            super(BugLocalization, self).__init__()\n\n            # Define a series of fully connected (Dense) layers\n            self.fc1 = nn.Linear(input_dim, hidden_dims[0])  # First hidden layer\n            self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])  # Second hidden layer\n            self.fc3 = nn.Linear(hidden_dims[1], output_dim)  # Output layer\n\n            # Define activation function (ReLU for hidden layers and Sigmoid for output)\n            self.relu = nn.ReLU()\n            self.sigmoid = nn.Sigmoid()\n\n        def forward(self, x):\n        # Forward pass through the DNN\n            x = self.relu(self.fc1(x))  # Pass through the first hidden layer with ReLU activation\n            x = self.relu(self.fc2(x))  # Pass through the second hidden layer with ReLU activation\n            x = self.sigmoid(self.fc3(x))  # Output layer with Sigmoid activation for binary classification\n            return x.squeeze()  # Remove extra dimension from the output (as it's a single value per input)\n\n\n        \n# Äá»‹nh nghÄ©a focal loss\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.999, gamma=2.0, eps=1e-6):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, preds, targets):\n        preds = preds.clamp(min=self.eps, max=1. - self.eps)\n        loss = -self.alpha * (1 - preds) ** self.gamma * targets * torch.log(preds) \\\n               - (1 - self.alpha) * preds ** self.gamma * (1 - targets) * torch.log(1 - preds)\n        return loss.mean()\n\n\n# Huáº¥n luyá»‡n mÃ´ hÃ¬nh\ndef train_model_generator(model, train_gen, epochs=10, lr=1e-3):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = FocalLoss()\n\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch_X, batch_y in train_gen:\n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n            # ğŸ§¹ Dá»n bá»™ nhá»› má»—i batch\n            del batch_X, batch_y, outputs, loss\n            torch.cuda.empty_cache()\n            import gc; gc.collect()\n\n        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n\n","metadata":{"id":"LiD_wJxjDeQ8","outputId":"9821aee2-a66e-480b-9f69-6e4d207c7ba6","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:21.135517Z","iopub.execute_input":"2025-04-13T13:55:21.135825Z","iopub.status.idle":"2025-04-13T13:55:21.153469Z","shell.execute_reply.started":"2025-04-13T13:55:21.135800Z","shell.execute_reply":"2025-04-13T13:55:21.152232Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# In thá»­ 1 batch Ä‘áº§u\nfor X_batch, y_batch in pair_generator(train_pairs, 128, glove_embeddings, bug_history):\n    print(\"ğŸ‘‰ Feature Sample:\", X_batch[2])\n    print(\"ğŸ‘‰ Label Sample:\", y_batch[2])\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:21.154487Z","iopub.execute_input":"2025-04-13T13:55:21.154827Z","iopub.status.idle":"2025-04-13T13:55:21.426374Z","shell.execute_reply.started":"2025-04-13T13:55:21.154792Z","shell.execute_reply":"2025-04-13T13:55:21.424905Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-9b8227225653>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# In thá»­ 1 batch Ä‘áº§u\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpair_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbug_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ‘‰ Feature Sample:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ‘‰ Label Sample:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-422c938477b1>\u001b[0m in \u001b[0;36mpair_generator\u001b[0;34m(pairs, batch_size, glove_dict, history, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_feature_matrix_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-e9d686b3849b>\u001b[0m in \u001b[0;36mbuild_feature_matrix_batch\u001b[0;34m(pairs_batch, glove_dict, history)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_feature_matrix_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlexical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_lexical_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msemantic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_semantic_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msimilar_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_similar_bug_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrecency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_time_since_last_fix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-89aa41332fb5>\u001b[0m in \u001b[0;36mcompute_semantic_similarity\u001b[0;34m(pairs, glove_dict, dim, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Convert all to tensor and move to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mbug_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membed_text_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbug_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0msrc_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membed_text_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrc_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-89aa41332fb5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Convert all to tensor and move to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mbug_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membed_text_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbug_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0msrc_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membed_text_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrc_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"],"ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","output_type":"error"}],"execution_count":27},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def test_pair_generator(pairs, batch_size, glove_dict, history, device=\"cuda\"):\n    for i in range(0, len(pairs), batch_size):\n        batch = pairs[i:i + batch_size]\n        X = build_feature_matrix_batch(batch, glove_dict, history)\n\n        # âœ… Tráº£ vá» tensor trÃªn GPU\n        yield torch.tensor(X, dtype=torch.float32, device=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:48.621814Z","iopub.execute_input":"2025-04-13T13:55:48.622291Z","iopub.status.idle":"2025-04-13T13:55:48.629429Z","shell.execute_reply.started":"2025-04-13T13:55:48.622249Z","shell.execute_reply":"2025-04-13T13:55:48.628241Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom sklearn.metrics import average_precision_score\nfrom collections import defaultdict\nimport numpy as np\n# Äá»‹nh nghÄ©a mÃ´ hÃ¬nh DNN giá»‘ng bÃ i bÃ¡o\nimport torch\nimport torch.nn as nn\n\nclass BugLocalization(nn.Module):\n        def __init__(self, input_dim=5, hidden_dims=[128, 64], output_dim=1):\n            super(BugLocalization, self).__init__()\n\n            # Define a series of fully connected (Dense) layers\n            self.fc1 = nn.Linear(input_dim, hidden_dims[0])  # First hidden layer\n            self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])  # Second hidden layer\n            self.fc3 = nn.Linear(hidden_dims[1], output_dim)  # Output layer\n\n            # Define activation function (ReLU for hidden layers and Sigmoid for output)\n            self.relu = nn.ReLU()\n            self.sigmoid = nn.Sigmoid()\n\n        def forward(self, x):\n        # Forward pass through the DNN\n            x = self.relu(self.fc1(x))  # Pass through the first hidden layer with ReLU activation\n            x = self.relu(self.fc2(x))  # Pass through the second hidden layer with ReLU activation\n            x = self.sigmoid(self.fc3(x))  # Output layer with Sigmoid activation for binary classification\n            return x.squeeze()  # Remove extra dimension from the output (as it's a single value per input)\n\n\n        \n# Äá»‹nh nghÄ©a focal loss\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.999, gamma=2.0, eps=1e-6):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, preds, targets):\n        preds = preds.clamp(min=self.eps, max=1. - self.eps)\n        loss = -self.alpha * (1 - preds) ** self.gamma * targets * torch.log(preds) \\\n               - (1 - self.alpha) * preds ** self.gamma * (1 - targets) * torch.log(1 - preds)\n        return loss.mean()\n\n\n\n# ÄÃ¡nh giÃ¡ cÃ¡c chá»‰ sá»‘ (MAP, MRR, Top-k)\ndef compute_topk_accuracy(y_true, y_scores, k=10):\n    bug_to_scores = {}\n    for (bug_id, _, src_path, _, label), score in zip(test_pairs, y_scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    correct_at_k = 0\n    total = 0\n\n    for bug_id, entries in bug_to_scores.items():\n        sorted_entries = sorted(entries, key=lambda x: x[0], reverse=True)\n        top_k = sorted_entries[:k]\n        if any(label == 1 for _, label in top_k):\n            correct_at_k += 1\n        total += 1\n\n    return correct_at_k / total if total > 0 else 0\n\n\ndef compute_MAP_per_bug(test_pairs, y_pred_probs):\n    # Gom nhÃ£n vÃ  score theo bug_id\n    bug_to_ytrue = defaultdict(list)\n    bug_to_yscore = defaultdict(list)\n\n    for (bug_id, _, _, _, label), score in zip(test_pairs, y_pred_probs):\n        bug_to_ytrue[bug_id].append(label)\n        bug_to_yscore[bug_id].append(score)\n\n    # TÃ­nh AP cho tá»«ng bug, chá»‰ giá»¯ bug cÃ³ Ã­t nháº¥t 1 label = 1\n    ap_list = []\n    for bug_id in bug_to_ytrue:\n        y_true = np.array(bug_to_ytrue[bug_id])\n        y_score = np.array(bug_to_yscore[bug_id])\n\n        if np.sum(y_true) == 0:\n            continue  # bá» qua bug khÃ´ng cÃ³ file liÃªn quan\n\n        ap = average_precision_score(y_true, y_score)\n        ap_list.append(ap)\n\n    # TÃ­nh MAP\n    MAP = np.mean(ap_list) if ap_list else 0.0\n    return MAP\n\n# MRR (Mean Reciprocal Rank)\ndef mean_reciprocal_rank(pairs, scores):\n    bug_to_scores = {}\n    for (bug_id, _, _, _, label), score in zip(pairs, scores):\n        if bug_id not in bug_to_scores:\n            bug_to_scores[bug_id] = []\n        bug_to_scores[bug_id].append((score, label))\n\n    rr_sum = 0\n    count = 0\n    for bug_id, ranked in bug_to_scores.items():\n        ranked = sorted(ranked, key=lambda x: x[0], reverse=True)\n        for idx, (_, label) in enumerate(ranked):\n            if label == 1:\n                rr_sum += 1 / (idx + 1)\n                break\n        count += 1\n    return rr_sum / count if count > 0 else 0","metadata":{"id":"hvK-tE_MD4zq","outputId":"5444ce3e-4624-4f81-eb6b-b19a543b0d49","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:48.630726Z","iopub.execute_input":"2025-04-13T13:55:48.631056Z","iopub.status.idle":"2025-04-13T13:55:48.653851Z","shell.execute_reply.started":"2025-04-13T13:55:48.631031Z","shell.execute_reply":"2025-04-13T13:55:48.652805Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"device=\"cpu\"\ndef run_kfold_training_and_eval(folds, source_files, glove_dict, k=3, device=\"cuda\"):\n    results = {\n        \"fold\": [],\n        \"MAP\": [],\n        \"MRR\": [],\n        \"Top1\": [],\n        \"Top2\": [],\n        \"Top3\": [],\n        \"Top4\": [],\n        \"Top5\": [],\n        \"Top10\": [],\n        \"Top15\": []\n    }\n\n    for i in range(k - 1):\n        print(f\"\\nğŸ“¦ Fold 0..{i} â¤ {i+1}\")\n        \n        # âœ… Gá»™p táº¥t cáº£ fold tá»« 0 Ä‘áº¿n i lÃ m train\n        train_fold = [pair for j in range(i + 1) for pair in folds[j]]\n        test_fold = folds[i + 1]\n\n        # âœ… Train: láº¥y má»™t sá»‘ lÆ°á»£ng negative cá»‘ Ä‘á»‹nh\n        train_pairs = generate_balanced_pairs(train_fold, source_files, num_negatives_per_positive=50)\n\n        # âœ… Test: láº¥y full negative (khÃ´ng sampling)\n        test_pairs = generate_all_negatives_pairs(test_fold, source_files)\n   \n\n        if sum(1 for p in train_pairs if p[-1] == 1) < 1:\n            print(\"âš ï¸ Bá» qua do quÃ¡ Ã­t positive samples\")\n            continue\n\n        # âœ… Build history\n        bug_history = build_bug_fix_history(train_pairs)\n\n        # âœ… Model\n        model = BugLocalization(input_dim=5).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\n        criterion = FocalLoss(alpha=0.99)\n\n        # âœ… Train\n        model.train()\n        for epoch in range(10):\n            train_gen = pair_generator(train_pairs, batch_size=128, glove_dict=glove_dict, history=bug_history, device=device)\n            total_loss = 0\n            for X_batch, y_batch in train_gen:\n                optimizer.zero_grad()\n                outputs = model(X_batch)\n                loss = criterion(outputs, y_batch)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n                del X_batch, y_batch, outputs, loss\n                torch.cuda.empty_cache()\n                import gc; gc.collect()\n\n            print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")\n\n        # âœ… Dá»± Ä‘oÃ¡n\n        bug_history_test = build_bug_fix_history(test_pairs)\n        y_test = get_labels(test_pairs)\n        y_pred_probs = []\n\n        model.eval()\n        with torch.no_grad():\n            for X_batch in test_pair_generator(test_pairs, batch_size=128, glove_dict=glove_dict, history=bug_history_test, device=device):\n                batch_probs = model(X_batch).cpu().numpy()\n                y_pred_probs.extend(batch_probs)\n\n        # âœ… Metrics\n        map_score = compute_MAP_per_bug(test_pairs, y_pred_probs)\n        mrr_score = mean_reciprocal_rank(test_pairs, y_pred_probs)\n        top1 = compute_topk_accuracy(y_test, y_pred_probs, k=1)\n        top2 = compute_topk_accuracy(y_test, y_pred_probs, k=2)\n        top3 = compute_topk_accuracy(y_test, y_pred_probs, k=3)\n        top4 = compute_topk_accuracy(y_test, y_pred_probs, k=4)\n        top5 = compute_topk_accuracy(y_test, y_pred_probs, k=5)\n        top10 = compute_topk_accuracy(y_test, y_pred_probs, k=10)\n        top15 = compute_topk_accuracy(y_test, y_pred_probs, k=15)\n\n        print(f\"âœ… Fold 0..{i} â¤ {i+1} Results:\")\n        print(f\"  â¤ MAP:   {map_score:.4f}\")\n        print(f\"  â¤ MRR:   {mrr_score:.4f}\")\n        print(f\"  â¤ Top@1: {top1:.4f} | Top@2: {top2:.4f} | Top@3: {top3:.4f}\")\n        print(f\"  â¤ Top@4: {top4:.4f} | Top@5: {top5:.4f} | Top@10: {top10:.4f} | Top@15: {top15:.4f}\")\n\n        results[\"fold\"].append(i)\n        results[\"MAP\"].append(map_score)\n        results[\"MRR\"].append(mrr_score)\n        results[\"Top1\"].append(top1)\n        results[\"Top2\"].append(top2)\n        results[\"Top3\"].append(top3)\n        results[\"Top4\"].append(top4)\n        results[\"Top5\"].append(top5)\n        results[\"Top10\"].append(top10)\n        results[\"Top15\"].append(top15)\n\n    return results\n","metadata":{"id":"0esoYB7ME-At","outputId":"cbd0b93b-371f-4886-b013-7532bccb8b96","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:56:20.594744Z","iopub.execute_input":"2025-04-13T13:56:20.595127Z","iopub.status.idle":"2025-04-13T13:56:20.609820Z","shell.execute_reply.started":"2025-04-13T13:56:20.595096Z","shell.execute_reply":"2025-04-13T13:56:20.608681Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim  # âœ… pháº§n bá»‹ thiáº¿u\nfrom sklearn.metrics import average_precision_score\n\n\n# Run K-fold training and evaluation\nfull_results = run_kfold_training_and_eval(data_folds, data_src, glove_embeddings)\n\n# Output full results\nprint(\"\\nFull Results:\")\nfor key, value in full_results.items():\n    print(f\"{key}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:56:25.947437Z","iopub.execute_input":"2025-04-13T13:56:25.947762Z","iopub.status.idle":"2025-04-13T13:56:28.961979Z","shell.execute_reply.started":"2025-04-13T13:56:25.947737Z","shell.execute_reply":"2025-04-13T13:56:28.960473Z"}},"outputs":[{"name":"stdout","text":"\nğŸ“¦ Fold 0..0 â¤ 1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-7f6579983741>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Run K-fold training and evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mfull_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_kfold_training_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Output full results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-89bafb90084b>\u001b[0m in \u001b[0;36mrun_kfold_training_and_eval\u001b[0;34m(folds, source_files, glove_dict, k, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# âœ… Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBugLocalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFocalLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"],"ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","output_type":"error"}],"execution_count":33},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n# In káº¿t quáº£ tá»•ng há»£p sau khi cháº¡y táº¥t cáº£ folds\nprint(\"\\nğŸ“Š Káº¿t quáº£ tá»•ng há»£p:\")\nfor i in range(len(full_results[\"fold\"])):\n    print(f\"Fold {full_results['fold'][i]}:\")\n    print(f\"  â¤ MAP: {full_results['MAP'][i]:.4f}\")\n    print(f\"  â¤ MRR: {full_results['MRR'][i]:.4f}\")\n    print(f\"  â¤ Top1: {full_results['Top1'][i]:.4f}\")\n    print(f\"  â¤ Top2: {full_results['Top2'][i]:.4f}\")\n    print(f\"  â¤ Top3: {full_results['Top3'][i]:.4f}\")\n    print(f\"  â¤ Top4: {full_results['Top4'][i]:.4f}\")\n    print(f\"  â¤ Top5: {full_results['Top5'][i]:.4f}\")\n    print(f\"  â¤ Top10: {full_results['Top10'][i]:.4f}\")\n    print(f\"  â¤ Top15: {full_results['Top15'][i]:.4f}\")\n\n# TÃ­nh trung bÃ¬nh cho táº¥t cáº£ cÃ¡c chá»‰ sá»‘\nmean_map = np.mean(full_results[\"MAP\"])\nmean_mrr = np.mean(full_results[\"MRR\"])\nmean_top1 = np.mean(full_results[\"Top1\"])\nmean_top2 = np.mean(full_results[\"Top2\"])\nmean_top3 = np.mean(full_results[\"Top3\"])\nmean_top4 = np.mean(full_results[\"Top4\"])\nmean_top5 = np.mean(full_results[\"Top5\"])\nmean_top10 = np.mean(full_results[\"Top10\"])\nmean_top15 = np.mean(full_results[\"Top15\"])\n\n# In káº¿t quáº£ trung bÃ¬nh\nprint(\"\\nğŸ“Š Káº¿t quáº£ trung bÃ¬nh trÃªn toÃ n bá»™ k-folds:\")\nprint(f\"  â¤ MAP: {mean_map:.4f}\")\nprint(f\"  â¤ MRR: {mean_mrr:.4f}\")\nprint(f\"  â¤ Top1: {mean_top1:.4f}\")\nprint(f\"  â¤ Top2: {mean_top2:.4f}\")\nprint(f\"  â¤ Top3: {mean_top3:.4f}\")\nprint(f\"  â¤ Top4: {mean_top4:.4f}\")\nprint(f\"  â¤ Top5: {mean_top5:.4f}\")\nprint(f\"  â¤ Top10: {mean_top10:.4f}\")\nprint(f\"  â¤ Top15: {mean_top15:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:53.364243Z","iopub.status.idle":"2025-04-13T13:55:53.364607Z","shell.execute_reply":"2025-04-13T13:55:53.364471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Dá»¯ liá»‡u SOTA: ImbalancedBugLoc\ndata = {\n    \"Project\": [\"AspectJ\", \"Tomcat\", \"Eclipse\", \"SWT\", \"Birt\"],\n    \"Top1_SOTA\": [52.5, 53.2, 48.1, 40.2, 28.3],\n    \"Top2_SOTA\": [68.7, 65.5, 62.1, 54.9, 39.3],\n    \"Top3_SOTA\": [77.2, 71.0, 68.8, 64.2, 45.7],\n    \"Top4_SOTA\": [81.0, 75.0, 73.0, 69.3, 51.0],\n    \"Top5_SOTA\": [83.8, 78.3, 76.7, 73.4, 53.6],\n    \"Top10_SOTA\": [89.0, 85.6, 84.7, 84.8, 63.2],\n    \"Top15_SOTA\": [91.5, 88.9, 87.8, 89.1, 69.2],\n    \"MRR_SOTA\": [0.66, 0.64, 0.60, 0.55, 0.40],\n    \"MAP_SOTA\": [0.50, 0.59, 0.54, 0.50, 0.32],\n    \"Top1_New\": [61.90, 56.75, 66.30, 60.63, 45.97],\n    \"Top2_New\": [71.13, 69.58, 78.20, 74.70, 59.04],\n    \"Top3_New\": [76.19, 77.48, 83.35, 81.28, 66.60],\n    \"Top4_New\": [80.06, 82.71, 86.71, 84.77, 71.74],\n    \"Top5_New\": [82.74, 85.79, 89.13, 87.49, 75.62],\n    \"Top10_New\": [88.39, 92.90, 94.38, 93.91, 85.76],\n    \"Top15_New\": [91.07, 94.90, 96.19, 96.07, 90.50],\n    \"MRR_New\": [0.7109, 0.6899, 0.7644, 0.7183, 0.5921],\n    \"MAP_New\": [0.5367, 0.4946, 0.5692, 0.4669, 0.3734]\n}\n\n# Táº¡o DataFrame tá»« dá»¯ liá»‡u\ndf = pd.DataFrame(data)\n\n# TÃ¡ch thÃ nh hai báº£ng: SOTA vÃ  New Model\ndf_sota = df[[\"Project\", \"Top1_SOTA\", \"Top2_SOTA\", \"Top3_SOTA\", \"Top4_SOTA\", \"Top5_SOTA\", \n              \"Top10_SOTA\", \"Top15_SOTA\", \"MRR_SOTA\", \"MAP_SOTA\"]].copy()\ndf_sota[\"Model\"] = \"SOTA\"\n\ndf_new = df[[\"Project\", \"Top1_New\", \"Top2_New\", \"Top3_New\", \"Top4_New\", \"Top5_New\", \n             \"Top10_New\", \"Top15_New\", \"MRR_New\", \"MAP_New\"]].copy()\ndf_new[\"Model\"] = \"New Model\"\n\n# Äá»•i tÃªn cá»™t giá»‘ng báº£ng trong áº£nh\nrename_cols = {\n    \"Top1_SOTA\": \"1\", \"Top2_SOTA\": \"2\", \"Top3_SOTA\": \"3\", \"Top4_SOTA\": \"4\",\n    \"Top5_SOTA\": \"5\", \"Top10_SOTA\": \"10\", \"Top15_SOTA\": \"15\", \"MRR_SOTA\": \"MRR\", \"MAP_SOTA\": \"MAP\",\n    \"Top1_New\": \"1\", \"Top2_New\": \"2\", \"Top3_New\": \"3\", \"Top4_New\": \"4\",\n    \"Top5_New\": \"5\", \"Top10_New\": \"10\", \"Top15_New\": \"15\", \"MRR_New\": \"MRR\", \"MAP_New\": \"MAP\"\n}\n\ndf_sota.rename(columns=rename_cols, inplace=True)\ndf_new.rename(columns=rename_cols, inplace=True)\n\n# Gá»™p láº¡i\ndf_combined = pd.concat([df_sota, df_new], axis=0)\ndf_combined = df_combined.sort_values(by=[\"Project\", \"Model\"]).reset_index(drop=True)\n\n# ÄÆ°a cá»™t 'Model' vá» sau 'Project'\ncols = df_combined.columns.tolist()\ncols.insert(1, cols.pop(cols.index('Model')))\ndf_combined = df_combined[cols]\n\n# Hiá»ƒn thá»‹ káº¿t quáº£\ndf_combined\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:53.365595Z","iopub.status.idle":"2025-04-13T13:55:53.365923Z","shell.execute_reply":"2025-04-13T13:55:53.365797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Xuáº¥t ra file Excel\ndf_combined.to_excel(\"model_comparison.xlsx\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:53.366769Z","iopub.status.idle":"2025-04-13T13:55:53.367086Z","shell.execute_reply":"2025-04-13T13:55:53.366960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Táº¡o láº¡i df_combined Ä‘á»ƒ cáº­p nháº­t MAP vÃ  MRR nhÃ¢n 100\ndf_combined_scaled = df_combined.copy()\ndf_combined_scaled[\"MAP\"] *= 100\ndf_combined_scaled[\"MRR\"] *= 100\n\n# Váº½ láº¡i 5 biá»ƒu Ä‘á»“ cá»™t vá»›i MAP vÃ  MRR nhÃ¢n 100\nprojects = df_combined_scaled[\"Project\"].unique()\nmetrics = [\"1\", \"2\", \"3\", \"4\", \"5\", \"10\", \"15\", \"MRR\", \"MAP\"]\n\nplt.figure(figsize=(20, 25))\n\nfor i, project in enumerate(projects, 1):\n    plt.subplot(3, 2, i)\n    sota_vals = df_combined_scaled[(df_combined_scaled[\"Project\"] == project) & (df_combined_scaled[\"Model\"] == \"SOTA\")][metrics].values.flatten()\n    new_vals = df_combined_scaled[(df_combined_scaled[\"Project\"] == project) & (df_combined_scaled[\"Model\"] == \"New Model\")][metrics].values.flatten()\n    \n    x = range(len(metrics))\n    bar_width = 0.35\n\n    plt.bar([xi - bar_width/2 for xi in x], sota_vals, width=bar_width, label=\"SOTA\")\n    plt.bar([xi + bar_width/2 for xi in x], new_vals, width=bar_width, label=\"New Model\")\n\n    plt.title(f\"So sÃ¡nh SOTA vs New Model - {project}\")\n    plt.xticks(ticks=x, labels=metrics)\n    plt.ylabel(\"GiÃ¡ trá»‹ (%)\")\n    plt.ylim(0, 110)\n    plt.grid(axis='y')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:55:53.367868Z","iopub.status.idle":"2025-04-13T13:55:53.368152Z","shell.execute_reply":"2025-04-13T13:55:53.368039Z"}},"outputs":[],"execution_count":null}]}